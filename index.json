[{"content":"Shadow Bias 有关Shadow Bias的介绍我只推荐看这篇知乎文章自适应Shadow Bias算法，里面介绍了有关Shadow Bias的几乎所有需要了解的信息。同时这篇文章也指出了Unity目前正在使用的Shadow Caster Vertex Based Bias方法的不足之处。\n如果是平时使用URP的话，Unity使用的在绘制Shadow Caster Pass时将ShadowBias添加到顶点的偏移上的方法，虽然称不上尽善尽美，但是也完全够用了。但是在二次元角色渲染的时候，为了营造丝袜勒肉的效果，会在腿部的模型和丝袜的模型交接的区域做一个向内凹陷的效果。这个区域两侧的模型是正常闭合的，其法线是相对的，此时如果使用Unity默认的使用ShadowBias去调整顶点位置的方式，ShadowBias中的NormalBias就会导致这个区域两侧的模型朝着法线的反方向偏移，导致这个区域的阴影在某些角度时会出现漏洞，可以在文章封面图的左边看到明显的瑕疵，右边则是在顶点着色器中使用了ShadowBias，看上去的效果就比较正常了，而且角色的阴影和角色的模型的大小也基本保持一致。当然了，把NormalBias设置成0就不会有这个问题了，但是失去了NormalBias则会带来其他角度的阴影的瑕疵。\n本文使用的是Unity2022.3.43f1c1，URP版本是14.0.11。\n关于Shadows.hlsl的碎碎念 我讨厌Unity URP Package里的Shadows.hlsl，因为它使用了一个LerpWhiteTo的方法，这个方法定义在CoreRP的CommonMaterial.hlsl里，而不知道为什么Shadows.hlsl并没有包含这个CommonMaterial，这就导致了我只想单独使用Shadows.hlsl时，必须还得手动包含一遍CommonMaterial.hlsl，我觉得这很不合理。\n更要命的是，如果在hlsl中想要使用URP的Light结构体，就起码得包含RealtimeLights.hlsl，这个hlsl又包含了Shadows.hlsl。索性我就直接从这些文件中摘取了一套自己的uber_lights.hlsl和uber_shadows.hlsl，这也方便后续做修改。\n对于URP的修改 虽然我的想法是尽量做到即插即用，也就是说尽量不去修改URP默认的代码，但是很遗憾为了将ShadowBias移动到片元着色器里，还是得稍作修改。\nURP设置ShadowBias是在绘制级联阴影的每一个slice的时候，将其作为参数传给顶点着色器的，也就是说虽然顶点着色器中的参数只有_ShadowBias一个，实际上Unity会使用级联阴影的层级数个ShadowBias，我们需要将其保存起来，在渲染LitForwardPass的时候，根据当前像素的世界坐标找到对应的级联阴影的层级，再查找对应的ShadowBias进行计算。\nMainLightShadowCasterPass.cs 主要是在MainLightShadowCasterPass.cs的RenderMainLightCascadeShadowmap方法中，计算得到了当前层级的ShadowBias之后，将其保存在一个数组中留待后续使用。\nfor (int cascadeIndex = 0; cascadeIndex \u0026lt; m_ShadowCasterCascadesCount; ++cascadeIndex) { settings.splitData = m_CascadeSlices[cascadeIndex].splitData; Vector4 shadowBias = ShadowUtils.GetShadowBias(ref shadowLight, shadowLightIndex, ref renderingData.shadowData, m_CascadeSlices[cascadeIndex].projectionMatrix, m_CascadeSlices[cascadeIndex].resolution); m_MainLightShadowBiases[cascadeIndex] = shadowBias; // Shadow Biases for fragment shader. ShadowUtils.SetupShadowCasterConstantBuffer(cmd, ref shadowLight, shadowBias); CoreUtils.SetKeyword(cmd, ShaderKeywordStrings.CastingPunctualLightShadow, false); ShadowUtils.RenderShadowSlice(cmd, ref context, ref m_CascadeSlices[cascadeIndex], ref settings, m_CascadeSlices[cascadeIndex].projectionMatrix, m_CascadeSlices[cascadeIndex].viewMatrix); } ShadowUtils.cs 比较神秘的是，Unity还额外设置了全局的DepthBias，由于我们直接在片元着色器里计算ShadowBias，这个地方可以不进行设置。如果不注释掉这一行，当我们将DepthBias和NormalBias都设置成0时，我们也几乎不能观察到条纹状的阴影瑕疵，这会影响我们对实际使用的ShadowBias的判断。\npublic static void RenderShadowSlice(CommandBuffer cmd, ref ScriptableRenderContext context, ref ShadowSliceData shadowSliceData, ref ShadowDrawingSettings settings, Matrix4x4 proj, Matrix4x4 view) { // cmd.SetGlobalDepthBias(1.0f, 2.5f); // these values match HDRP defaults (see https://github.com/Unity-Technologies/Graphics/blob/9544b8ed2f98c62803d285096c91b44e9d8cbc47/com.unity.render-pipelines.high-definition/Runtime/Lighting/Shadow/HDShadowAtlas.cs#L197 ) cmd.SetViewport(new Rect(shadowSliceData.offsetX, shadowSliceData.offsetY, shadowSliceData.resolution, shadowSliceData.resolution)); cmd.SetViewProjectionMatrices(view, proj); context.ExecuteCommandBuffer(cmd); cmd.Clear(); context.DrawShadows(ref settings); cmd.DisableScissorRect(); context.ExecuteCommandBuffer(cmd); cmd.Clear(); cmd.SetGlobalDepthBias(0.0f, 0.0f); // Restore previous depth bias values } uber_shadow.hlsl 这里节选了比较关键的部分，大部分是和Unity的Shadows.hlsl一致的，但是需要注意片元中的ApplyShadowBias的方向和顶点中的ApplyShadowBias的方向是相反的，顶点需要远离光源，而片元则需要靠近光源，法线也是同理。采样器使用的是sampler_LinearClampCompare，似乎是将双线性对应的四个像素点的深度和当前深度进行比较之后，再对比较的结果进行线性插值，能够得到一些小小的渐变，配合PCF就能得到比较好看的软阴影了。\n至于要不要使用PCSS，我的想法是并不完全必要。首先PCSS的计算比较复杂，采样数比较多，对性能会有影响，如果使用Dither的方法的话，在使用二次元渲染经常使用的Ramp图时不一定能够得到理想的结果，而使用Ramp图制作皮肤在受光和阴影交接处的次表面散射效果时，需要比较大的阴影变化范围才能做出比较好看的散射效果，使用了PCSS在被较近物体遮挡时就没有很大的半影区域了。\nfloat3 ApplyShadowBias(float3 positionWS, float3 normalWS, float3 lightDirection, float2 shadowBias) { float invNdotL = 1.0 - saturate(dot(lightDirection, normalWS)); float scale = invNdotL * shadowBias.y; // normal bias is negative since we want to apply an inset normal offset positionWS = -lightDirection * shadowBias.xxx + positionWS; positionWS = -normalWS * scale.xxx + positionWS; return positionWS; } half MainLightShadow(float3 positionWS, float3 normalWS) { #if !defined(MAIN_LIGHT_CALCULATE_SHADOWS) return 1.0f; #else #ifdef _MAIN_LIGHT_SHADOWS_CASCADE int cascadeIndex = (int)ComputeCascadeIndex(positionWS); #else int cascadeIndex = 0; #endif float3 lightDirWS = _LightDirection; float2 shadowBias = _MainLightShadowBiases[cascadeIndex].xy; positionWS = ApplyShadowBias(positionWS, normalWS, lightDirWS, shadowBias); float4 shadowCoord = mul(_MainLightWorldToShadow[cascadeIndex], float4(positionWS, 1.0)); half shadow = SampleShadowmap(_MainLightShadowmapTexture, sampler_LinearClampCompare, shadowCoord, _MainLightShadowmapSize); return shadow; #endif } 后记 非常简单的一篇小文章，写这篇文章的主要目的其实是告诉大家我还在做新的东西。。。文章中使用的角色是少前2追放里的绛雨，这里也放一个小小的图透。祝大家国庆节快乐捏！\n","permalink":"https://zznewclear13.github.io/posts/unity-urp-apply-shadow-bias-in-fragment-shader/","summary":"Shadow Bias 有关Shadow Bias的介绍我只推荐看这篇知乎文章自适应Shadow Bias算法，里面介绍了有关Shadow Bias的几乎所有需要了解的信息。同时这篇文章也指出了Unity目前正在使用的Shadow Caster Vertex Based Bias方法的不足之处。\n如果是平时使用URP的话，Unity使用的在绘制Shadow Caster Pass时将ShadowBias添加到顶点的偏移上的方法，虽然称不上尽善尽美，但是也完全够用了。但是在二次元角色渲染的时候，为了营造丝袜勒肉的效果，会在腿部的模型和丝袜的模型交接的区域做一个向内凹陷的效果。这个区域两侧的模型是正常闭合的，其法线是相对的，此时如果使用Unity默认的使用ShadowBias去调整顶点位置的方式，ShadowBias中的NormalBias就会导致这个区域两侧的模型朝着法线的反方向偏移，导致这个区域的阴影在某些角度时会出现漏洞，可以在文章封面图的左边看到明显的瑕疵，右边则是在顶点着色器中使用了ShadowBias，看上去的效果就比较正常了，而且角色的阴影和角色的模型的大小也基本保持一致。当然了，把NormalBias设置成0就不会有这个问题了，但是失去了NormalBias则会带来其他角度的阴影的瑕疵。\n本文使用的是Unity2022.3.43f1c1，URP版本是14.0.11。\n关于Shadows.hlsl的碎碎念 我讨厌Unity URP Package里的Shadows.hlsl，因为它使用了一个LerpWhiteTo的方法，这个方法定义在CoreRP的CommonMaterial.hlsl里，而不知道为什么Shadows.hlsl并没有包含这个CommonMaterial，这就导致了我只想单独使用Shadows.hlsl时，必须还得手动包含一遍CommonMaterial.hlsl，我觉得这很不合理。\n更要命的是，如果在hlsl中想要使用URP的Light结构体，就起码得包含RealtimeLights.hlsl，这个hlsl又包含了Shadows.hlsl。索性我就直接从这些文件中摘取了一套自己的uber_lights.hlsl和uber_shadows.hlsl，这也方便后续做修改。\n对于URP的修改 虽然我的想法是尽量做到即插即用，也就是说尽量不去修改URP默认的代码，但是很遗憾为了将ShadowBias移动到片元着色器里，还是得稍作修改。\nURP设置ShadowBias是在绘制级联阴影的每一个slice的时候，将其作为参数传给顶点着色器的，也就是说虽然顶点着色器中的参数只有_ShadowBias一个，实际上Unity会使用级联阴影的层级数个ShadowBias，我们需要将其保存起来，在渲染LitForwardPass的时候，根据当前像素的世界坐标找到对应的级联阴影的层级，再查找对应的ShadowBias进行计算。\nMainLightShadowCasterPass.cs 主要是在MainLightShadowCasterPass.cs的RenderMainLightCascadeShadowmap方法中，计算得到了当前层级的ShadowBias之后，将其保存在一个数组中留待后续使用。\nfor (int cascadeIndex = 0; cascadeIndex \u0026lt; m_ShadowCasterCascadesCount; ++cascadeIndex) { settings.splitData = m_CascadeSlices[cascadeIndex].splitData; Vector4 shadowBias = ShadowUtils.GetShadowBias(ref shadowLight, shadowLightIndex, ref renderingData.shadowData, m_CascadeSlices[cascadeIndex].projectionMatrix, m_CascadeSlices[cascadeIndex].resolution); m_MainLightShadowBiases[cascadeIndex] = shadowBias; // Shadow Biases for fragment shader. ShadowUtils.SetupShadowCasterConstantBuffer(cmd, ref shadowLight, shadowBias); CoreUtils.SetKeyword(cmd, ShaderKeywordStrings.CastingPunctualLightShadow, false); ShadowUtils.RenderShadowSlice(cmd, ref context, ref m_CascadeSlices[cascadeIndex], ref settings, m_CascadeSlices[cascadeIndex].","title":"在URP的片元着色器中应用阴影偏移"},{"content":"屏幕空间接触阴影 屏幕空间接触阴影是用来解决普通的阴影贴图精度不够的问题而提出来的一种通过深度图在屏幕空间计算阴影的方法。索尼的Bend Studio的Graham Aldridge在Sigraph 2023的索尼创作者大会上，介绍了往日不再（Days Gone）中计算屏幕空间接触阴影的方式，这里可以找到演示文稿和参考代码。\n本篇文章相当于是Radial Dispatch系列的第三篇文章了，与上一篇文章一样，这篇文章是基于径向分派Compute Shader中相关算法的实际应用，具体的缓存方式也可以参考上一篇文章使用Group Shared Memory加速径向模糊，这里就不再赘述了。实际上我发现了这样计算接触阴影的一个缺陷，就是不太好计算软阴影了，由于缓存的限制，随机采样只能在一个很小的范围内分布，基本上用不上了。由于使用的是屏幕空间的深度图的信息，加上厚度检测之后很容易出现漏面的问题，封面中的瑕疵也有一部分是来自于我的Relaxed Cone Step Mapping本身深度值的瑕疵，屏幕上半部分的阴影就好很多。这就当作是一个Proof of Concept吧，之后有机会的话再回来优化优化。\n本文使用的是Unity 2022.3.21f1，URP版本是14.0.10。\n具体的代码 ContactShadowComputeShader.compute 核心的代码来自于前一篇文章径向分派Compute Shader。前一篇文章在循环中是通过统一步长进行采样的，会采样到四个像素中间因此需要双线性插值，这次我们固定水平或者竖直方向的步长为一个像素，这样我们只需要在一个方向上进行线性插值了。由于深度和颜色信息是两种不同的信息，我们仅对距离很近的深度进行线性插值，对于距离较远的两个深度值，我们使用离采样点最近像素的深度值。至于如何判断深度远近，我使用了和屏幕空间反射中相同的_ThicknessParams，默认物体的厚度为linearSampleDepth * _Thickness.y + _Thickness.x。\n#pragma kernel ContactShadowPoint #pragma kernel ClearMain // #pragma warning(disable: 3556) #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #define THREAD_COUNT 128 Texture2D\u0026lt;float4\u0026gt; _ColorTex; Texture2D\u0026lt;float\u0026gt; _CameraDepthTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTex; float3 _LightPosWS; float4 _LightPosCS; float2 _LightPosSS; float2 _ThicknessParams; float4 _TextureSize; float _LightRange; float _Debug; struct DispatchParams { int2 offset; int count; int stride; int xMajor; }; StructuredBuffer\u0026lt;DispatchParams\u0026gt; _DispatchData; int GetDispatchType(int index, out int dispatchIndex, out DispatchParams dispatchParams) { for (int i=0; i\u0026lt;8; ++i) { dispatchParams = _DispatchData[i]; dispatchIndex = dispatchParams.count - 1 - index; if (dispatchIndex \u0026gt;= 0) return i; } return 0; } int2 GetDispatchDirection(int dispatchType, out int2 iLightPosOffset) { dispatchType /= 2; int xDir = dispatchType / 2; int yDir = dispatchType % 2; int2 dir = int2(xDir, yDir); iLightPosOffset = dir - 1; return dir * 2 - 1; } int2 GetDispatchOffset(int dispatchType, int dispatchIndex, DispatchParams dispatchParams, out int groupIndex) { groupIndex = 0; int2 dispatchOffset = int2(0, 0); int offsetType = dispatchType % 2; int colIndexOffset = max(dispatchParams.offset.x,dispatchParams.offset.y)/THREAD_COUNT; int2 indexOffset = dispatchParams.xMajor==1?dispatchParams.offset:dispatchParams.offset.yx; int stride = dispatchParams.stride; int colIndex = dispatchIndex / stride; int rowIndex = dispatchIndex - colIndex * stride; if (offsetType == 0) { int offsetedColIndex = colIndex + colIndexOffset; int tempIndex = rowIndex + indexOffset.y - (offsetedColIndex + 1) * THREAD_COUNT; if (tempIndex \u0026gt;= 0) { dispatchOffset = int2(tempIndex + indexOffset.x, dispatchParams.stride - (colIndex + colIndexOffset + 1) * THREAD_COUNT + indexOffset.x + indexOffset.y); groupIndex = tempIndex; } else { dispatchOffset = int2((offsetedColIndex + 1) * THREAD_COUNT - 1, rowIndex + indexOffset.y); groupIndex = rowIndex; } } else { int minOffsetX = max(dispatchParams.stride + indexOffset.y, (colIndexOffset + 1) * THREAD_COUNT); dispatchOffset = int2(minOffsetX + colIndex * THREAD_COUNT - 1, rowIndex + indexOffset.y); groupIndex = rowIndex; } if (dispatchParams.xMajor == 0) dispatchOffset.xy = dispatchOffset.yx; return dispatchOffset; } void GetIndexedOffset(int index, float2 absDir, bool xMajor, out int2 offset1, out int2 offset2, out int2 offset3) { if (!xMajor) { absDir = absDir.yx; } float val = float(index) * absDir.y / absDir.x; float floorVal = floor(val); float fracVal = frac(val); if (fracVal \u0026lt;= 0.5f) { offset1 = int2(index, floorVal - 1.0f); offset2 = int2(index, floorVal); offset3 = int2(index, floorVal + 1.0f); } else { offset1 = int2(index, floorVal); offset2 = int2(index, floorVal + 1.0f); offset3 = int2(index, floorVal + 2.0f); } if (!xMajor) { offset1 = offset1.yx; offset2 = offset2.yx; offset3 = offset3.yx; } } float LoadDepthTexture(int2 coord) { // coord.y = int(_TextureSize.y) - 1 - coord.y; coord = clamp(coord, int2(0, 0), int2(_TextureSize.xy - 1.0f)); return _CameraDepthTexture.Load(uint3(coord, 0)); } struct DepthData { float depth; float linearDepth; }; groupshared DepthData cachedDepth[THREAD_COUNT * 6]; void SetCachedDepth(DepthData depthData, int2 threadPos) {cachedDepth[threadPos.x+threadPos.y*THREAD_COUNT]=depthData;} DepthData GetCachedDepth(int2 threadPos) {return cachedDepth[threadPos.x+threadPos.y*THREAD_COUNT*2];} void CacheDepth(int2 groupStartSS, float2 lightPosSS, int cacheIndex) { float2 toLight = lightPosSS - (groupStartSS + 0.5f); float2 absDir = abs(toLight); int2 signDir = int2(sign(toLight)); bool xMajor = absDir.x \u0026gt;= absDir.y; float depthVal1, depthVal2, depthVal3; DepthData depthData1, depthData2, depthData3; int2 offset1, offset2, offset3; { GetIndexedOffset(cacheIndex, absDir, xMajor, offset1, offset2, offset3); depthVal1 = LoadDepthTexture(groupStartSS + offset1 * signDir); depthVal2 = LoadDepthTexture(groupStartSS + offset2 * signDir); depthVal3 = LoadDepthTexture(groupStartSS + offset3 * signDir); depthData1.depth = depthVal1; depthData1.linearDepth = LinearEyeDepth(depthVal1, _ZBufferParams); depthData2.depth = depthVal2; depthData2.linearDepth = LinearEyeDepth(depthVal2, _ZBufferParams); depthData3.depth = depthVal3; depthData3.linearDepth = LinearEyeDepth(depthVal3, _ZBufferParams); SetCachedDepth(depthData1, int2(cacheIndex, 0)); SetCachedDepth(depthData2, int2(cacheIndex, 2)); SetCachedDepth(depthData3, int2(cacheIndex, 4)); } { int extIndex = cacheIndex + THREAD_COUNT; GetIndexedOffset(extIndex, absDir, xMajor, offset1, offset2, offset3); depthVal1 = LoadDepthTexture(groupStartSS + offset1 * signDir); depthVal2 = LoadDepthTexture(groupStartSS + offset2 * signDir); depthVal3 = LoadDepthTexture(groupStartSS + offset3 * signDir); depthData1.depth = depthVal1; depthData1.linearDepth = LinearEyeDepth(depthVal1, _ZBufferParams); depthData2.depth = depthVal2; depthData2.linearDepth = LinearEyeDepth(depthVal2, _ZBufferParams); depthData3.depth = depthVal3; depthData3.linearDepth = LinearEyeDepth(depthVal3, _ZBufferParams); SetCachedDepth(depthData1, int2(cacheIndex, 1)); SetCachedDepth(depthData2, int2(cacheIndex, 3)); SetCachedDepth(depthData3, int2(cacheIndex, 5)); } } [numthreads(1, THREAD_COUNT, 1)] void ContactShadowPoint(uint3 id : SV_DISPATCHTHREADID) { float2 lightPosSS = _LightPosSS; int2 iLightPosSS = int2(floor(lightPosSS + 0.5f)); int dispatchIndex; DispatchParams dispatchParams; int dispatchType = GetDispatchType(id.x, dispatchIndex, dispatchParams); int2 iLightPosOffset; int2 dispatchDirection = GetDispatchDirection(dispatchType, iLightPosOffset); int groupIndex; int2 dispatchOffset = GetDispatchOffset(dispatchType, dispatchIndex, dispatchParams, groupIndex); int2 iGroupStartSS = iLightPosSS + iLightPosOffset + dispatchDirection * dispatchOffset; CacheDepth(iGroupStartSS, lightPosSS, id.y); GroupMemoryBarrierWithGroupSync(); float2 toLight = lightPosSS - (float2(iGroupStartSS) + 0.5f); float2 absDir = abs(toLight); int2 signDir = sign(toLight); bool xMajor = absDir.x \u0026gt;= absDir.y; float2 absNDir = normalize(absDir); float absToLightStepRatio = xMajor ? absDir.y / absDir.x : absDir.x / absDir.y; int baseOffsetY = int(float(id.y) * absToLightStepRatio + 0.5f); int2 iOffset = xMajor ? int2(id.y, baseOffsetY) : int2(baseOffsetY, id.y); int2 iPosSS = iGroupStartSS + iOffset * signDir; if (any(iPosSS \u0026lt; int2(0, 0)) || any(iPosSS \u0026gt;= int2(_TextureSize.xy))) return; float2 posSS = float2(iPosSS) + 0.5f; float2 toPosSS = posSS - lightPosSS; float2 absToPos = abs(toPosSS); float absToPosStepRatio = xMajor ? absToPos.y / absToPos.x : absToPos.x / absToPos.y; int yIntersect = int(float(id.y) * absToPosStepRatio + 0.5f); int yVal = baseOffsetY; if (yIntersect != yVal) return; float lenToPosSS = xMajor ? absToPos.x : absToPos.y; DepthData thisDepth = GetCachedDepth(int2(id.y, 1)); float lightLinearDepth = LinearEyeDepth(_LightPosCS.z / _LightPosCS.w, _ZBufferParams); float shadow = 1.0f; float3 positionNDC = float3(posSS*_TextureSize.zw*2.0f-1.0f, thisDepth.depth); positionNDC.y = -positionNDC.y; float4 positionWS = mul(UNITY_MATRIX_I_VP, float4(positionNDC, 1.0f)); positionWS.xyz /= positionWS.w; float lenToLightWS = length(positionWS.xyz - _LightPosWS); if (lenToLightWS \u0026gt; _LightRange) { _RW_TargetTex[iPosSS] = float4(0.0f, 0.0f, 0.0f, 1.0f); return; } for (int i=1; i\u0026lt;THREAD_COUNT; ++i) { if (i\u0026gt;=lenToPosSS) break; float baseOffsetY0 = (i + id.y) * absToLightStepRatio; int iBaseStartY0 = int(floor(baseOffsetY0)) + (frac(baseOffsetY0)\u0026lt;=0.5f ? -1 : 0); float offsetY0 = i * absToPosStepRatio + baseOffsetY; int offsetStartY0 = int(floor(offsetY0)); int sampleIndex0 = offsetStartY0 - iBaseStartY0; DepthData depthData0 = GetCachedDepth(int2(id.y + i, sampleIndex0)); DepthData depthData1 = GetCachedDepth(int2(id.y + i, sampleIndex0 + 1)); float weightY0 = frac(offsetY0); if (abs(depthData0.linearDepth-depthData1.linearDepth) \u0026gt;(min(depthData0.linearDepth, depthData1.linearDepth) * _ThicknessParams.y + _ThicknessParams.x)) { weightY0 = step(0.5f, weightY0); } float interpolatedLinearDepth = 1.0f / lerp(1.0f/depthData0.linearDepth, 1.0f/depthData1.linearDepth, weightY0); float estimatedDepth = 1.0f / lerp(1.0f/thisDepth.linearDepth, 1.0f/lightLinearDepth, clamp(i/lenToPosSS, 0.0f, 1.0f)); if (estimatedDepth\u0026gt;interpolatedLinearDepth \u0026amp;\u0026amp; ((estimatedDepth-interpolatedLinearDepth)\u0026lt;interpolatedLinearDepth*_ThicknessParams.y+_ThicknessParams.x)) { shadow = 0.0f; break; } } float3 color = shadow; _RW_TargetTex[iPosSS] = float4(color, 1.0f); } [numthreads(8, 8, 1)] void ClearMain(uint3 id : SV_DISPATCHTHREADID) { _RW_TargetTex[id.xy] = 0.0f; } ContactShadowRenderPass.cs 和上一篇文章如出一辙，看上去只是把Radial Blur替换成了Contact Shadow，把center替换成了light。这里我们只考虑了点光源的接触阴影，平行光较为简单，所有点的采样方向都是同一个方向，聚光灯和点光源实际上是一样的。\nusing Unity.Mathematics; namespace UnityEngine.Rendering.Universal { public class ContactShadowRenderPass : ScriptableRenderPass { public static Light lightSource; private static readonly string passName = \u0026#34;Contact Shadow Render Pass\u0026#34;; private ScriptableRenderer renderer; private ContactShadowRendererFeature.ContactShadowSettings settings; private ContactShadow contactShadow; private ComputeShader computeShader; private Vector2Int textureSize; private static readonly string contactShadowTextureName = \u0026#34;_ContactShadowTexture\u0026#34;; private static readonly int contactShadowTextureID = Shader.PropertyToID(contactShadowTextureName); private RTHandle contactShadowTextureHandle; private ComputeBuffer computeBuffer; private static readonly int THREAD_COUNT = 128; private static readonly int DISPATCH_DATA_COUNT = 8; private static readonly int DISPATCH_DATA_STRIDE = 5; private static readonly int DISPATCH_DATA_SIZE = DISPATCH_DATA_COUNT * DISPATCH_DATA_STRIDE; private int[] dispatchData = new int[DISPATCH_DATA_SIZE]; public ContactShadowRenderPass(ContactShadowRendererFeature.ContactShadowSettings settings) { this.settings = settings; computeShader = settings.computeShader; renderPassEvent = settings.renderPassEvent; profilingSampler = new ProfilingSampler(passName); } public void Setup(ScriptableRenderer renderer, ContactShadow contactShadow) { this.renderer = renderer; this.contactShadow = contactShadow; } private void EnsureComputeBuffer(int count, int stride) { if (computeBuffer == null || computeBuffer.count != count || computeBuffer.stride != stride) { if (computeBuffer != null) { computeBuffer.Release(); } computeBuffer = new ComputeBuffer(count, stride, ComputeBufferType.Structured); } } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { EnsureComputeBuffer(DISPATCH_DATA_COUNT, DISPATCH_DATA_STRIDE * 4); RenderTextureDescriptor desc = renderingData.cameraData.cameraTargetDescriptor; textureSize = new Vector2Int(desc.width, desc.height); desc.enableRandomWrite = true; desc.graphicsFormat = Experimental.Rendering.GraphicsFormat.R16G16B16A16_SFloat; desc.depthBufferBits = 0; desc.msaaSamples = 1; desc.useMipMap = false; RenderingUtils.ReAllocateIfNeeded(ref contactShadowTextureHandle, desc, FilterMode.Point, TextureWrapMode.Clamp, false, 1, 0, contactShadowTextureName); ; } private Vector4 GetTextureSizeParameter(Vector2Int textureSize) { return new Vector4(textureSize.x, textureSize.y, 1.0f / textureSize.x, 1.0f / textureSize.y); } private struct DispatchParams { public int2 offset; public int count; public int stride; public int xMajor; public DispatchParams(int2 offset, int count, int stride, int xMajor) { this.offset = offset; this.count = count; this.stride = stride; this.xMajor = xMajor; } } private void GetDispatchParams(int2 coord, int2 offset, out DispatchParams dp1, out DispatchParams dp2) { int colIndexOffset = math.max(offset.x, offset.y) / THREAD_COUNT; int yIndexOffset; int minVal, maxVal, xMajor; if (coord.x \u0026gt;= coord.y) { minVal = coord.y; maxVal = coord.x; yIndexOffset = offset.y; xMajor = 1; } else { minVal = coord.x; maxVal = coord.y; yIndexOffset = offset.x; xMajor = 0; } int stride1 = math.max(0, (minVal + colIndexOffset + 1) * THREAD_COUNT - 1 - offset.x - offset.y); int count1 = stride1 * math.max(0, minVal - colIndexOffset); int stride2 = math.max(0, (minVal + 1) * THREAD_COUNT - yIndexOffset); int count2 = stride2 * math.max(0, maxVal - math.max(minVal, colIndexOffset)); dp1 = new DispatchParams(offset, count1, stride1, xMajor); dp2 = new DispatchParams(offset, count2, stride2, xMajor); } private void GetDispatchList(int2 iCenterPosSS, int2 textureSize, out DispatchParams[] dispatchList) { int2 offsetLB = math.max(0, iCenterPosSS - textureSize); int2 offsetRT = math.max(0, new int2(0, 0) - iCenterPosSS); int2 coordLB = (iCenterPosSS + THREAD_COUNT - 1) / THREAD_COUNT; int2 coordRT = (textureSize - iCenterPosSS + THREAD_COUNT - 1) / THREAD_COUNT; int2 coordRB = new int2(coordRT.x, coordLB.y); int2 coordLT = new int2(coordLB.x, coordRT.y); int2 offsetRB = new int2(offsetRT.x, offsetLB.y); int2 offsetLT = new int2(offsetLB.x, offsetRT.y); GetDispatchParams(coordLB, offsetLB, out DispatchParams dpLB1, out DispatchParams dpLB2); GetDispatchParams(coordLT, offsetLT, out DispatchParams dpLT1, out DispatchParams dpLT2); GetDispatchParams(coordRB, offsetRB, out DispatchParams dpRB1, out DispatchParams dpRB2); GetDispatchParams(coordRT, offsetRT, out DispatchParams dpRT1, out DispatchParams dpRT2); dispatchList = new DispatchParams[] { dpLB1, dpLB2, dpLT1, dpLT2, dpRB1, dpRB2, dpRT1, dpRT2 }; } private int SetDispatchData(DispatchParams[] dispatchList) { if (dispatchList.Length != 8) return 0; int totalCount = 0; for (int i = 0; i \u0026lt; 8; ++i) { var param = dispatchList[i]; totalCount += param.count; dispatchData[5 * i + 0] = param.offset.x; dispatchData[5 * i + 1] = param.offset.y; dispatchData[5 * i + 2] = totalCount; dispatchData[5 * i + 3] = param.stride; dispatchData[5 * i + 4] = param.xMajor; } computeBuffer.SetData(dispatchData); return totalCount; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = renderingData.commandBuffer; UniversalRenderer universalRenderer = renderer as UniversalRenderer; if (universalRenderer == null || computeShader == null || lightSource == null || lightSource.type != LightType.Point) return; using (new ProfilingScope(cmd, profilingSampler)) { float4 lightPosWS = new float4(lightSource.transform.position, 1.0f); float4x4 viewMat = renderingData.cameraData.GetViewMatrix(); float4x4 projMat = renderingData.cameraData.GetGPUProjectionMatrix(); float4x4 vpMat = math.mul(projMat, viewMat); float4 lightPosCS = math.mul(vpMat, lightPosWS); float3 lightPosNDC = lightPosCS.xyz / lightPosCS.w; lightPosNDC.y = -lightPosNDC.y; float2 lightPosSS = (lightPosNDC.xy * 0.5f + 0.5f) * new float2(textureSize.x, textureSize.y); int2 iLightPosSS = new int2(math.floor(lightPosSS + 0.5f)); int2 ts = new int2(textureSize.x, textureSize.y); GetDispatchList(iLightPosSS, ts, out DispatchParams[] dispatchList); int totalDispatchCount = SetDispatchData(dispatchList); var backBuffer = universalRenderer.m_ColorBufferSystem.GetBackBuffer(cmd); // int clearID = computeShader.FindKernel(\u0026#34;ClearMain\u0026#34;); // cmd.SetComputeTextureParam(computeShader, clearID, \u0026#34;_RW_TargetTex\u0026#34;, contactShadowTextureHandle); // computeShader.GetKernelThreadGroupSizes(clearID, out uint x1, out uint y1, out uint z1); // cmd.DispatchCompute(computeShader, clearID, // Mathf.CeilToInt((float)textureSize.x / x1), // Mathf.CeilToInt((float)textureSize.y / y1), // 1); int kernelID = computeShader.FindKernel(\u0026#34;ContactShadowPoint\u0026#34;); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_ColorTex\u0026#34;, backBuffer); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_RW_TargetTex\u0026#34;, contactShadowTextureHandle); cmd.SetComputeVectorParam(computeShader, \u0026#34;_LightPosWS\u0026#34;, lightPosWS); cmd.SetComputeVectorParam(computeShader, \u0026#34;_LightPosCS\u0026#34;, lightPosCS); cmd.SetComputeVectorParam(computeShader, \u0026#34;_LightPosSS\u0026#34;, new float4(lightPosSS, 0.0f, 0.0f)); cmd.SetComputeVectorParam(computeShader, \u0026#34;_ThicknessParams\u0026#34;, contactShadow.thicknessParams.value); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TextureSize\u0026#34;, GetTextureSizeParameter(textureSize)); cmd.SetComputeFloatParam(computeShader, \u0026#34;_LightRange\u0026#34;, lightSource.range); cmd.SetComputeFloatParam(computeShader, \u0026#34;_Debug\u0026#34;, contactShadow.debug.value); cmd.SetComputeBufferParam(computeShader, kernelID, \u0026#34;_DispatchData\u0026#34;, computeBuffer); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)totalDispatchCount / x), 1, 1); cmd.Blit(contactShadowTextureHandle, backBuffer); } } public void Dispose() { contactShadowTextureHandle?.Release(); if (computeBuffer != null) { computeBuffer.Release(); computeBuffer = null; } } } } ContactShadowRendererFeature.cs using System; namespace UnityEngine.Rendering.Universal { public class ContactShadowRendererFeature : ScriptableRendererFeature { [Serializable] public class ContactShadowSettings { public ComputeShader computeShader; public RenderPassEvent renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; } public ContactShadowSettings settings = new ContactShadowSettings(); private ContactShadowRenderPass contactShadowRenderPass; public override void Create() { contactShadowRenderPass = new ContactShadowRenderPass(settings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { ContactShadow cs = VolumeManager.instance.stack.GetComponent\u0026lt;ContactShadow\u0026gt;(); if (cs.IsActive()) { contactShadowRenderPass.Setup(renderer, cs); renderer.EnqueuePass(contactShadowRenderPass); } } protected override void Dispose(bool disposing) { contactShadowRenderPass?.Dispose(); base.Dispose(disposing); } } } ContactShadow.cs using System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenuForRenderPipeline(\u0026#34;Post-processing/Contact Shadow\u0026#34;, typeof(UniversalRenderPipeline))] public sealed class ContactShadow : VolumeComponent, IPostProcessComponent { public BoolParameter isEnabled = new BoolParameter(false); public Vector2Parameter thicknessParams = new Vector2Parameter(new Vector2(0.1f, 0.02f)); public FloatParameter debug = new FloatParameter(0.0f); public bool IsActive() { return isEnabled.value; } public bool IsTileCompatible() =\u0026gt; false; } } ScreenSpaceContactShadowLightSource.cs using UnityEngine; [ExecuteAlways] [RequireComponent(typeof(Light))] public class ScreenSpaceContactShadowLightSource : MonoBehaviour { public static ScreenSpaceContactShadowLightSource Instance { get; private set; } private void OnEnable() { if (Instance == null) { Instance = this; UnityEngine.Rendering.Universal.ContactShadowRenderPass.lightSource = GetComponent\u0026lt;Light\u0026gt;(); } else { Debug.LogError(\u0026#34;Only one instance of ScreenSpaceContactShadowLightSource is allowed to exist at the same time.\u0026#34;); enabled = false; } } private void OnDisable() { if (Instance == this) { Instance = null; } } private void OnDestroy() { if (Instance == this ) { Instance = null; } } } 后记 摸了两周，但也没摸，但是确实有点没有动力了。Radial Dispatch系列估计到这里就告一段落了，这个屏幕空间接触阴影没有我之前想象中的效果那么好，之后会在草场、云、海洋、Radiance Cascade GI中间选一个来做吧，不过估计要很久了。本来封面是想放一个点光源在视差映射贴图上的接触阴影和平行光在远处山上森林的接触阴影的，不过有点懒得整了。2024虽然糟透了，但也还有一丝丝的好消息吧。\n","permalink":"https://zznewclear13.github.io/posts/calculate-screen-space-contact-shadow-using-compute-shader/","summary":"屏幕空间接触阴影 屏幕空间接触阴影是用来解决普通的阴影贴图精度不够的问题而提出来的一种通过深度图在屏幕空间计算阴影的方法。索尼的Bend Studio的Graham Aldridge在Sigraph 2023的索尼创作者大会上，介绍了往日不再（Days Gone）中计算屏幕空间接触阴影的方式，这里可以找到演示文稿和参考代码。\n本篇文章相当于是Radial Dispatch系列的第三篇文章了，与上一篇文章一样，这篇文章是基于径向分派Compute Shader中相关算法的实际应用，具体的缓存方式也可以参考上一篇文章使用Group Shared Memory加速径向模糊，这里就不再赘述了。实际上我发现了这样计算接触阴影的一个缺陷，就是不太好计算软阴影了，由于缓存的限制，随机采样只能在一个很小的范围内分布，基本上用不上了。由于使用的是屏幕空间的深度图的信息，加上厚度检测之后很容易出现漏面的问题，封面中的瑕疵也有一部分是来自于我的Relaxed Cone Step Mapping本身深度值的瑕疵，屏幕上半部分的阴影就好很多。这就当作是一个Proof of Concept吧，之后有机会的话再回来优化优化。\n本文使用的是Unity 2022.3.21f1，URP版本是14.0.10。\n具体的代码 ContactShadowComputeShader.compute 核心的代码来自于前一篇文章径向分派Compute Shader。前一篇文章在循环中是通过统一步长进行采样的，会采样到四个像素中间因此需要双线性插值，这次我们固定水平或者竖直方向的步长为一个像素，这样我们只需要在一个方向上进行线性插值了。由于深度和颜色信息是两种不同的信息，我们仅对距离很近的深度进行线性插值，对于距离较远的两个深度值，我们使用离采样点最近像素的深度值。至于如何判断深度远近，我使用了和屏幕空间反射中相同的_ThicknessParams，默认物体的厚度为linearSampleDepth * _Thickness.y + _Thickness.x。\n#pragma kernel ContactShadowPoint #pragma kernel ClearMain // #pragma warning(disable: 3556) #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #define THREAD_COUNT 128 Texture2D\u0026lt;float4\u0026gt; _ColorTex; Texture2D\u0026lt;float\u0026gt; _CameraDepthTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTex; float3 _LightPosWS; float4 _LightPosCS; float2 _LightPosSS; float2 _ThicknessParams; float4 _TextureSize; float _LightRange; float _Debug; struct DispatchParams { int2 offset; int count; int stride; int xMajor; }; StructuredBuffer\u0026lt;DispatchParams\u0026gt; _DispatchData; int GetDispatchType(int index, out int dispatchIndex, out DispatchParams dispatchParams) { for (int i=0; i\u0026lt;8; ++i) { dispatchParams = _DispatchData[i]; dispatchIndex = dispatchParams.","title":"使用Compute Shader计算屏幕空间接触阴影"},{"content":"径向模糊 在写了上一篇文章径向分派Compute Shader之后，很自然的就需要用径向模糊来检验我们的算法。\n径向模糊的效果在网上可以说是一找一大堆了，百分之九十五的教程，会使用一个很大的循环重复采样纹理，百分之四点九的教程，会使用多pass的方法减少采样的次数（当然这些数字是我瞎编的，我还留了百分之零点一，以防真的有人也用Compute Shader做了径向模糊）。这篇文章就来探讨如何使用Compute Shader中的Group Shared Memory来加速径向模糊的计算。我们的目标是在一个Compute Shader中实现超大模糊距离(甚至能超过128像素)的径向模糊。\n缓存颜色信息 照例我们要再看一遍索尼的Bend Studio的Graham Aldridge在Sigraph 2023的索尼创作者大会上，介绍的往日不再（Days Gone）中计算屏幕空间接触阴影的方式，这里可以找到演示文稿和参考代码。演示文稿的第24-25页介绍了如何在Group Shared Memory中缓存深度信息，当然在径向模糊中我们缓存的是颜色信息了。\n下图是缓存颜色信息的示意图，绿色方块对应的像素是我们整个Thread Group的起点，红色方块是我们当前计算的像素，黑色和灰色的线代表了该像素到径向模糊中心的方向。我们的Thread Group Size是12，深蓝色（包括绿色和红色）的像素同属于一个Thread Group，我们需要计算这些像素径向模糊后的颜色。深蓝色、浅蓝色、灰色和浅灰色、镂空的灰色和浅灰色，代表了我们需要缓存的颜色，因为我们需要对缓存的颜色进行双线性插值，我们除了需要缓存射线经过的最近的像素（即蓝色区域）外，还需要缓存射线经过的第二近的像素（即灰色区域）和射线经过的第三近的像素（即镂空灰色区域）。也就是说当Thread Group Size为12时，我们需要缓存6 * 12个像素，亦即每个像素六次采样。\n仅缓存第二近的像素在大部分情况下能够得到正确的双线性插值，但是注意看红色方块向右的第二次采样，仅对蓝色和实心灰色区域的颜色进行插值是不能得到正确的颜色的，因此我们需要额外缓存镂空灰色区域的颜色，亦即第三近的像素。\nGroup Shared Memory分配 如上所述，我们的Group Shared Memory的大小是THREAD_COUNT * 6（懒得把THREAD_COUNT统一改成THREAD_GROUP_SIZE了），其中包含了上述的深蓝色、浅蓝色、灰色、浅灰色、镂空灰色、镂空浅灰色区域对应像素的颜色信息。数组的长度是THREAD_COUNT * 6，在我们缓存时，我们会将其视为6行THREAD_COUNT列的表来储存颜色信息，而在我们读取时，我们会将其视为3行THREAD_COUNT*2列的表来读取数据。\ngroupshared float3 cachedColor[THREAD_COUNT * 6]; void SetCachedColor(float3 color, int2 threadPos) {cachedColor[threadPos.x+threadPos.y*THREAD_COUNT]=color;} float3 GetCachedColor(int2 threadPos) {return cachedColor[threadPos.x+threadPos.y*THREAD_COUNT*2];} 具体的代码 RadialDispatchComputeShader.compute 核心的代码来自于上一篇文章径向分派Compute Shader。循环读取颜色值的方式有很多，可以严格按照格点来读取，这样只需要对y方向做线性插值，也可以按照统一的步长来读取，这样需要对xy方向都做线性插值，我这里使用的是统一的步长。每一组缓存的起始像素是最接近于射线的像素的下面一个像素，读取缓存使用的偏移是用的当前采样点下第一个像素的坐标和当前缓存列最下面像素的坐标相减，这里面比较绕不太好描述。。。\n#pragma kernel RadialBlurMain // #pragma warning(disable: 3556) #define THREAD_COUNT 128 Texture2D\u0026lt;float4\u0026gt; _ColorTex; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTex; float2 _CenterPosSS; float4 _TextureSize; float _Intensity; struct DispatchParams { int2 offset; int count; int stride; int xMajor; }; StructuredBuffer\u0026lt;DispatchParams\u0026gt; _DispatchData; int GetDispatchType(int index, out int dispatchIndex, out DispatchParams dispatchParams) { for (int i=0; i\u0026lt;8; ++i) { dispatchParams = _DispatchData[i]; dispatchIndex = dispatchParams.count - 1 - index; if (dispatchIndex \u0026gt;= 0) return i; } return 0; } int2 GetDispatchDirection(int dispatchType, out int2 iLightPosOffset) { dispatchType /= 2; int xDir = dispatchType / 2; int yDir = dispatchType % 2; int2 dir = int2(xDir, yDir); iLightPosOffset = dir - 1; return dir * 2 - 1; } int2 GetDispatchOffset(int dispatchType, int dispatchIndex, DispatchParams dispatchParams, out int groupIndex) { groupIndex = 0; int2 dispatchOffset = int2(0, 0); int offsetType = dispatchType % 2; int colIndexOffset = max(dispatchParams.offset.x,dispatchParams.offset.y)/THREAD_COUNT; int2 indexOffset = dispatchParams.xMajor==1?dispatchParams.offset:dispatchParams.offset.yx; int stride = dispatchParams.stride; int colIndex = dispatchIndex / stride; int rowIndex = dispatchIndex - colIndex * stride; if (offsetType == 0) { int offsetedColIndex = colIndex + colIndexOffset; int tempIndex = rowIndex + indexOffset.y - (offsetedColIndex + 1) * THREAD_COUNT; if (tempIndex \u0026gt;= 0) { dispatchOffset = int2(tempIndex + indexOffset.x, dispatchParams.stride - (colIndex + colIndexOffset + 1) * THREAD_COUNT + indexOffset.x + indexOffset.y); groupIndex = tempIndex; } else { dispatchOffset = int2((offsetedColIndex + 1) * THREAD_COUNT - 1, rowIndex + indexOffset.y); groupIndex = rowIndex; } } else { int minOffsetX = max(dispatchParams.stride + indexOffset.y, (colIndexOffset + 1) * THREAD_COUNT); dispatchOffset = int2(minOffsetX + colIndex * THREAD_COUNT - 1, rowIndex + indexOffset.y); groupIndex = rowIndex; } if (dispatchParams.xMajor == 0) dispatchOffset.xy = dispatchOffset.yx; return dispatchOffset; } void GetIndexedOffset(int index, float2 absDir, bool xMajor, out int2 offset1, out int2 offset2, out int2 offset3) { if (!xMajor) { absDir = absDir.yx; } float val = float(index) * absDir.y / absDir.x; float floorVal = floor(val); float fracVal = frac(val); if (fracVal \u0026lt;= 0.5f) { offset1 = int2(index, floorVal - 1.0f); offset2 = int2(index, floorVal); offset3 = int2(index, floorVal + 1.0f); } else { offset1 = int2(index, floorVal); offset2 = int2(index, floorVal + 1.0f); offset3 = int2(index, floorVal + 2.0f); } if (!xMajor) { offset1 = offset1.yx; offset2 = offset2.yx; offset3 = offset3.yx; } } float3 LoadColorTexture(int2 coord) { // coord.y = int(_TextureSize.y) - 1 - coord.y; coord = clamp(coord, int2(0, 0), int2(_TextureSize.xy - 1.0f)); return _ColorTex.Load(uint3(coord, 0)).rgb; } groupshared float3 cachedColor[THREAD_COUNT * 6]; void SetCachedColor(float3 color, int2 threadPos) {cachedColor[threadPos.x+threadPos.y*THREAD_COUNT]=color;} float3 GetCachedColor(int2 threadPos) {return cachedColor[threadPos.x+threadPos.y*THREAD_COUNT*2];} void CacheColor(int2 groupStartSS, float2 centerPosSS, int cacheIndex) { float2 toCenter = centerPosSS - (groupStartSS + 0.5f); float2 absDir = abs(toCenter); int2 signDir = int2(sign(toCenter)); bool xMajor = absDir.x \u0026gt;= absDir.y; float3 colorVal1, colorVal2, colorVal3; int2 offset1, offset2, offset3; { GetIndexedOffset(cacheIndex, absDir, xMajor, offset1, offset2, offset3); colorVal1 = LoadColorTexture(groupStartSS + offset1 * signDir); colorVal2 = LoadColorTexture(groupStartSS + offset2 * signDir); colorVal3 = LoadColorTexture(groupStartSS + offset3 * signDir); SetCachedColor(colorVal1, int2(cacheIndex, 0)); SetCachedColor(colorVal2, int2(cacheIndex, 2)); SetCachedColor(colorVal3, int2(cacheIndex, 4)); } { int extIndex = cacheIndex + THREAD_COUNT; GetIndexedOffset(extIndex, absDir, xMajor, offset1, offset2, offset3); colorVal1 = LoadColorTexture(groupStartSS + offset1 * signDir); colorVal2 = LoadColorTexture(groupStartSS + offset2 * signDir); colorVal3 = LoadColorTexture(groupStartSS + offset3 * signDir); SetCachedColor(colorVal1, int2(cacheIndex, 1)); SetCachedColor(colorVal2, int2(cacheIndex, 3)); SetCachedColor(colorVal3, int2(cacheIndex, 5)); } } [numthreads(1, THREAD_COUNT, 1)] void RadialBlurMain(uint3 id : SV_DISPATCHTHREADID) { float2 centerPosSS = _CenterPosSS; int2 iCenterPosSS = int2(floor(centerPosSS + 0.5f)); int dispatchIndex; DispatchParams dispatchParams; int dispatchType = GetDispatchType(id.x, dispatchIndex, dispatchParams); int2 iCenterPosOffset; int2 dispatchDirection = GetDispatchDirection(dispatchType, iCenterPosOffset); int groupIndex; int2 dispatchOffset = GetDispatchOffset(dispatchType, dispatchIndex, dispatchParams, groupIndex); int2 iGroupStartSS = iCenterPosSS + iCenterPosOffset + dispatchDirection * dispatchOffset; CacheColor(iGroupStartSS, centerPosSS, id.y); GroupMemoryBarrierWithGroupSync(); float2 toCenter = centerPosSS - (float2(iGroupStartSS) + 0.5f); float2 absDir = abs(toCenter); int2 signDir = sign(toCenter); bool xMajor = absDir.x \u0026gt;= absDir.y; float2 absNDir = normalize(absDir); float absToCenterStepRatio = xMajor ? absDir.y / absDir.x : absDir.x / absDir.y; int baseOffsetY = int(float(id.y) * absToCenterStepRatio + 0.5f); int2 iOffset = xMajor ? int2(id.y, baseOffsetY) : int2(baseOffsetY, id.y); int2 iPosSS = iGroupStartSS + iOffset * signDir; if (any(iPosSS \u0026lt; int2(0, 0)) || any(iPosSS \u0026gt;= int2(_TextureSize.xy))) return; float2 posSS = float2(iPosSS) + 0.5f; float2 toPosSS = posSS - centerPosSS; float2 absToPos = abs(toPosSS); float absToPosStepRatio = xMajor ? absToPos.y / absToPos.x : absToPos.x / absToPos.y; int yIntersect = int(float(id.y) * absToPosStepRatio + 0.5f); int yVal = baseOffsetY; if (yIntersect != yVal) return; float2 marchDir = normalize(absToPos); float lenToPosSS = length(toPosSS); float3 color = float3(0.0f, 0.0f, 0.0f); float weight = 0.0f; for (int i=0; i\u0026lt;THREAD_COUNT; ++i) { float intensity = float(i) * min(1.0f, lenToPosSS * _TextureSize.w * _Intensity); float2 sampleOffset = marchDir * intensity; if (length(sampleOffset) \u0026gt; lenToPosSS) continue; float xOffset = xMajor ? sampleOffset.x : sampleOffset.y; float floorXOffset = floor(xOffset); float fracXOffset = frac(xOffset); float baseOffsetY0 = (floorXOffset + id.y) * absToCenterStepRatio; int iBaseStartY0 = int(floor(baseOffsetY0)) + (frac(baseOffsetY0)\u0026lt;=0.5f ? -1 : 0); float offsetY0 = floorXOffset * absToPosStepRatio + baseOffsetY; int offsetStartY0 = int(floor(offsetY0)); int sampleIndex0 = offsetStartY0 - iBaseStartY0; float3 col00 = GetCachedColor(int2(id.y + floorXOffset, sampleIndex0)); float3 col01 = GetCachedColor(int2(id.y + floorXOffset, sampleIndex0 + 1)); float weightY0 = frac(offsetY0); float baseOffsetY1 = (floorXOffset + 1.0f + id.y) * absToCenterStepRatio; int iBaseStartY1 = int(floor(baseOffsetY1)) + (frac(baseOffsetY1)\u0026lt;=0.5f ? -1 : 0); float offsetY1 = (floorXOffset + 1.0f) * absToPosStepRatio + baseOffsetY; int offsetStartY1 = int(floor(offsetY1)); int sampleIndex1 = offsetStartY1 - iBaseStartY1; float3 col10 = GetCachedColor(int2(id.y + floorXOffset + 1.0f, sampleIndex1)); float3 col11 = GetCachedColor(int2(id.y + floorXOffset + 1.0f, sampleIndex1 + 1)); float weightY1 = frac(offsetY1); color += lerp(lerp(col00, col01, weightY0), lerp(col10, col11, weightY1), fracXOffset); weight += 1.0f; } float3 thisColor = GetCachedColor(int2(id.y, 0)); color /= weight; _RW_TargetTex[iPosSS] = float4(color, 1.0f); } RadialBlurRenderPass.cs 和上一篇文章如出一辙，看上去只是把Radial Dispatch替换成了Radial Blur。\nusing Unity.Mathematics; namespace UnityEngine.Rendering.Universal { public class RadialBlurRenderPass : ScriptableRenderPass { public static Transform centerTrans; private static readonly string passName = \u0026#34;Radial Blur Render Pass\u0026#34;; private ScriptableRenderer renderer; private RadialBlurRendererFeature.RadialBlurSettings settings; private RadialBlur radialBlur; private ComputeShader computeShader; private Vector2Int textureSize; private static readonly string radialBlurTextureName = \u0026#34;_RadialBlurTexture\u0026#34;; private static readonly int radialBlurTextureID = Shader.PropertyToID(radialBlurTextureName); private RTHandle radialBlurTextureHandle; private ComputeBuffer computeBuffer; private static readonly int THREAD_COUNT = 128; private static readonly int DISPATCH_DATA_COUNT = 8; private static readonly int DISPATCH_DATA_STRIDE = 5; private static readonly int DISPATCH_DATA_SIZE = DISPATCH_DATA_COUNT * DISPATCH_DATA_STRIDE; private int[] dispatchData = new int[DISPATCH_DATA_SIZE]; public RadialBlurRenderPass(RadialBlurRendererFeature.RadialBlurSettings settings) { this.settings = settings; computeShader = settings.computeShader; renderPassEvent = settings.renderPassEvent; profilingSampler = new ProfilingSampler(passName); } public void Setup(ScriptableRenderer renderer, RadialBlur radialBlur) { this.renderer = renderer; this.radialBlur = radialBlur; } private void EnsureComputeBuffer(int count, int stride) { if (computeBuffer == null || computeBuffer.count != count || computeBuffer.stride != stride) { if (computeBuffer != null) { computeBuffer.Release(); } computeBuffer = new ComputeBuffer(count, stride, ComputeBufferType.Structured); } } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { EnsureComputeBuffer(DISPATCH_DATA_COUNT, DISPATCH_DATA_STRIDE * 4); RenderTextureDescriptor desc = renderingData.cameraData.cameraTargetDescriptor; textureSize = new Vector2Int(desc.width, desc.height); desc.enableRandomWrite = true; desc.graphicsFormat = Experimental.Rendering.GraphicsFormat.R16G16B16A16_SFloat; desc.depthBufferBits = 0; desc.msaaSamples = 1; desc.useMipMap = false; RenderingUtils.ReAllocateIfNeeded(ref radialBlurTextureHandle, desc, FilterMode.Point, TextureWrapMode.Clamp, false, 1, 0, radialBlurTextureName); ; } private Vector4 GetTextureSizeParameter(Vector2Int textureSize) { return new Vector4(textureSize.x, textureSize.y, 1.0f / textureSize.x, 1.0f / textureSize.y); } private struct DispatchParams { public int2 offset; public int count; public int stride; public int xMajor; public DispatchParams(int2 offset, int count, int stride, int xMajor) { this.offset = offset; this.count = count; this.stride = stride; this.xMajor = xMajor; } } private void GetDispatchParams(int2 coord, int2 offset, out DispatchParams dp1, out DispatchParams dp2) { int colIndexOffset = math.max(offset.x, offset.y) / THREAD_COUNT; int yIndexOffset; int minVal, maxVal, xMajor; if (coord.x \u0026gt;= coord.y) { minVal = coord.y; maxVal = coord.x; yIndexOffset = offset.y; xMajor = 1; } else { minVal = coord.x; maxVal = coord.y; yIndexOffset = offset.x; xMajor = 0; } int stride1 = math.max(0, (minVal + colIndexOffset + 1) * THREAD_COUNT - 1 - offset.x - offset.y); int count1 = stride1 * math.max(0, minVal - colIndexOffset); int stride2 = math.max(0, (minVal + 1) * THREAD_COUNT - yIndexOffset); int count2 = stride2 * math.max(0, maxVal - math.max(minVal, colIndexOffset)); dp1 = new DispatchParams(offset, count1, stride1, xMajor); dp2 = new DispatchParams(offset, count2, stride2, xMajor); } private void GetDispatchList(int2 iCenterPosSS, int2 textureSize, out DispatchParams[] dispatchList) { int2 offsetLB = math.max(0, iCenterPosSS - textureSize); int2 offsetRT = math.max(0, new int2(0, 0) - iCenterPosSS); int2 coordLB = (iCenterPosSS + THREAD_COUNT - 1) / THREAD_COUNT; int2 coordRT = (textureSize - iCenterPosSS + THREAD_COUNT - 1) / THREAD_COUNT; int2 coordRB = new int2(coordRT.x, coordLB.y); int2 coordLT = new int2(coordLB.x, coordRT.y); int2 offsetRB = new int2(offsetRT.x, offsetLB.y); int2 offsetLT = new int2(offsetLB.x, offsetRT.y); GetDispatchParams(coordLB, offsetLB, out DispatchParams dpLB1, out DispatchParams dpLB2); GetDispatchParams(coordLT, offsetLT, out DispatchParams dpLT1, out DispatchParams dpLT2); GetDispatchParams(coordRB, offsetRB, out DispatchParams dpRB1, out DispatchParams dpRB2); GetDispatchParams(coordRT, offsetRT, out DispatchParams dpRT1, out DispatchParams dpRT2); dispatchList = new DispatchParams[] { dpLB1, dpLB2, dpLT1, dpLT2, dpRB1, dpRB2, dpRT1, dpRT2 }; } private int SetDispatchData(DispatchParams[] dispatchList) { if (dispatchList.Length != 8) return 0; int totalCount = 0; for (int i = 0; i \u0026lt; 8; ++i) { var param = dispatchList[i]; totalCount += param.count; dispatchData[5 * i + 0] = param.offset.x; dispatchData[5 * i + 1] = param.offset.y; dispatchData[5 * i + 2] = totalCount; dispatchData[5 * i + 3] = param.stride; dispatchData[5 * i + 4] = param.xMajor; } computeBuffer.SetData(dispatchData); return totalCount; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = renderingData.commandBuffer; UniversalRenderer universalRenderer = renderer as UniversalRenderer; if (universalRenderer == null || computeShader == null || centerTrans == null) return; using (new ProfilingScope(cmd, profilingSampler)) { float4 centerPosWS = new float4(centerTrans.position, 1.0f); float4x4 viewMat = renderingData.cameraData.GetViewMatrix(); float4x4 projMat = renderingData.cameraData.GetGPUProjectionMatrix(); float4x4 vpMat = math.mul(projMat, viewMat); float4 centerPosCS = math.mul(vpMat, centerPosWS); centerPosCS.xyz /= math.abs(centerPosCS.w); centerPosCS.y = -centerPosCS.y; float2 centerPosSS = (centerPosCS.xy * 0.5f + 0.5f) * new float2(textureSize.x, textureSize.y); int2 iCenterPosSS = new int2(math.floor(centerPosSS + 0.5f)); int2 ts = new int2(textureSize.x, textureSize.y); GetDispatchList(iCenterPosSS, ts, out DispatchParams[] dispatchList); int totalDispatchCount = SetDispatchData(dispatchList); var backBuffer = universalRenderer.m_ColorBufferSystem.GetBackBuffer(cmd); // int clearID = computeShader.FindKernel(\u0026#34;ClearMain\u0026#34;); // cmd.SetComputeTextureParam(computeShader, clearID, \u0026#34;_RW_TargetTex\u0026#34;, radialBlurTextureHandle); // computeShader.GetKernelThreadGroupSizes(clearID, out uint x1, out uint y1, out uint z1); // cmd.DispatchCompute(computeShader, clearID, // Mathf.CeilToInt((float)textureSize.x / x1), // Mathf.CeilToInt((float)textureSize.y / y1), // 1); int kernelID = computeShader.FindKernel(\u0026#34;RadialBlurMain\u0026#34;); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_ColorTex\u0026#34;, backBuffer); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_RW_TargetTex\u0026#34;, radialBlurTextureHandle); cmd.SetComputeVectorParam(computeShader, \u0026#34;_CenterPosSS\u0026#34;, new float4(centerPosSS, 0.0f, 0.0f)); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TextureSize\u0026#34;, GetTextureSizeParameter(textureSize)); cmd.SetComputeFloatParam(computeShader, \u0026#34;_Intensity\u0026#34;, radialBlur.intensity.value); cmd.SetComputeBufferParam(computeShader, kernelID, \u0026#34;_DispatchData\u0026#34;, computeBuffer); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)totalDispatchCount / x), 1, 1); cmd.Blit(radialBlurTextureHandle, backBuffer); } } public void Dispose() { radialBlurTextureHandle?.Release(); if (computeBuffer != null) { computeBuffer.Release(); computeBuffer = null; } } } } RadialBlurRendererFeature.cs using System; namespace UnityEngine.Rendering.Universal { public class RadialBlurRendererFeature : ScriptableRendererFeature { [Serializable] public class RadialBlurSettings { public ComputeShader computeShader; public RenderPassEvent renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; } public RadialBlurSettings settings = new RadialBlurSettings(); private RadialBlurRenderPass radialBlurRenderPass; public override void Create() { radialBlurRenderPass = new RadialBlurRenderPass(settings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { RadialBlur rb = VolumeManager.instance.stack.GetComponent\u0026lt;RadialBlur\u0026gt;(); if (rb.IsActive()) { radialBlurRenderPass.Setup(renderer, rb); renderer.EnqueuePass(radialBlurRenderPass); } } protected override void Dispose(bool disposing) { radialBlurRenderPass?.Dispose(); base.Dispose(disposing); } } } RadialBlur.cs using System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenuForRenderPipeline(\u0026#34;Post-processing/Radial Blur\u0026#34;, typeof(UniversalRenderPipeline))] public sealed class RadialBlur : VolumeComponent, IPostProcessComponent { public BoolParameter isEnabled = new BoolParameter(false); public ClampedFloatParameter intensity = new ClampedFloatParameter(1.0f, 0.0f, 1.0f); public bool IsActive() { return isEnabled.value \u0026amp;\u0026amp; intensity.value \u0026gt; 0.0f; } public bool IsTileCompatible() =\u0026gt; false; } } RadialBlurCenter.cs using UnityEngine; [ExecuteAlways] public class RadialBlurCenter : MonoBehaviour { public static RadialBlurCenter Instance { get; private set; } private void OnEnable() { if (Instance == null) { Instance = this; UnityEngine.Rendering.Universal.RadialBlurRenderPass.centerTrans = this.transform; } else { Debug.LogError(\u0026#34;Only one instance of RadialBlurCenter is allowed to exist at the same time.\u0026#34;); enabled = false; } } private void OnDisable() { if (Instance == this) { Instance = null; UnityEngine.Rendering.Universal.RadialBlurRenderPass.centerTrans = null; } } private void OnDestroy() { if (Instance == this) { Instance = null; UnityEngine.Rendering.Universal.RadialBlurRenderPass.centerTrans = null; } } } 后记 非常快地做完了径向模糊的效果，完成了径向分派Compute Shader之后简直就如顺水推舟一般。别的似乎也没啥好说的了，因为使用了新版本的Unity，懒得再复制之前祖传的模糊场景文件了，现场搭了一个。我还挺喜欢Unity URP的这张写着UP的图片的，可惜新版本不再提供了。2024年4月13日更新了，缓存前三近像素以进行双线性插值的逻辑，我自己看下来感觉似乎没什么明显的错误。\n","permalink":"https://zznewclear13.github.io/posts/accelerate-radial-blur-using-group-shared-memory/","summary":"径向模糊 在写了上一篇文章径向分派Compute Shader之后，很自然的就需要用径向模糊来检验我们的算法。\n径向模糊的效果在网上可以说是一找一大堆了，百分之九十五的教程，会使用一个很大的循环重复采样纹理，百分之四点九的教程，会使用多pass的方法减少采样的次数（当然这些数字是我瞎编的，我还留了百分之零点一，以防真的有人也用Compute Shader做了径向模糊）。这篇文章就来探讨如何使用Compute Shader中的Group Shared Memory来加速径向模糊的计算。我们的目标是在一个Compute Shader中实现超大模糊距离(甚至能超过128像素)的径向模糊。\n缓存颜色信息 照例我们要再看一遍索尼的Bend Studio的Graham Aldridge在Sigraph 2023的索尼创作者大会上，介绍的往日不再（Days Gone）中计算屏幕空间接触阴影的方式，这里可以找到演示文稿和参考代码。演示文稿的第24-25页介绍了如何在Group Shared Memory中缓存深度信息，当然在径向模糊中我们缓存的是颜色信息了。\n下图是缓存颜色信息的示意图，绿色方块对应的像素是我们整个Thread Group的起点，红色方块是我们当前计算的像素，黑色和灰色的线代表了该像素到径向模糊中心的方向。我们的Thread Group Size是12，深蓝色（包括绿色和红色）的像素同属于一个Thread Group，我们需要计算这些像素径向模糊后的颜色。深蓝色、浅蓝色、灰色和浅灰色、镂空的灰色和浅灰色，代表了我们需要缓存的颜色，因为我们需要对缓存的颜色进行双线性插值，我们除了需要缓存射线经过的最近的像素（即蓝色区域）外，还需要缓存射线经过的第二近的像素（即灰色区域）和射线经过的第三近的像素（即镂空灰色区域）。也就是说当Thread Group Size为12时，我们需要缓存6 * 12个像素，亦即每个像素六次采样。\n仅缓存第二近的像素在大部分情况下能够得到正确的双线性插值，但是注意看红色方块向右的第二次采样，仅对蓝色和实心灰色区域的颜色进行插值是不能得到正确的颜色的，因此我们需要额外缓存镂空灰色区域的颜色，亦即第三近的像素。\nGroup Shared Memory分配 如上所述，我们的Group Shared Memory的大小是THREAD_COUNT * 6（懒得把THREAD_COUNT统一改成THREAD_GROUP_SIZE了），其中包含了上述的深蓝色、浅蓝色、灰色、浅灰色、镂空灰色、镂空浅灰色区域对应像素的颜色信息。数组的长度是THREAD_COUNT * 6，在我们缓存时，我们会将其视为6行THREAD_COUNT列的表来储存颜色信息，而在我们读取时，我们会将其视为3行THREAD_COUNT*2列的表来读取数据。\ngroupshared float3 cachedColor[THREAD_COUNT * 6]; void SetCachedColor(float3 color, int2 threadPos) {cachedColor[threadPos.x+threadPos.y*THREAD_COUNT]=color;} float3 GetCachedColor(int2 threadPos) {return cachedColor[threadPos.x+threadPos.y*THREAD_COUNT*2];} 具体的代码 RadialDispatchComputeShader.compute 核心的代码来自于上一篇文章径向分派Compute Shader。循环读取颜色值的方式有很多，可以严格按照格点来读取，这样只需要对y方向做线性插值，也可以按照统一的步长来读取，这样需要对xy方向都做线性插值，我这里使用的是统一的步长。每一组缓存的起始像素是最接近于射线的像素的下面一个像素，读取缓存使用的偏移是用的当前采样点下第一个像素的坐标和当前缓存列最下面像素的坐标相减，这里面比较绕不太好描述。。。\n#pragma kernel RadialBlurMain // #pragma warning(disable: 3556) #define THREAD_COUNT 128 Texture2D\u0026lt;float4\u0026gt; _ColorTex; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTex; float2 _CenterPosSS; float4 _TextureSize; float _Intensity; struct DispatchParams { int2 offset; int count; int stride; int xMajor; }; StructuredBuffer\u0026lt;DispatchParams\u0026gt; _DispatchData; int GetDispatchType(int index, out int dispatchIndex, out DispatchParams dispatchParams) { for (int i=0; i\u0026lt;8; ++i) { dispatchParams = _DispatchData[i]; dispatchIndex = dispatchParams.","title":"使用Group Shared Memory加速径向模糊"},{"content":"动机 最直接的动机是我最近需要实现屏幕空间接触阴影了。索尼的Bend Studio的Graham Aldridge在Sigraph 2023的索尼创作者大会上，介绍了往日不再（Days Gone）中计算屏幕空间接触阴影的方式，这里可以找到演示文稿和参考代码。演示文稿的第24-27页，展示了一种新颖的分派Compute Shader的方法，传统的分派Compute Shader往往是将画面水平和竖直切分成像素数量为64倍数的小块，将分派的Compute Shader对应到这些小块上，而Days Gone中则是将分派的Compute Shader对应到呈放射状的像素小块上。大致的意思可以看下图，下图中相同颜色的相邻像素属于同一个thread group，左边是传统的分派方式，右边则是径向的分派方式。\n当进行径向模糊或是计算接触阴影时，往往需要沿着某个方向连续采样纹理。对于多次采样，我们一般会想到使用Compute Shader中的Group Shared Memory进行缓存从而减少采样次数。但是对特定方向进行缓存的话，会要缓存O((N+C)^2)个颜色，如果分派的Thread Group Size或是步进的次数比较大，很容易就超出了Group Shared Memory的最大限制。如果我们使用径向分派的方式，将每一个Thread Group对应的像素沿着采样的方向排列，算上线性插值也只需要缓存(N+C)*2个颜色，这样就能很方便地进行较远的步进了。\n相较于索尼的演示，本文解决了Thread Group对应的像素重叠的问题，也尽量地介绍了设置分派参数时的各种条件判断。本文使用的是Unity 2022.3.21f1，URP版本是14.0.10。\n如何进行径向分派 分派方式和原因 首先我们注意到对于屏幕中所有指向中心的射线，可以将其分为左下、左上、右下、右上四种，这四种射线最明显的是符号相反，因此在我们分派的时候可以分成四组数据，每一组数据使用同样的方式找到对应的偏移值，再乘上符号和中心的坐标相加，就能得到对应的像素坐标。\n因此我们只需要考虑一种情况，我们以右上角为例。下图是一个径向分派的示意图，绿色是我们的中心点，所有的Thread Group都会以绿点为中心放射状排布，黑框就是屏幕上中心点右上角对应的区域（为了简便这里选取了比较小的18x10像素），这里每四个相邻白色方框同属于一个Thread Group（更多的Thread Group我没有画出来），蓝色的区域是每一个Thread Group的起点，这里可以看到深蓝和浅蓝两种颜色，它们对应了两种分派的规律，一种是呈正方形的，另一种则是呈矩形的，灰色的区域是所有计算而得的每一个Thread对应的像素，为了让灰色的区域覆盖整个黑框的区域，我们需要做比当前像素更多的分派。\n直接计算每一个Thread对应的像素似乎有点困难，我们可以将分派分成两个维度，用第一个维度计算Thread Group的起点，即上图的蓝色区域，用第二个维度和Thread Group的起点，计算对应的像素的位置。因此我们分派的数据也就变成了一个GroupID和GroupIndex了。注意到浅蓝色的区域的位置决定于黑框的长宽比，当黑框的高大于长时，浅蓝色的区域会在深蓝色的上方且横向排布。我们可以做一个xMajor的判断，如果不是xMajor，我们就调换xy分量，全部计算完毕之后再换回来。\n根据图上的深蓝色和浅蓝色区域，我们会将两个区域分开来计算GroupID。比较简单的是浅蓝色的区域，从数学上我们需要传入每一列的列高，计算出GroupID的列序号和在一列中的序号，就能得到起点的坐标了。深蓝色的区域，如果单纯对每一圈求和的话，这是一个二次方程，虽然也能计算但效率肯定不会很高。我们可以考虑高斯求和的方法，将第一圈的竖向的像素和最后一圈的横向像素合并成一列（也就是图上深蓝色方框左上角图案相同的为同一列），这样得到的每一列的列高都是相同的，就能使用浅蓝色区域的方式计算序号了，之后我们再对比较序号的大小来决定是竖向的像素还是横向的像素。\n得到了Thread Group的起点坐标之后，我们只需要使用起点坐标到中心的向量，对X方向或Y方向以1为单位步进，再对另一个方向取最近的整数，就能得到当前Thread对应的像素相对于整个Thread Group起点坐标的偏移，两者相加就能得到最终的像素坐标了。\n事实上，我们的中心点有可能会在屏幕外部，这个时候上图就会变成这样，我们在计算列高的时候需要额外的考虑中心点的偏移，深蓝色的区域也不会考虑完全在屏幕外的圈。\n径向分派的额外参数 为了在Compute Shader中计算每个Thread对应的像素，我们需要从CPU额外传递一些参数。在径向分派中，我们从SV_DispatchThreadID中获取到的其实是GroupID和GroupIndex两个参数。由上面的讨论，我们将所有情况分为4 * 2种，即左下、左上、右下、右上、深蓝、浅蓝的组合，对于每一种组合我们需要知道总的数量，才能计算在每一种组合中的GroupID。根据我们上述的计算方式，我们还需要知道每一种组合对应的列高和xMajor的信息。为了兼容中心点在屏幕外的情况，我们还需要知道中心点的偏移值。这样我们的参数就是8组5个int值，分别对应偏移值X，偏移值Y，当前总Thread Group数，列高和xMajor，其中xMajor其实是一个布尔值可以封装到列高的第一位，这样就刚好是四个int值了，我们这里为了方便演示就不做这样的优化了。\nprivate struct DispatchParams { public int2 offset; public int count; public int stride; public int xMajor; public DispatchParams(int2 offset, int count, int stride, int xMajor) { this.offset = offset; this.count = count; this.stride = stride; this.xMajor = xMajor; } } 解决多个Thread对应同一个像素带来的闪烁 由于我们是从外部向内部步进，这必然会导致越靠近中心，越多的Thread会在同一个像素发生碰撞，在示意图中我们也能看到越靠近中心锯齿感越强烈。为了解决这个问题，我们需要从中心向当前Thread对应的像素发射射线，计算和这个射线最接近的Thread Group的起始像素。如果这个起始像素和当前Thread Group的起始像素相同，我们认为当前像素属于当前的Thread Group，保留这个像素，否则，当前像素属于别的Thread Group，我们跳过后续的填色。这样我们就能确保一个像素最多只会被一个Thread Group写入。\n由于此时我们屏幕上的像素并不一定总会被写入（尤其是我们写了什么bug的时候），建议在Debug时先对RenderTexture进行一次初始化为0的操作，本文也将ClearMain保留在Compute Shader中。\n具体的代码 指导思想就是上面所描述的了，但是实际实现的时候会被各种取模、取余、加一、减一搞得晕头转向。。。这边还稍做了优化，比较{从中心到当前Thread Group的射线的斜率和当前像素水平偏移值的乘积}和{从中心到当前像素的射线的斜率和当前像素水平偏移值的乘积}，从而快速地判断当前像素是否属于当前Thread Group。RadialDispatch即为径向分派的主函数，NormalDispatch为普通分派的主函数，通过对GroupID做哈希来可视化。\nRadialDispatchComputeShader.compute #pragma kernel RadialDispatch #pragma kernel NormalDispatch #pragma kernel ClearMain // #pragma warning(disable: 3556) #define THREAD_COUNT 128 Texture2D\u0026lt;float4\u0026gt; _ColorTex; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTex; float2 _CenterPosSS; float4 _TextureSize; struct DispatchParams { int2 offset; int count; int stride; int xMajor; }; StructuredBuffer\u0026lt;DispatchParams\u0026gt; _DispatchData; int GetDispatchType(int index, out int dispatchIndex, out DispatchParams dispatchParams) { for (int i=0; i\u0026lt;8; ++i) { dispatchParams = _DispatchData[i]; dispatchIndex = dispatchParams.count - 1 - index; if (dispatchIndex \u0026gt;= 0) return i; } return 0; } int2 GetDispatchDirection(int dispatchType, out int2 iLightPosOffset) { dispatchType /= 2; int xDir = dispatchType / 2; int yDir = dispatchType % 2; int2 dir = int2(xDir, yDir); iLightPosOffset = dir - 1; return dir * 2 - 1; } int2 GetDispatchOffset(int dispatchType, int dispatchIndex, DispatchParams dispatchParams, out int groupIndex) { groupIndex = 0; int2 dispatchOffset = int2(0, 0); int offsetType = dispatchType % 2; int colIndexOffset = max(dispatchParams.offset.x,dispatchParams.offset.y)/THREAD_COUNT; int2 indexOffset = dispatchParams.xMajor==1?dispatchParams.offset:dispatchParams.offset.yx; int stride = dispatchParams.stride; int colIndex = dispatchIndex / stride; int rowIndex = dispatchIndex - colIndex * stride; if (offsetType == 0) { int offsetedColIndex = colIndex + colIndexOffset; int tempIndex = rowIndex + indexOffset.y - (offsetedColIndex + 1) * THREAD_COUNT; if (tempIndex \u0026gt;= 0) { dispatchOffset = int2(tempIndex + indexOffset.x, dispatchParams.stride - (colIndex + colIndexOffset + 1) * THREAD_COUNT + indexOffset.x + indexOffset.y); groupIndex = tempIndex; } else { dispatchOffset = int2((offsetedColIndex + 1) * THREAD_COUNT - 1, rowIndex + indexOffset.y); groupIndex = rowIndex; } } else { int minOffsetX = max(dispatchParams.stride + indexOffset.y, (colIndexOffset + 1) * THREAD_COUNT); dispatchOffset = int2(minOffsetX + colIndex * THREAD_COUNT - 1, rowIndex + indexOffset.y); groupIndex = rowIndex; } if (dispatchParams.xMajor == 0) dispatchOffset.xy = dispatchOffset.yx; return dispatchOffset; } // https://www.shadertoy.com/view/4djSRW // 1 out, 1 in... float hash11(float p) { p = frac(p * .1031); p *= p + 33.33; p *= p + p; return frac(p); } // https://www.shadertoy.com/view/MsS3Wc // Smooth HSV to RGB conversion float3 hsv2rgb_smooth(float3 c) { float3 rgb = clamp(abs(fmod(c.x*6.0+float3(0.0,4.0,2.0),6.0)-3.0)-1.0, 0.0, 1.0); rgb = rgb*rgb*(3.0-2.0*rgb); // cubic smoothing\treturn c.z * lerp(float3(1.0, 1.0f, 1.0f), rgb, c.y); } [numthreads(1, THREAD_COUNT, 1)] void RadialDispatch(uint3 id : SV_DISPATCHTHREADID) { float2 centerPosSS = _CenterPosSS; int2 iCenterPosSS = int2(floor(centerPosSS + 0.5f)); int dispatchIndex; DispatchParams dispatchParams; int dispatchType = GetDispatchType(id.x, dispatchIndex, dispatchParams); int2 iCenterPosOffset; int2 dispatchDirection = GetDispatchDirection(dispatchType, iCenterPosOffset); int groupIndex; int2 dispatchOffset = GetDispatchOffset(dispatchType, dispatchIndex, dispatchParams, groupIndex); int2 iGroupStartSS = iCenterPosSS + iCenterPosOffset + dispatchDirection * dispatchOffset; float2 toCenter = centerPosSS - (float2(iGroupStartSS) + 0.5f); float2 absDir = abs(toCenter); int2 signDir = sign(toCenter); bool xMajor = absDir.x \u0026gt;= absDir.y; float2 absNDir = normalize(absDir); float absToCenterStepRatio = xMajor ? absDir.y / absDir.x : absDir.x / absDir.y; int baseOffsetY = int(float(id.y) * absToCenterStepRatio + 0.5f); int2 iOffset = xMajor ? int2(id.y, baseOffsetY) : int2(baseOffsetY, id.y); int2 iPosSS = iGroupStartSS + iOffset * signDir; if (any(iPosSS \u0026lt; int2(0, 0)) || any(iPosSS \u0026gt;= int2(_TextureSize.xy))) return; float2 posSS = float2(iPosSS) + 0.5f; float2 toPosSS = posSS - centerPosSS; float2 absToPos = abs(toPosSS); float absToPosStepRatio = xMajor ? absToPos.y / absToPos.x : absToPos.x / absToPos.y; int yIntersect = int(float(id.y) * absToPosStepRatio + 0.5f); int yVal = baseOffsetY; if (yIntersect != yVal) return; float rv1 = hash11(id.x); float3 color = hsv2rgb_smooth(float3(rv1, 0.8f, 1.0f)); _RW_TargetTex[iPosSS] = float4(color, 1.0f); } [numthreads(1, THREAD_COUNT, 1)] void NormalDispatch(uint3 groupID : SV_GroupID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { float rv1 = hash11(THREAD_COUNT * groupID.x + groupID.y + THREAD_COUNT * groupID.z); float3 color = hsv2rgb_smooth(float3(rv1, 0.8f, 1.0f)); _RW_TargetTex[dispatchThreadID.xy] = float4(color, 1.0f); } [numthreads(16, 16, 1)] void ClearMain(uint3 id : SV_DISPATCHTHREADID) { _RW_TargetTex[id.xy] = 0.0f; } RadialDispatchRenderPass.cs 很需要注意的是，在计算中心点最近的整数时，不能简单地使用int2 iCenterPosSS = new int2(centerPosSS + 0.5f);来计算，因为centerPosSS的分量很可能会小于0，转换为int时会变成最接近零的整数。\nusing Unity.Mathematics; namespace UnityEngine.Rendering.Universal { public class RadialDispatchRenderPass : ScriptableRenderPass { public static Transform centerTrans; private static readonly string passName = \u0026#34;Radial Dispatch Render Pass\u0026#34;; private ScriptableRenderer renderer; private RadialDispatchRendererFeature.RadialDispatchSettings settings; private RadialDispatch radialDispatch; private ComputeShader computeShader; private Vector2Int textureSize; private static readonly string radialDispatchTextureName = \u0026#34;_RadialDispatchTexture\u0026#34;; private static readonly int radialDispatchTextureID = Shader.PropertyToID(radialDispatchTextureName); private RTHandle radialDispatchTextureHandle; private ComputeBuffer computeBuffer; private static readonly int THREAD_COUNT = 128; private static readonly int DISPATCH_DATA_COUNT = 8; private static readonly int DISPATCH_DATA_STRIDE = 5; private static readonly int DISPATCH_DATA_SIZE = DISPATCH_DATA_COUNT * DISPATCH_DATA_STRIDE; private int[] dispatchData = new int[DISPATCH_DATA_SIZE]; public RadialDispatchRenderPass(RadialDispatchRendererFeature.RadialDispatchSettings settings) { this.settings = settings; computeShader = settings.computeShader; renderPassEvent = settings.renderPassEvent; profilingSampler = new ProfilingSampler(passName); } public void Setup(ScriptableRenderer renderer, RadialDispatch RadialDispatch) { this.renderer = renderer; this.radialDispatch = RadialDispatch; } private void EnsureComputeBuffer(int count, int stride) { if (computeBuffer == null || computeBuffer.count != count || computeBuffer.stride != stride) { if (computeBuffer != null) { computeBuffer.Release(); } computeBuffer = new ComputeBuffer(count, stride, ComputeBufferType.Structured); } } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { EnsureComputeBuffer(DISPATCH_DATA_COUNT, DISPATCH_DATA_STRIDE * 4); RenderTextureDescriptor desc = renderingData.cameraData.cameraTargetDescriptor; textureSize = new Vector2Int(desc.width, desc.height); desc.enableRandomWrite = true; desc.graphicsFormat = Experimental.Rendering.GraphicsFormat.R16G16B16A16_SFloat; desc.depthBufferBits = 0; desc.msaaSamples = 1; desc.useMipMap = false; RenderingUtils.ReAllocateIfNeeded(ref radialDispatchTextureHandle, desc, FilterMode.Point, TextureWrapMode.Clamp, false, 1, 0, radialDispatchTextureName); ; } private Vector4 GetTextureSizeParameter(Vector2Int textureSize) { return new Vector4(textureSize.x, textureSize.y, 1.0f / textureSize.x, 1.0f / textureSize.y); } private struct DispatchParams { public int2 offset; public int count; public int stride; public int xMajor; public DispatchParams(int2 offset, int count, int stride, int xMajor) { this.offset = offset; this.count = count; this.stride = stride; this.xMajor = xMajor; } } private void GetDispatchParams(int2 coord, int2 offset, out DispatchParams dp1, out DispatchParams dp2) { int colIndexOffset = math.max(offset.x, offset.y) / THREAD_COUNT; int yIndexOffset; int minVal, maxVal, xMajor; if (coord.x \u0026gt;= coord.y) { minVal = coord.y; maxVal = coord.x; yIndexOffset = offset.y; xMajor = 1; } else { minVal = coord.x; maxVal = coord.y; yIndexOffset = offset.x; xMajor = 0; } int stride1 = math.max(0, (minVal + colIndexOffset + 1) * THREAD_COUNT - 1 - offset.x - offset.y); int count1 = stride1 * math.max(0, minVal - colIndexOffset); int stride2 = math.max(0, (minVal + 1) * THREAD_COUNT - yIndexOffset); int count2 = stride2 * math.max(0, maxVal - math.max(minVal, colIndexOffset)); dp1 = new DispatchParams(offset, count1, stride1, xMajor); dp2 = new DispatchParams(offset, count2, stride2, xMajor); } private void GetDispatchList(int2 iCenterPosSS, int2 textureSize, out DispatchParams[] dispatchList) { int2 offsetLB = math.max(0, iCenterPosSS - textureSize); int2 offsetRT = math.max(0, new int2(0, 0) - iCenterPosSS); int2 coordLB = (iCenterPosSS + THREAD_COUNT - 1) / THREAD_COUNT; int2 coordRT = (textureSize - iCenterPosSS + THREAD_COUNT - 1) / THREAD_COUNT; int2 coordRB = new int2(coordRT.x, coordLB.y); int2 coordLT = new int2(coordLB.x, coordRT.y); int2 offsetRB = new int2(offsetRT.x, offsetLB.y); int2 offsetLT = new int2(offsetLB.x, offsetRT.y); GetDispatchParams(coordLB, offsetLB, out DispatchParams dpLB1, out DispatchParams dpLB2); GetDispatchParams(coordLT, offsetLT, out DispatchParams dpLT1, out DispatchParams dpLT2); GetDispatchParams(coordRB, offsetRB, out DispatchParams dpRB1, out DispatchParams dpRB2); GetDispatchParams(coordRT, offsetRT, out DispatchParams dpRT1, out DispatchParams dpRT2); dispatchList = new DispatchParams[] { dpLB1, dpLB2, dpLT1, dpLT2, dpRB1, dpRB2, dpRT1, dpRT2 }; } private int SetDispatchData(DispatchParams[] dispatchList) { if (dispatchList.Length != 8) return 0; int totalCount = 0; for (int i = 0; i \u0026lt; 8; ++i) { var param = dispatchList[i]; totalCount += param.count; dispatchData[5 * i + 0] = param.offset.x; dispatchData[5 * i + 1] = param.offset.y; dispatchData[5 * i + 2] = totalCount; dispatchData[5 * i + 3] = param.stride; dispatchData[5 * i + 4] = param.xMajor; } computeBuffer.SetData(dispatchData); return totalCount; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = renderingData.commandBuffer; UniversalRenderer universalRenderer = renderer as UniversalRenderer; if (universalRenderer == null || computeShader == null || centerTrans == null) return; using (new ProfilingScope(cmd, profilingSampler)) { float4 centerPosWS = new float4(centerTrans.position, 1.0f); float4x4 viewMat = renderingData.cameraData.GetViewMatrix(); float4x4 projMat = renderingData.cameraData.GetGPUProjectionMatrix(); float4x4 vpMat = math.mul(projMat, viewMat); float4 centerPosCS = math.mul(vpMat, centerPosWS); centerPosCS.xyz /= math.abs(centerPosCS.w); centerPosCS.y = -centerPosCS.y; float2 centerPosSS = (centerPosCS.xy * 0.5f + 0.5f) * new float2(textureSize.x, textureSize.y); int2 iCenterPosSS = new int2(math.floor(centerPosSS + 0.5f)); int2 ts = new int2(textureSize.x, textureSize.y); GetDispatchList(iCenterPosSS, ts, out DispatchParams[] dispatchList); int totalDispatchCount = SetDispatchData(dispatchList); var backBuffer = universalRenderer.m_ColorBufferSystem.GetBackBuffer(cmd); int clearID = computeShader.FindKernel(\u0026#34;ClearMain\u0026#34;); cmd.SetComputeTextureParam(computeShader, clearID, \u0026#34;_RW_TargetTex\u0026#34;, radialDispatchTextureHandle); computeShader.GetKernelThreadGroupSizes(clearID, out uint x1, out uint y1, out uint z1); cmd.DispatchCompute(computeShader, clearID, Mathf.CeilToInt((float)textureSize.x / x1), Mathf.CeilToInt((float)textureSize.y / y1), 1); if (radialDispatch.radialDispatch.value) { int kernelID = computeShader.FindKernel(\u0026#34;RadialDispatch\u0026#34;); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_ColorTex\u0026#34;, backBuffer); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_RW_TargetTex\u0026#34;, radialDispatchTextureHandle); cmd.SetComputeVectorParam(computeShader, \u0026#34;_CenterPosSS\u0026#34;, new float4(centerPosSS, 0.0f, 0.0f)); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TextureSize\u0026#34;, GetTextureSizeParameter(textureSize)); cmd.SetComputeBufferParam(computeShader, kernelID, \u0026#34;_DispatchData\u0026#34;, computeBuffer); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)totalDispatchCount / x), 1, 1); } else { int kernelID = computeShader.FindKernel(\u0026#34;NormalDispatch\u0026#34;); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_ColorTex\u0026#34;, backBuffer); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_RW_TargetTex\u0026#34;, radialDispatchTextureHandle); cmd.SetComputeVectorParam(computeShader, \u0026#34;_CenterPosSS\u0026#34;, new float4(centerPosSS, 0.0f, 0.0f)); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TextureSize\u0026#34;, GetTextureSizeParameter(textureSize)); cmd.SetComputeBufferParam(computeShader, kernelID, \u0026#34;_DispatchData\u0026#34;, computeBuffer); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)textureSize.x / x), Mathf.CeilToInt((float)textureSize.y / y), 1); } cmd.Blit(radialDispatchTextureHandle, backBuffer); } } public void Dispose() { radialDispatchTextureHandle?.Release(); if (computeBuffer != null) { computeBuffer.Release(); computeBuffer = null; } } } } RadialDispatchRendererFeature.cs using System; namespace UnityEngine.Rendering.Universal { public class RadialDispatchRendererFeature : ScriptableRendererFeature { [Serializable] public class RadialDispatchSettings { public ComputeShader computeShader; public RenderPassEvent renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; } public RadialDispatchSettings settings = new RadialDispatchSettings(); private RadialDispatchRenderPass radialDispatchRenderPass; public override void Create() { radialDispatchRenderPass = new RadialDispatchRenderPass(settings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { RadialDispatch rd = VolumeManager.instance.stack.GetComponent\u0026lt;RadialDispatch\u0026gt;(); if (rd.IsActive()) { radialDispatchRenderPass.Setup(renderer, rd); renderer.EnqueuePass(radialDispatchRenderPass); } } protected override void Dispose(bool disposing) { radialDispatchRenderPass?.Dispose(); base.Dispose(disposing); } } } RadialDispatch.cs using System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenuForRenderPipeline(\u0026#34;Post-processing/Radial Dispatch\u0026#34;, typeof(UniversalRenderPipeline))] public sealed class RadialDispatch : VolumeComponent, IPostProcessComponent { public BoolParameter isEnabled = new BoolParameter(false); public BoolParameter radialDispatch = new BoolParameter(true); public bool IsActive() { return isEnabled.value; } public bool IsTileCompatible() =\u0026gt; false; } } RadialDispatchCenter.cs using UnityEngine; [ExecuteAlways] public class RadialDispatchCenter : MonoBehaviour { public static RadialDispatchCenter Instance { get; private set; } private void OnEnable() { if (Instance == null) { Instance = this; UnityEngine.Rendering.Universal.RadialDispatchRenderPass.centerTrans = this.transform; } else { Debug.LogError(\u0026#34;Only one instance of RadialDispatchCenter is allowed to exist at the same time.\u0026#34;); enabled = false; } } private void OnDisable() { if (Instance == this) { Instance = null; UnityEngine.Rendering.Universal.RadialDispatchRenderPass.centerTrans = null; } } private void OnDestroy() { if (Instance == this) { Instance = null; UnityEngine.Rendering.Universal.RadialDispatchRenderPass.centerTrans = null; } } } 后记 又是头晕目眩的取模、取余、加一、减一，而且是极难debug的不规则Thread Group和像素对应的方式，好几次对着屏幕上黑色的区域发呆，但最终还是艰难的做了出来。但是最终的代码写的很抽象，就像我没有怎么看Bend Studio提供的参考代码一样，读者（如果有的话）也不会怎么看我写的代码吧。。。\n非常感谢Unity的Mathematics这个包，这个包极大地减少了我将同样的代码复制到C#中debug的工作量。但愿没有什么没查出来的bug，明天应该能写一个径向模糊的文章了，之后就是屏幕空间接触阴影了，在之后大概就能到草场的渲染了。\n","permalink":"https://zznewclear13.github.io/posts/dispatch-compute-shader-in-a-radial-way/","summary":"动机 最直接的动机是我最近需要实现屏幕空间接触阴影了。索尼的Bend Studio的Graham Aldridge在Sigraph 2023的索尼创作者大会上，介绍了往日不再（Days Gone）中计算屏幕空间接触阴影的方式，这里可以找到演示文稿和参考代码。演示文稿的第24-27页，展示了一种新颖的分派Compute Shader的方法，传统的分派Compute Shader往往是将画面水平和竖直切分成像素数量为64倍数的小块，将分派的Compute Shader对应到这些小块上，而Days Gone中则是将分派的Compute Shader对应到呈放射状的像素小块上。大致的意思可以看下图，下图中相同颜色的相邻像素属于同一个thread group，左边是传统的分派方式，右边则是径向的分派方式。\n当进行径向模糊或是计算接触阴影时，往往需要沿着某个方向连续采样纹理。对于多次采样，我们一般会想到使用Compute Shader中的Group Shared Memory进行缓存从而减少采样次数。但是对特定方向进行缓存的话，会要缓存O((N+C)^2)个颜色，如果分派的Thread Group Size或是步进的次数比较大，很容易就超出了Group Shared Memory的最大限制。如果我们使用径向分派的方式，将每一个Thread Group对应的像素沿着采样的方向排列，算上线性插值也只需要缓存(N+C)*2个颜色，这样就能很方便地进行较远的步进了。\n相较于索尼的演示，本文解决了Thread Group对应的像素重叠的问题，也尽量地介绍了设置分派参数时的各种条件判断。本文使用的是Unity 2022.3.21f1，URP版本是14.0.10。\n如何进行径向分派 分派方式和原因 首先我们注意到对于屏幕中所有指向中心的射线，可以将其分为左下、左上、右下、右上四种，这四种射线最明显的是符号相反，因此在我们分派的时候可以分成四组数据，每一组数据使用同样的方式找到对应的偏移值，再乘上符号和中心的坐标相加，就能得到对应的像素坐标。\n因此我们只需要考虑一种情况，我们以右上角为例。下图是一个径向分派的示意图，绿色是我们的中心点，所有的Thread Group都会以绿点为中心放射状排布，黑框就是屏幕上中心点右上角对应的区域（为了简便这里选取了比较小的18x10像素），这里每四个相邻白色方框同属于一个Thread Group（更多的Thread Group我没有画出来），蓝色的区域是每一个Thread Group的起点，这里可以看到深蓝和浅蓝两种颜色，它们对应了两种分派的规律，一种是呈正方形的，另一种则是呈矩形的，灰色的区域是所有计算而得的每一个Thread对应的像素，为了让灰色的区域覆盖整个黑框的区域，我们需要做比当前像素更多的分派。\n直接计算每一个Thread对应的像素似乎有点困难，我们可以将分派分成两个维度，用第一个维度计算Thread Group的起点，即上图的蓝色区域，用第二个维度和Thread Group的起点，计算对应的像素的位置。因此我们分派的数据也就变成了一个GroupID和GroupIndex了。注意到浅蓝色的区域的位置决定于黑框的长宽比，当黑框的高大于长时，浅蓝色的区域会在深蓝色的上方且横向排布。我们可以做一个xMajor的判断，如果不是xMajor，我们就调换xy分量，全部计算完毕之后再换回来。\n根据图上的深蓝色和浅蓝色区域，我们会将两个区域分开来计算GroupID。比较简单的是浅蓝色的区域，从数学上我们需要传入每一列的列高，计算出GroupID的列序号和在一列中的序号，就能得到起点的坐标了。深蓝色的区域，如果单纯对每一圈求和的话，这是一个二次方程，虽然也能计算但效率肯定不会很高。我们可以考虑高斯求和的方法，将第一圈的竖向的像素和最后一圈的横向像素合并成一列（也就是图上深蓝色方框左上角图案相同的为同一列），这样得到的每一列的列高都是相同的，就能使用浅蓝色区域的方式计算序号了，之后我们再对比较序号的大小来决定是竖向的像素还是横向的像素。\n得到了Thread Group的起点坐标之后，我们只需要使用起点坐标到中心的向量，对X方向或Y方向以1为单位步进，再对另一个方向取最近的整数，就能得到当前Thread对应的像素相对于整个Thread Group起点坐标的偏移，两者相加就能得到最终的像素坐标了。\n事实上，我们的中心点有可能会在屏幕外部，这个时候上图就会变成这样，我们在计算列高的时候需要额外的考虑中心点的偏移，深蓝色的区域也不会考虑完全在屏幕外的圈。\n径向分派的额外参数 为了在Compute Shader中计算每个Thread对应的像素，我们需要从CPU额外传递一些参数。在径向分派中，我们从SV_DispatchThreadID中获取到的其实是GroupID和GroupIndex两个参数。由上面的讨论，我们将所有情况分为4 * 2种，即左下、左上、右下、右上、深蓝、浅蓝的组合，对于每一种组合我们需要知道总的数量，才能计算在每一种组合中的GroupID。根据我们上述的计算方式，我们还需要知道每一种组合对应的列高和xMajor的信息。为了兼容中心点在屏幕外的情况，我们还需要知道中心点的偏移值。这样我们的参数就是8组5个int值，分别对应偏移值X，偏移值Y，当前总Thread Group数，列高和xMajor，其中xMajor其实是一个布尔值可以封装到列高的第一位，这样就刚好是四个int值了，我们这里为了方便演示就不做这样的优化了。\nprivate struct DispatchParams { public int2 offset; public int count; public int stride; public int xMajor; public DispatchParams(int2 offset, int count, int stride, int xMajor) { this.","title":"径向分派Compute Shader"},{"content":"动机 最近看到了三角洲行动介绍的在虚幻引擎4中实现的地形渲染方案，感觉受益匪浅。不过在Unity中要想实现一个即插即用的虚拟贴图的技术应该有些困难，于是我把目光放在了最一开始所介绍的对地形贴图做混合的方案上。\n三角洲行动提出的方案是这样的，在地形计算的时候，从对材质ID图的四个像素采样退化成对三个像素采样，这样一来既能减少地形混合的时候的采样次数，二来相较于采样四个像素，三个像素多了一个斜向45度的效果，能够减轻一些地形的块面感。不过他们也有语焉不详的地方，虽然只采样三个像素能够提供斜向45度，但是对于斜向-45度，仅使用同一种方式采样三个像素是不能消除这个方向的块面感的，当然想必他们也有对应的解决方案就是了。此外他们声称材质ID图是一张R8的贴图，但这张贴图里面怎么会有5bit的下层ID，5bit的上层ID，再有3bit的权重值呢？我只能认为这张材质ID图实际上只包含了一个5bit的材质ID和3bit的权重了，这个3bit的权重值会在和另外几个像素混合时使用到。\n不过三次采样倒是让我想起了Hex Tiling。在Practical Real-Time Hex-Tiling里介绍了一种通过六边形平铺来降低平铺时纹理重复感的算法，这种算法正巧需要对主贴图采样三次（不考虑随机采样的话）。Github中也能找到参考的代码。\n这样一来我们就能在三角洲行动的方案上再提出一种新的地形混合的方法了。我们同样是采样三个像素，不过我们在地形中会将这三个像素用等边三角形的方式排布，而不是目前所用的直角三角形。所以我们的流程是，先将当前的世界空间或者uv做一次三角形格点的变换，使用变换后的格点采样材质ID图获得三个材质ID和权重，再使用获得的数据和本身的六边形平铺的权重进行混合，就能得到我们最终的混合后的地形材质了。如果把我们的材质ID图平铺到世界空间，看上去应该是这样的：\n生成材质ID图 为了快速生成材质ID图（我可不想手画），我们考虑使用Compute Shader通过Perlin Noise生成材质ID，使用普通的hash生成权重。为了使我们的材质ID图也能正常的平铺，我们在计算Perlin Noise的时候，要注意使用取模的运算将计算的uv值限制在同一个范围内。\n我们只需要一个8bit的数据，但是由于Unity保存贴图的种种限制，我们可以将R8_Uint的数据除以255转换成R8_UNorm类型的数据再储存到贴图中。\nGenerateMatIDComputeShader.compute #pragma kernel MatIDGenMain RWTexture2D\u0026lt;float\u0026gt; _RW_MatIDTex; float4 _TextureSize; // https://www.shadertoy.com/view/4djSRW float hash12(float2 p) { float3 p3 = frac(float3(p.xyx) * .1031); p3 += dot(p3, p3.yzx + 33.33); return frac((p3.x + p3.y) * p3.z); } float2 repeat(float2 coord, float2 size, float2 offset) { return coord - floor(coord / size) * size + offset; } float encode(int weight, int index) { int encoded = (weight \u0026lt;\u0026lt; 5) | index; return float(encoded) / 255.0f; } float noise(float2 p ) { float2 i = floor( p ); float2 f = frac( p );\tfloat2 u = f*f*f*(f*(f*6.0-15.0)+10.0); float2 ts = _TextureSize.xy / 32.0f; float2 offset = 50.0f; float rv = lerp( lerp( hash12( repeat(i + float2(0.0,0.0), ts, offset) ), hash12( repeat(i + float2(1.0,0.0), ts, offset) ), u.x), lerp( hash12( repeat(i + float2(0.0,1.0), ts, offset) ), hash12( repeat(i + float2(1.0,1.0), ts, offset)), u.x), u.y); return rv * 2.0f - 1.0f; } [numthreads(8,8,1)] void MatIDGenMain (uint3 id : SV_DispatchThreadID) { float hashVal1 = hash12(float2(id.xy) + 12.3f); int weight = int(floor(hashVal1 * 8.0f)); float rv = 0.0f; float2 uv = float2(id.xy) / 32.0f + 50.0f; rv = 0.5000*noise( uv ); uv *=2.0f; rv += 0.2500*noise( uv ); uv *=2.0f; rv += 0.1250*noise( uv ); uv *=2.0f; rv += 0.0625*noise( uv ); rv = rv * 0.5f + 0.5f; int index = int(rv * 32.0f); float returnVal = encode(weight, index); _RW_MatIDTex[id.xy] = returnVal; } MatIDGenerator.cs using UnityEngine; using UnityEditor; using System.IO; public class MatIDGenerator : EditorWindow { private TextureSize textureSize = TextureSize._256x256; private ComputeShader computeShader; private string savePath = \u0026#34;Assets/HexTiling/MatID\u0026#34;; private static readonly string suffix = \u0026#34;.tga\u0026#34;; private RenderTexture rt; private enum TextureSize { _256x256 = 256, _512x512 = 512, _1024x1024 = 1024, } Rect rect { get { return new Rect(20.0f, 20.0f, position.width - 40.0f, position.height - 10.0f); } } private void EnsureRT() { if (rt == null || rt.width != (int)textureSize || rt.height != (int)textureSize) { if (rt != null) rt.Release(); RenderTextureDescriptor desc = new RenderTextureDescriptor { width = (int)textureSize, height = (int)textureSize, volumeDepth = 1, dimension = UnityEngine.Rendering.TextureDimension.Tex2D, depthBufferBits = 0, msaaSamples = 1, graphicsFormat = UnityEngine.Experimental.Rendering.GraphicsFormat.R8G8B8A8_UNorm, enableRandomWrite = true }; rt = new RenderTexture(desc); if (!rt.IsCreated()) rt.Create(); } } [MenuItem(\u0026#34;zznewclear13/Mat ID Generator\u0026#34;)] public static void Init() { MatIDGenerator window = GetWindow\u0026lt;MatIDGenerator\u0026gt;(\u0026#34;Mat ID Generator\u0026#34;); window.Show(); window.Repaint(); window.Focus(); } private void OnGUI() { using (new GUILayout.AreaScope(rect)) { computeShader = (ComputeShader)EditorGUILayout.ObjectField(\u0026#34;Compute Shader\u0026#34;, computeShader, typeof(ComputeShader), false); textureSize = (TextureSize)EditorGUILayout.EnumPopup(\u0026#34;Output Texture Size\u0026#34;, textureSize); savePath = EditorGUILayout.TextField(\u0026#34;Save Path\u0026#34;, savePath); using (new EditorGUI.DisabledGroupScope(!computeShader)) { if (GUILayout.Button(\u0026#34;Generate!\u0026#34;, new GUILayoutOption[] { GUILayout.Height(30.0f) })) { GenerateMatID(); } } } } private void GenerateMatID() { EnsureRT(); float ts = (float)textureSize; int kernelID = computeShader.FindKernel(\u0026#34;MatIDGenMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); Vector2Int dispatchCount = new Vector2Int(Mathf.CeilToInt(ts / x), Mathf.CeilToInt(ts / y)); computeShader.SetTexture(kernelID, \u0026#34;_RW_MatIDTex\u0026#34;, rt); computeShader.SetVector(\u0026#34;_TextureSize\u0026#34;, new Vector4(ts, ts, 1.0f / ts, 1.0f / ts)); computeShader.Dispatch(kernelID, dispatchCount.x, dispatchCount.y, 1); SaveRenderTextureToFile(rt); } private void SaveRenderTextureToFile(RenderTexture rt) { RenderTexture prev = RenderTexture.active; RenderTexture.active = rt; int ts = (int)textureSize; Texture2D toSave = new Texture2D(ts, ts, TextureFormat.R8, false, true); toSave.ReadPixels(new Rect(0.0f, 0.0f, ts, ts), 0, 0); byte[] bytes = toSave.EncodeToTGA(); FileStream fs = File.OpenWrite(savePath + suffix); fs.Write(bytes); fs.Close(); AssetDatabase.Refresh(); TextureImporter ti = (TextureImporter)AssetImporter.GetAtPath(savePath + suffix); ti.mipmapEnabled = false; ti.sRGBTexture = false; ti.textureCompression = TextureImporterCompression.Uncompressed; ti.SaveAndReimport(); Texture2D tempTexture = AssetDatabase.LoadAssetAtPath\u0026lt;Texture2D\u0026gt;(savePath + suffix); EditorGUIUtility.PingObject(tempTexture); RenderTexture.active = prev; } private void OnDestroy() { if (rt != null) rt.Release(); } } 对地形贴图做混合 为了简便，我们只对地形的Base Color进行混合，要想对法线或者是三平面映射进行混合，Hex Tiling的代码案例里也提供了相应的函数。我额外的使用了一个_Shape参数，数值越小就越多地显示出六边形的特征，越大则越多地显示出权重混合的特征。权重混合应该还有更好的方式，比如当三角形的两个点材质ID一致的时候，将混合的效果从三个权重混合简化成两个权重混合，不过这个后续有时间再考虑吧。这个Shader也能支持32张地形贴图的混合，第142行改成index = (data \u0026amp; 31);即可，不过我可没那么多张贴图，就限制到4张了。\nHexTilingShader.shader // MIT License // // Copyright (c) 2024 zznewclear@gmail.com // Copyright (c) 2022 mmikk // // Permission is hereby granted, free of charge, to any person obtaining a copy // of this software and associated documentation files (the \u0026#34;Software\u0026#34;), to deal // in the Software without restriction, including without limitation the rights // to use, copy, modify, merge, publish, distribute, sublicense, and/or sell // copies of the Software, and to permit persons to whom the Software is // furnished to do so, subject to the following conditions: // // The above copyright notice and this permission notice shall be included in all // copies or substantial portions of the Software. // // THE SOFTWARE IS PROVIDED \u0026#34;AS IS\u0026#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR // IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, // FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE // AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER // LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, // OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE // SOFTWARE. Shader \u0026#34;zznewclear13/HexTilingShader\u0026#34; { Properties { _MatIDTex (\u0026#34;Mat ID Texture\u0026#34;, 2D) = \u0026#34;black\u0026#34; {} _TerrainMainTex (\u0026#34;Terrain Main Tex\u0026#34;, 2DArray) = \u0026#34;\u0026#34; {} _Rotation (\u0026#34;Rotation\u0026#34;, Range(0, 1)) = 0.5 _Shape (\u0026#34;Shape\u0026#34;, Range(0, 1)) = 0.5 _G_EXP (\u0026#34;G Exp\u0026#34;, Range(1, 10)) = 3.0 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #define M_PI 3.1415926f static float g_fallOffContrast = 0.6; static float g_exp = 7; Texture2DArray _TerrainMainTex; Texture2D\u0026lt;float\u0026gt; _MatIDTex; CBUFFER_START(UnityPerMaterial) SamplerState sampler_MainTex; SamplerState sampler_TerrainMainTex; SamplerState sampler_PointRepeat; float4 _MatIDTex_TexelSize; float _Rotation; float _Shape; float _G_EXP; CBUFFER_END struct Attributes { float4 positionOS : POSITION; float2 texcoord : TEXCOORD0; }; struct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; float3 positionWS : TEXCOORD1; }; Varyings vert(Attributes input) { Varyings output = (Varyings)0; VertexPositionInputs vpi = GetVertexPositionInputs(input.positionOS.xyz); output.positionCS = vpi.positionCS; output.uv = input.texcoord; output.positionWS = vpi.positionWS; return output; } float2 hash22(float2 p) { float3 p3 = frac(float3(p.xyx) * float3(.1031, .1030, .0973)); p3 += dot(p3, p3.yzx+33.33); return frac((p3.xx+p3.yz)*p3.zy); } void TriangleGrid(out float w1, out float w2, out float w3, out int2 vertex1, out int2 vertex2, out int2 vertex3, float2 st) { // Scaling of the input st *= 2 * sqrt(3); // Skew input space into simplex triangle grid const float2x2 gridToSkewedGrid = float2x2(1.0, -0.57735027, 0.0, 1.15470054); float2 skewedCoord = mul(gridToSkewedGrid, st); int2 baseId = int2( floor ( skewedCoord )); float3 temp = float3( frac( skewedCoord ), 0); temp.z = 1.0 - temp.x - temp.y; float s = step(0.0, -temp.z); float s2 = 2*s-1; w1 = -temp.z*s2; w2 = s - temp.y*s2; w3 = s - temp.x*s2; vertex1 = baseId + int2(s,s); vertex2 = baseId + int2(s,1-s); vertex3 = baseId + int2(1-s,s); } float2 MakeCenST(int2 Vertex) { float2x2 invSkewMat = float2x2(1.0, 0.5, 0.0, 1.0/1.15470054); return mul(invSkewMat, Vertex) / (2 * sqrt(3)); } float2x2 LoadRot2x2(int2 idx, float rotStrength) { float angle = abs(idx.x*idx.y) + abs(idx.x+idx.y) + M_PI; angle = fmod(angle, 2*M_PI); if(angle\u0026lt;0) angle += 2*M_PI; if(angle\u0026gt;M_PI) angle -= 2*M_PI; angle *= rotStrength; float cs = cos(angle), si = sin(angle); return float2x2(cs, -si, si, cs); } int2 repeat(int2 coord, float2 size) { return coord - int2(floor(float2(coord) / size) * size); } void decodeData(int data, out float weight, out int index) { index = (data \u0026amp; 31) % 4; int iWeight = (data\u0026gt;\u0026gt;5)\u0026amp;7; weight = float(iWeight + 1) / 8.0f; } float4 frag(Varyings input) : SV_TARGET { float2 xy = input.positionWS.xz * 0.3f; float4 color; float w1, w2, w3; int2 vertex1, vertex2, vertex3; TriangleGrid(w1, w2, w3, vertex1, vertex2, vertex3, xy); int2 rect1 = vertex1 + int2(vertex1.y / 2, 0); int2 rect2 = vertex2 + int2(vertex2.y / 2, 0); int2 rect3 = vertex3 + int2(vertex3.y / 2, 0); float2 size = _MatIDTex_TexelSize.zw; int data1 = int(_MatIDTex.Load(int3(repeat(rect1, size), 0)) * 255.0f + 0.5f); int data2 = int(_MatIDTex.Load(int3(repeat(rect2, size), 0)) * 255.0f + 0.5f); int data3 = int(_MatIDTex.Load(int3(repeat(rect3, size), 0)) * 255.0f + 0.5f); float weight1, weight2, weight3; int index1, index2, index3; decodeData(data1, weight1, index1);decodeData(data2, weight2, index2);decodeData(data3, weight3, index3); float3 weights = float3(weight1, weight2, weight3); float weightSum = dot(weights, float3(1.0f, 1.0f, 1.0f)); weights /= weightSum; weights = lerp(1.0f, weights, _Shape); float rotStrength = _Rotation; float2x2 rot1 = LoadRot2x2(vertex1, rotStrength); float2x2 rot2 = LoadRot2x2(vertex2, rotStrength); float2x2 rot3 = LoadRot2x2(vertex3, rotStrength); float2 cen1 = MakeCenST(vertex1); float2 cen2 = MakeCenST(vertex2); float2 cen3 = MakeCenST(vertex3); float2 st1 = mul(xy - cen1, rot1) + cen1 + hash22(vertex1); float2 st2 = mul(xy - cen2, rot2) + cen2 + hash22(vertex2); float2 st3 = mul(xy - cen3, rot3) + cen3 + hash22(vertex3); float2 dSTdx = ddx(xy), dSTdy = ddy(xy); float4 c1 = _TerrainMainTex.SampleGrad(sampler_TerrainMainTex, float3(st1, index1), mul(dSTdx, rot1), mul(dSTdy, rot1)); float4 c2 = _TerrainMainTex.SampleGrad(sampler_TerrainMainTex, float3(st2, index2), mul(dSTdx, rot2), mul(dSTdy, rot2)); float4 c3 = _TerrainMainTex.SampleGrad(sampler_TerrainMainTex, float3(st3, index3), mul(dSTdx, rot3), mul(dSTdy, rot3)); // use luminance as weight float3 Lw = float3(0.299, 0.587, 0.114); float3 Dw = float3(dot(c1.xyz,Lw),dot(c2.xyz,Lw),dot(c3.xyz,Lw)); Dw = lerp(1.0, Dw, g_fallOffContrast); float3 W = Dw*pow(float3(w1 * weights.x, w2 * weights.y, w3 * weights.z), _G_EXP); W /= (W.x+W.y+W.z); color = W.x * c1 + W.y * c2 + W.z * c3; return color; } ENDHLSL SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } LOD 100 Pass { HLSLPROGRAM #pragma vertex vert #pragma fragment frag ENDHLSL } } } 最终的效果 效果看上去就是这样了，左边是远景的截图，从中我们看不出任何平铺的痕迹，右边是近景的截图，该怎么说呢，看上去好像很有机地混合在了一起。。。\n后记 呃呃这篇不知道该扯些啥了，反正是一个比较简单的效果。六边形平铺好早之前就想做了，终于有时间一试。大体上效果还是可以的，但是当出现垂直的线条的时候，会有左一个六边形，右一个六边形的情况出现，我觉得可能可以通过手动修改权重值来优化这个问题，或者是上面提到的三个权重变两个的方法，不过也懒得再去试了。\n","permalink":"https://zznewclear13.github.io/posts/use-hex-tiling-for-terrain-texture-blending/","summary":"动机 最近看到了三角洲行动介绍的在虚幻引擎4中实现的地形渲染方案，感觉受益匪浅。不过在Unity中要想实现一个即插即用的虚拟贴图的技术应该有些困难，于是我把目光放在了最一开始所介绍的对地形贴图做混合的方案上。\n三角洲行动提出的方案是这样的，在地形计算的时候，从对材质ID图的四个像素采样退化成对三个像素采样，这样一来既能减少地形混合的时候的采样次数，二来相较于采样四个像素，三个像素多了一个斜向45度的效果，能够减轻一些地形的块面感。不过他们也有语焉不详的地方，虽然只采样三个像素能够提供斜向45度，但是对于斜向-45度，仅使用同一种方式采样三个像素是不能消除这个方向的块面感的，当然想必他们也有对应的解决方案就是了。此外他们声称材质ID图是一张R8的贴图，但这张贴图里面怎么会有5bit的下层ID，5bit的上层ID，再有3bit的权重值呢？我只能认为这张材质ID图实际上只包含了一个5bit的材质ID和3bit的权重了，这个3bit的权重值会在和另外几个像素混合时使用到。\n不过三次采样倒是让我想起了Hex Tiling。在Practical Real-Time Hex-Tiling里介绍了一种通过六边形平铺来降低平铺时纹理重复感的算法，这种算法正巧需要对主贴图采样三次（不考虑随机采样的话）。Github中也能找到参考的代码。\n这样一来我们就能在三角洲行动的方案上再提出一种新的地形混合的方法了。我们同样是采样三个像素，不过我们在地形中会将这三个像素用等边三角形的方式排布，而不是目前所用的直角三角形。所以我们的流程是，先将当前的世界空间或者uv做一次三角形格点的变换，使用变换后的格点采样材质ID图获得三个材质ID和权重，再使用获得的数据和本身的六边形平铺的权重进行混合，就能得到我们最终的混合后的地形材质了。如果把我们的材质ID图平铺到世界空间，看上去应该是这样的：\n生成材质ID图 为了快速生成材质ID图（我可不想手画），我们考虑使用Compute Shader通过Perlin Noise生成材质ID，使用普通的hash生成权重。为了使我们的材质ID图也能正常的平铺，我们在计算Perlin Noise的时候，要注意使用取模的运算将计算的uv值限制在同一个范围内。\n我们只需要一个8bit的数据，但是由于Unity保存贴图的种种限制，我们可以将R8_Uint的数据除以255转换成R8_UNorm类型的数据再储存到贴图中。\nGenerateMatIDComputeShader.compute #pragma kernel MatIDGenMain RWTexture2D\u0026lt;float\u0026gt; _RW_MatIDTex; float4 _TextureSize; // https://www.shadertoy.com/view/4djSRW float hash12(float2 p) { float3 p3 = frac(float3(p.xyx) * .1031); p3 += dot(p3, p3.yzx + 33.33); return frac((p3.x + p3.y) * p3.z); } float2 repeat(float2 coord, float2 size, float2 offset) { return coord - floor(coord / size) * size + offset; } float encode(int weight, int index) { int encoded = (weight \u0026lt;\u0026lt; 5) | index; return float(encoded) / 255.","title":"在地形贴图混合时使用六边形平铺"},{"content":"POM和RCSM 在我之前的文章在Unity里实现松散圆锥步进Relaxed Cone Step Mapping就已经介绍过了视差映射和松散圆锥步进浮雕映射的计算方法了，但是之前并没有对计算深度值做相应的研究，同时也限制于篇幅的原因就没有再展开了，这篇文章相当于是之前文章的后续。为了简便，后续将这两种计算方法统称为视差映射。\n在视差映射中计算深度值是一个很直接的想法，因为很有可能会有其他物体被放置在视差映射的表面，与之发生穿插，如果不做特殊处理，就会使用模型本身的深度值进行深度比较，导致别的物体不能有正确的被遮挡的效果，削弱了视差映射带来的真实感。网上我找了一圈，并没有找到和计算视差映射的深度值相关的文章，因此我想用这篇文章进行相关的介绍。\nUnity的高清管线（HDRP）的Lit Shader支持计算像素深度偏移，提供了Primitive Length，Primitive Width，和Amplitude三个参数。Amplitude可以用来控制视差映射的强度值，虽然其一个单位和世界空间的一米完全不能直接等同起来，但是值越大视差看上去就越深，可以根据视觉实时调整这个参数。另外两个参数就很奇怪了，居然和模型的大小有关，同一个材质球，用在Quad上这里就要填1，用在Plane上就要填10，哪有这种道理？虚幻引擎则是提供了POM的接口，至于输入和输出完全都由用户控制，这里就不太好直接比较了。\n回顾POM的计算过程 视差映射一般不会直接在世界空间步进，而是会先将世界空间的视线viewWS转换到切线空间viewTS，在切线空间步进。照常理_ParallaxIntensity是用来控制视差映射的深度的，因此会使用这个参数控制z方向步进的距离，但为了方便和高度图中记载的高度进行对比，会先对viewTS的z分量进行归一化，将_ParallaxIntensity在步进时乘到viewTS的xy分量上，之后就是循环比较深度进入下一个循环了。\n但是为什么是切线空间呢？这是因为切线tangent和副切线bitangent代表了贴图UV的xy的正方向，将视线转换到切线空间，其实目的是将视线转到UV空间，或者说是贴图空间（Texture Space，因为其与切线空间的相似性，我们还是用TS来做简写）。这里就出现了最重要的一个问题，Unity中通过GetVertexNormalInputs获得到的世界空间的切线是经过归一化的，丢失了物体自身的缩放，所以我们其实应该先将世界坐标的视线viewWS转换到物体空间viewOS，然后再使用物体空间的tbn矩阵，将viewOS转换到切线空间viewTS。但又如我上面说到的，我们真实的目的是贴图空间，切线空间和贴图空间是存在差异性的。这也就是为什么Unity的HDRP要使用额外的参数Primitive Length和Primitive Width了，这两个参数的目的是通过额外的缩放，将切线空间和贴图空间对应起来。\n这两个参数的意义应当是，贴图空间的xy分量每一个单位在物体空间的长度，这里我们记为uvScale。同时我们可以顺理成章地正式引入_ParallaxIntensity这个参数，它的含义应当是，贴图中颜色为0的点对应的物体空间的深度值。贴图空间转换到物体空间，只需要对xyz三个分量分别乘上uvScale.x，uvScale.y，和_ParallaxIntensity即可。_ParallaxIntensity这个参数我们可以作为材质球的一个输入进行控制，uvScale是一个跟模型相关的参数，我们可以在Geometry Shader中计算而得。\nuvScale的计算 如上面所属，uvScale指代的是贴图空间的xy分量每一个单位在物体空间的长度。对于两个顶点v0和v1，贴图空间的xy分量其实就是这两个顶点uv值的差，物体空间的长度其实就是两个顶点之间的距离，为了对应到贴图空间上，我们需要计算这段距离在切线和副切线上的投影长度，后者除以前者就是我们需要的uvScale了。由于构成三角形的三个顶点可能会存在某两个顶点之间uv的某个分量的变化率为0，导致我们计算uvScale的时候除以零，我们在检测到这个情况的时候使用第三个顶点即可。\n贴图空间变换 在获得了物体空间的切线、副切线和法线之后，为了构成贴图空间的三个基向量，我们需要对这个向量使用uvScale和_ParallaxIntensity进行缩放。这个缩放导致了我们按照以往的float3x3(tangentOS * uvScale.x, bitangentOS * uvScale.y, normalOS * _ParallaxIntensity)构成的矩阵不再是一个正交矩阵，它实际上是贴图空间到物体空间的变换矩阵的转置。因此将物体空间的视线viewOS转换到贴图空间viewTS时，我们要用这个矩阵的转置的逆左乘viewOS，将贴图空间的视线viewTS转换到物体空间viewOS时，我们要用这个矩阵的转置左乘viewTS。\n深度的获取 这个就相对来说比较简单了，我们在贴图空间步进的时候，可以知道我们在贴图空间步进的z方向的深度值len。而由于我们的viewTS会做除以z分量的归一化，我们只需要用归一化前的-viewTS乘上len再除以z分量，就能知道我们在贴图空间中总的步进的向量，将其转换到物体空间再转换到世界空间，和当前点的世界空间的坐标相加后再转换到裁剪空间，其z分量除以w分量就是我们需要的深度值了。\n具体的代码 这里只做了可行性的研究，应该有个方法能够简化计算矩阵的逆这一步操作。在计算世界空间的切线、副切线和法线的时候，可以不进行归一化，这样我们也就不需要先转换到物体空间再转换到贴图空间了。\nPOMShader.shader Shader \u0026#34;zznewclear13/POMShader\u0026#34; { Properties { [Toggle(OUTPUT_DEPTH)] _OutputDepth (\u0026#34;Output Depth\u0026#34;, Float) = 1 _BaseColor(\u0026#34;Base Color\u0026#34;, Color) = (1, 1, 1, 1) _MainTex (\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _HeightMap(\u0026#34;Height Map\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _NormalMap(\u0026#34;Normal Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _NormalIntensity(\u0026#34;Normal Intensity\u0026#34;, Range(0, 2)) = 1 _ParallaxIntensity (\u0026#34;Parallax Intensity\u0026#34;, Float) = 1 _ParallaxIteration (\u0026#34;Parallax Iteration\u0026#34;, Float) = 15 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; #pragma shader_feature OUTPUT_DEPTH sampler2D _MainTex; sampler2D _HeightMap; sampler2D _NormalMap; CBUFFER_START(UnityPerMaterial) float4 _BaseColor; float4 _MainTex_ST; float _NormalIntensity; float _ParallaxIntensity; float _ParallaxIteration; CBUFFER_END struct a2v { float4 positionOS : POSITION; float3 normalOS : NORMAL; float4 tangentOS : TANGENT; float2 texcoord : TEXCOORD0; }; struct v2g { float4 positionCS : SV_POSITION; float3 positionOS : TEXCOORD0; float3 positionWS : TEXCOORD1; float4 tangentOS : TEXCOORD2; float3 bitangentOS : TEXCOORD3; float3 normalOS : TEXCOORD4; float2 texcoord : TEXCOORD5; }; struct g2f { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD1; float4 tbnWSPos[3] : TEXCOORD2; // tbnWS, posWS float4 tbnOSView[3] : TEXCOORD5; // tbnOS, viewWS float2 uvScale : TEXCOORD8; }; v2g vert(a2v input) { v2g output = (v2g)0; VertexPositionInputs vpi = GetVertexPositionInputs(input.positionOS.xyz); VertexNormalInputs vni = GetVertexNormalInputs(input.normalOS, input.tangentOS); output.positionCS = vpi.positionCS; output.positionOS = input.positionOS.xyz; output.positionWS = vpi.positionWS; output.normalOS = input.normalOS; output.tangentOS = input.tangentOS; output.bitangentOS = cross(input.normalOS, input.tangentOS.xyz) * input.tangentOS.w * GetOddNegativeScale(); output.texcoord = input.texcoord; return output; } [maxvertexcount(3)] void geom(triangle v2g IN[3], inout TriangleStream\u0026lt;g2f\u0026gt; tristream) { float3 camWS = GetCameraPositionWS(); g2f output = (g2f)0; float3 posDiff01 = IN[1].positionOS - IN[0].positionOS; float3 posDiff02 = IN[2].positionOS - IN[0].positionOS; float3 tangentOS0 = IN[0].tangentOS.xyz; float3 bitangentOS0 = IN[1].bitangentOS; float2 uvDiff01 = IN[1].texcoord - IN[0].texcoord; float2 uvDiff02 = IN[2].texcoord - IN[0].texcoord; float2 uvScale; if (uvDiff01.x != 0.0f) uvScale.x = dot(posDiff01, tangentOS0) / uvDiff01.x; else uvScale.x = dot(posDiff02, tangentOS0) / uvDiff02.x; if (uvDiff01.y != 0.0f) uvScale.y = dot(posDiff01, bitangentOS0) / uvDiff01.y; else uvScale.y = dot(posDiff02, bitangentOS0) / uvDiff02.y; for (int i=0; i\u0026lt;3; ++i) { v2g input = IN[i]; VertexNormalInputs vni = GetVertexNormalInputs(input.normalOS, input.tangentOS); float3 viewWS = camWS - input.positionWS; output.positionCS = input.positionCS; output.uv = input.texcoord; output.tbnWSPos[0] = float4(vni.tangentWS, input.positionWS.x); output.tbnWSPos[1] = float4(vni.bitangentWS, input.positionWS.y); output.tbnWSPos[2] = float4(vni.normalWS, input.positionWS.z); output.tbnOSView[0] = float4(input.tangentOS.xyz, viewWS.x); output.tbnOSView[1] = float4(input.bitangentOS, viewWS.y); output.tbnOSView[2] = float4(input.normalOS, viewWS.z); output.uvScale = uvScale; tristream.Append(output); } tristream.RestartStrip(); } float sampleHeight(float2 uv) { return 1.0f - tex2D(_HeightMap, uv).r; } float2 parallax(float2 uv, float3 view, out float len) { float numLayers = _ParallaxIteration; float layerDepth = 1.0f / numLayers; float2 p = view.xy; float2 deltaUVs = p / numLayers; float texd = sampleHeight(uv); float d = 0.0f; [unroll(30)] for (; d \u0026lt; texd; d += layerDepth) { uv -= deltaUVs; texd = sampleHeight(uv); } float2 lastUVs = uv + deltaUVs; float lastD = d - layerDepth; float after = texd - d; float before = sampleHeight(lastUVs) - d + layerDepth; float w = after / (after - before); len = lerp(d, lastD, w); return lerp(uv, lastUVs, w); } // Returns the determinant of a 2x2 matrix. float spvDet2x2(float a1, float a2, float b1, float b2) { return a1 * b2 - b1 * a2; } // Returns the inverse of a matrix, by using the algorithm of calculating the classical // adjoint and dividing by the determinant. The contents of the matrix are changed. float3x3 spvInverse(float3x3 m) { float3x3 adj;\t// The adjoint matrix (inverse after dividing by determinant) // Create the transpose of the cofactors, as the classical adjoint of the matrix. adj[0][0] = spvDet2x2(m[1][1], m[1][2], m[2][1], m[2][2]); adj[0][1] = -spvDet2x2(m[0][1], m[0][2], m[2][1], m[2][2]); adj[0][2] = spvDet2x2(m[0][1], m[0][2], m[1][1], m[1][2]); adj[1][0] = -spvDet2x2(m[1][0], m[1][2], m[2][0], m[2][2]); adj[1][1] = spvDet2x2(m[0][0], m[0][2], m[2][0], m[2][2]); adj[1][2] = -spvDet2x2(m[0][0], m[0][2], m[1][0], m[1][2]); adj[2][0] = spvDet2x2(m[1][0], m[1][1], m[2][0], m[2][1]); adj[2][1] = -spvDet2x2(m[0][0], m[0][1], m[2][0], m[2][1]); adj[2][2] = spvDet2x2(m[0][0], m[0][1], m[1][0], m[1][1]); // Calculate the determinant as a combination of the cofactors of the first row. float det = (adj[0][0] * m[0][0]) + (adj[0][1] * m[1][0]) + (adj[0][2] * m[2][0]); // Divide the classical adjoint matrix by the determinant. // If determinant is zero, matrix is not invertable, so leave it unchanged. return (det != 0.0f) ? (adj * (1.0f / det)) : m; } float4 frag(g2f input #if defined(OUTPUT_DEPTH) , out float depth : SV_DEPTH #endif ) : SV_TARGET { float3 tos = input.tbnOSView[0].xyz * input.uvScale.x; float3 bos = input.tbnOSView[1].xyz * input.uvScale.y; float3 nos = input.tbnOSView[2].xyz * _ParallaxIntensity; float3x3 t2wOS = float3x3(tos.x, bos.x, nos.x, tos.y, bos.y, nos.y, tos.z, bos.z, nos.z); float3 viewWS = float3(input.tbnOSView[0].w, input.tbnOSView[1].w, input.tbnOSView[2].w); float3 viewOS = mul((float3x3)UNITY_MATRIX_I_M, viewWS); float3 viewTS = mul(spvInverse(t2wOS), viewOS); float z = max(abs(viewTS.z), 1e-5) * (viewTS.z \u0026gt;= 0.0f ? 1.0f : -1.0f); float len; float2 uv = parallax((input.uv * _MainTex_ST.xy + _MainTex_ST.zw), viewTS * float3(_MainTex_ST.xy, 1.0f) / z, len); #if defined(OUTPUT_DEPTH) float3 offsetTS = -viewTS * (len / z); float3 offsetOS = mul(t2wOS, offsetTS); float3 positionWS = float3(input.tbnWSPos[0].w, input.tbnWSPos[1].w, input.tbnWSPos[2].w); float3 posWS = positionWS + mul((float3x3)UNITY_MATRIX_M, offsetOS); float4 posCS = mul(UNITY_MATRIX_VP, float4(posWS, 1.0f)); depth = posCS.z / posCS.w; #endif float4 mainTex = tex2D(_MainTex, uv) * _BaseColor; float3 normalTS = normalize(UnpackNormalScale(tex2D(_NormalMap, uv), _NormalIntensity)); float3 tws = input.tbnWSPos[0].xyz; float3 bws = input.tbnWSPos[1].xyz; float3 nws = input.tbnWSPos[2].xyz; float3 n = normalize(mul(normalTS, float3x3(tws, bws, nws))); Light mainLight = GetMainLight(); float ndotl = max(0.0f, dot(n, mainLight.direction)); float3 color = mainTex.rgb * mainLight.color * ndotl; float alpha = mainTex.a; return float4(color, alpha); } ENDHLSL SubShader { Tags{ \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Geometry\u0026#34;} Cull Back Pass { HLSLPROGRAM #pragma vertex vert #pragma geometry geom #pragma fragment frag ENDHLSL } } } RCSMShader.Shader Shader \u0026#34;zznewclear13/RCSMShader\u0026#34; { Properties { [Toggle(OUTPUT_DEPTH)] _OutputDepth (\u0026#34;Output Depth\u0026#34;, Float) = 1 _BaseColor(\u0026#34;Base Color\u0026#34;, Color) = (1, 1, 1, 1) _MainTex (\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _RCSMTex(\u0026#34;RCSM Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _NormalMap(\u0026#34;Normal Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _NormalIntensity(\u0026#34;Normal Intensity\u0026#34;, Range(0, 2)) = 1 _ParallaxIntensity(\u0026#34;Parallax Intensity\u0026#34;, Float) = 1 _ParallaxIteration(\u0026#34;Parallax Iteration\u0026#34;, Float) = 15 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; #pragma shader_feature OUTPUT_DEPTH sampler2D _MainTex; sampler2D _NormalMap; sampler2D _RCSMTex; CBUFFER_START(UnityPerMaterial) float4 _BaseColor; float4 _MainTex_ST; float _NormalIntensity; float _ParallaxIntensity; float _ParallaxIteration; CBUFFER_END struct a2v { float4 positionOS : POSITION; float3 normalOS : NORMAL; float4 tangentOS : TANGENT; float2 texcoord : TEXCOORD0; }; struct v2g { float4 positionCS : SV_POSITION; float3 positionOS : TEXCOORD0; float3 positionWS : TEXCOORD1; float4 tangentOS : TEXCOORD2; float3 bitangentOS : TEXCOORD3; float3 normalOS : TEXCOORD4; float2 texcoord : TEXCOORD5; }; struct g2f { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD1; float4 tbnWSPos[3] : TEXCOORD2; // tbnWS, posWS float4 tbnOSView[3] : TEXCOORD5; // tbnOS, viewWS float2 uvScale : TEXCOORD8; }; v2g vert(a2v input) { v2g output = (v2g)0; VertexPositionInputs vpi = GetVertexPositionInputs(input.positionOS.xyz); VertexNormalInputs vni = GetVertexNormalInputs(input.normalOS, input.tangentOS); output.positionCS = vpi.positionCS; output.positionOS = input.positionOS.xyz; output.positionWS = vpi.positionWS; output.normalOS = input.normalOS; output.tangentOS = input.tangentOS; output.bitangentOS = cross(input.normalOS, input.tangentOS.xyz) * input.tangentOS.w * GetOddNegativeScale(); output.texcoord = input.texcoord; return output; } [maxvertexcount(3)] void geom(triangle v2g IN[3], inout TriangleStream\u0026lt;g2f\u0026gt; tristream) { float3 camWS = GetCameraPositionWS(); g2f output = (g2f)0; float3 posDiff01 = IN[1].positionOS - IN[0].positionOS; float3 posDiff02 = IN[2].positionOS - IN[0].positionOS; float3 tangentOS0 = IN[0].tangentOS.xyz; float3 bitangentOS0 = IN[1].bitangentOS; float2 uvDiff01 = IN[1].texcoord - IN[0].texcoord; float2 uvDiff02 = IN[2].texcoord - IN[0].texcoord; float2 uvScale; if (uvDiff01.x != 0.0f) uvScale.x = dot(posDiff01, tangentOS0) / uvDiff01.x; else uvScale.x = dot(posDiff02, tangentOS0) / uvDiff02.x; if (uvDiff01.y != 0.0f) uvScale.y = dot(posDiff01, bitangentOS0) / uvDiff01.y; else uvScale.y = dot(posDiff02, bitangentOS0) / uvDiff02.y; for (int i=0; i\u0026lt;3; ++i) { v2g input = IN[i]; VertexNormalInputs vni = GetVertexNormalInputs(input.normalOS, input.tangentOS); float3 viewWS = camWS - input.positionWS; output.positionCS = input.positionCS; output.uv = input.texcoord; output.tbnWSPos[0] = float4(vni.tangentWS, input.positionWS.x); output.tbnWSPos[1] = float4(vni.bitangentWS, input.positionWS.y); output.tbnWSPos[2] = float4(vni.normalWS, input.positionWS.z); output.tbnOSView[0] = float4(input.tangentOS.xyz, viewWS.x); output.tbnOSView[1] = float4(input.bitangentOS, viewWS.y); output.tbnOSView[2] = float4(input.normalOS, viewWS.z); output.uvScale = uvScale; tristream.Append(output); } tristream.RestartStrip(); } float2 sampleRCSM(float2 uv) { float2 rcsm = tex2D(_RCSMTex, uv).xy; return float2(1.0f - rcsm.x, rcsm.y); } float getStepLength(float rayRatio, float coneRatio, float rayHeight, float sampleHeight) { float totalRatio = rayRatio / coneRatio + 1.0f; return (sampleHeight - rayHeight) / totalRatio; } float2 parallax(float2 uv, float3 view, out float len) { view.xy = -view.xy * _ParallaxIntensity; float3 samplePos = float3(uv, 0.0f); float2 rcsm = sampleRCSM(samplePos.xy); float rayRatio = length(view.xy); float coneRatio = rcsm.y; float rayHeight = samplePos.z; float sampleHeight = rcsm.x; float stepLength = getStepLength(rayRatio, coneRatio, rayHeight, sampleHeight); [unroll(30)] for (int i = 0; i \u0026lt; _ParallaxIteration; ++i) { samplePos += stepLength * view; rcsm = sampleRCSM(samplePos.xy); coneRatio = rcsm.y; rayHeight = samplePos.z; sampleHeight = rcsm.x; if (sampleHeight \u0026lt;= rayHeight) break; stepLength = getStepLength(rayRatio, coneRatio, rayHeight, sampleHeight); } stepLength *= 0.5f; samplePos -= stepLength * view; [unroll] for (int j = 0; j \u0026lt; 5; ++j) { rcsm = sampleRCSM(samplePos.xy); stepLength *= 0.5f; if (samplePos.z \u0026gt;= rcsm.x) { samplePos -= stepLength * view; } else if(samplePos.z \u0026lt; rcsm.x) { samplePos += stepLength * view; } } len = samplePos.z; return samplePos.xy; } // Returns the determinant of a 2x2 matrix. float spvDet2x2(float a1, float a2, float b1, float b2) { return a1 * b2 - b1 * a2; } // Returns the inverse of a matrix, by using the algorithm of calculating the classical // adjoint and dividing by the determinant. The contents of the matrix are changed. float3x3 spvInverse(float3x3 m) { float3x3 adj;\t// The adjoint matrix (inverse after dividing by determinant) // Create the transpose of the cofactors, as the classical adjoint of the matrix. adj[0][0] = spvDet2x2(m[1][1], m[1][2], m[2][1], m[2][2]); adj[0][1] = -spvDet2x2(m[0][1], m[0][2], m[2][1], m[2][2]); adj[0][2] = spvDet2x2(m[0][1], m[0][2], m[1][1], m[1][2]); adj[1][0] = -spvDet2x2(m[1][0], m[1][2], m[2][0], m[2][2]); adj[1][1] = spvDet2x2(m[0][0], m[0][2], m[2][0], m[2][2]); adj[1][2] = -spvDet2x2(m[0][0], m[0][2], m[1][0], m[1][2]); adj[2][0] = spvDet2x2(m[1][0], m[1][1], m[2][0], m[2][1]); adj[2][1] = -spvDet2x2(m[0][0], m[0][1], m[2][0], m[2][1]); adj[2][2] = spvDet2x2(m[0][0], m[0][1], m[1][0], m[1][1]); // Calculate the determinant as a combination of the cofactors of the first row. float det = (adj[0][0] * m[0][0]) + (adj[0][1] * m[1][0]) + (adj[0][2] * m[2][0]); // Divide the classical adjoint matrix by the determinant. // If determinant is zero, matrix is not invertable, so leave it unchanged. return (det != 0.0f) ? (adj * (1.0f / det)) : m; } float4 frag(g2f input #if defined(OUTPUT_DEPTH) , out float depth : SV_DEPTH #endif ) : SV_TARGET { float3 tos = input.tbnOSView[0].xyz * input.uvScale.x; float3 bos = input.tbnOSView[1].xyz * input.uvScale.y; float3 nos = input.tbnOSView[2].xyz * _ParallaxIntensity; float3x3 t2wOS = float3x3(tos.x, bos.x, nos.x, tos.y, bos.y, nos.y, tos.z, bos.z, nos.z); float3 viewWS = float3(input.tbnOSView[0].w, input.tbnOSView[1].w, input.tbnOSView[2].w); float3 viewOS = mul((float3x3)UNITY_MATRIX_I_M, viewWS); float3 viewTS = mul(spvInverse(t2wOS), viewOS); float z = max(abs(viewTS.z), 1e-5) * (viewTS.z \u0026gt;= 0.0f ? 1.0f : -1.0f); float len; float2 uv = parallax((input.uv * _MainTex_ST.xy + _MainTex_ST.zw), viewTS * float3(_MainTex_ST.xy, 1.0f) / z, len); #if defined(OUTPUT_DEPTH) float3 offsetTS = -viewTS * (len / z); float3 offsetOS = mul(t2wOS, offsetTS); float3 positionWS = float3(input.tbnWSPos[0].w, input.tbnWSPos[1].w, input.tbnWSPos[2].w); float3 posWS = positionWS + mul((float3x3)UNITY_MATRIX_M, offsetOS); float4 posCS = mul(UNITY_MATRIX_VP, float4(posWS, 1.0f)); depth = posCS.z / posCS.w; #endif float4 mainTex = tex2D(_MainTex, uv) * _BaseColor; float3 normalTS = normalize(UnpackNormalScale(tex2D(_NormalMap, uv), _NormalIntensity)); float3 tws = input.tbnWSPos[0].xyz; float3 bws = input.tbnWSPos[1].xyz; float3 nws = input.tbnWSPos[2].xyz; float3 n = normalize(mul(normalTS, float3x3(tws, bws, nws))); Light mainLight = GetMainLight(); float ndotl = max(0.0f, dot(n, mainLight.direction)); float3 color = mainTex.rgb * mainLight.color * ndotl; float alpha = mainTex.a; return float4(color, alpha); } ENDHLSL SubShader { Tags{ \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Geometry\u0026#34;} Cull Back Pass { HLSLPROGRAM #pragma vertex vert #pragma geometry geom #pragma fragment frag ENDHLSL } } } 最终的效果 最后的效果也就如封面图一样了，左边是RCSM做的，其余的则是普通的POM效果。特地对模型做了缩放，对贴图的平铺进行调整，用来表明这个计算方式的正确性，同样的材质球用在不同的模型上也能够得到正确的深度值。但是像球体这样的uv并不规则的模型，用上述的方法并不能得到完美的深度效果。上面和下面平面使用的贴图来自Quixel的Megascans。\n后记 又迅速地写了一篇文章，计算了视差映射的深度值之后，各种屏幕空间的算法也都能够正常地使用了，很好。话又说回来了我被LearnOpenGL的贴图坑了一波，居然没有意识到上面的法线图和平常使用的法线图是不一样的，我就说怎么看上去有一种违和感。后来我直接在Blender里自己导出了这个Toy Box的法线和深度图，这才感觉一切都正常了。\n","permalink":"https://zznewclear13.github.io/posts/get-correct-depth-from-parallax-occlusion-mapping/","summary":"POM和RCSM 在我之前的文章在Unity里实现松散圆锥步进Relaxed Cone Step Mapping就已经介绍过了视差映射和松散圆锥步进浮雕映射的计算方法了，但是之前并没有对计算深度值做相应的研究，同时也限制于篇幅的原因就没有再展开了，这篇文章相当于是之前文章的后续。为了简便，后续将这两种计算方法统称为视差映射。\n在视差映射中计算深度值是一个很直接的想法，因为很有可能会有其他物体被放置在视差映射的表面，与之发生穿插，如果不做特殊处理，就会使用模型本身的深度值进行深度比较，导致别的物体不能有正确的被遮挡的效果，削弱了视差映射带来的真实感。网上我找了一圈，并没有找到和计算视差映射的深度值相关的文章，因此我想用这篇文章进行相关的介绍。\nUnity的高清管线（HDRP）的Lit Shader支持计算像素深度偏移，提供了Primitive Length，Primitive Width，和Amplitude三个参数。Amplitude可以用来控制视差映射的强度值，虽然其一个单位和世界空间的一米完全不能直接等同起来，但是值越大视差看上去就越深，可以根据视觉实时调整这个参数。另外两个参数就很奇怪了，居然和模型的大小有关，同一个材质球，用在Quad上这里就要填1，用在Plane上就要填10，哪有这种道理？虚幻引擎则是提供了POM的接口，至于输入和输出完全都由用户控制，这里就不太好直接比较了。\n回顾POM的计算过程 视差映射一般不会直接在世界空间步进，而是会先将世界空间的视线viewWS转换到切线空间viewTS，在切线空间步进。照常理_ParallaxIntensity是用来控制视差映射的深度的，因此会使用这个参数控制z方向步进的距离，但为了方便和高度图中记载的高度进行对比，会先对viewTS的z分量进行归一化，将_ParallaxIntensity在步进时乘到viewTS的xy分量上，之后就是循环比较深度进入下一个循环了。\n但是为什么是切线空间呢？这是因为切线tangent和副切线bitangent代表了贴图UV的xy的正方向，将视线转换到切线空间，其实目的是将视线转到UV空间，或者说是贴图空间（Texture Space，因为其与切线空间的相似性，我们还是用TS来做简写）。这里就出现了最重要的一个问题，Unity中通过GetVertexNormalInputs获得到的世界空间的切线是经过归一化的，丢失了物体自身的缩放，所以我们其实应该先将世界坐标的视线viewWS转换到物体空间viewOS，然后再使用物体空间的tbn矩阵，将viewOS转换到切线空间viewTS。但又如我上面说到的，我们真实的目的是贴图空间，切线空间和贴图空间是存在差异性的。这也就是为什么Unity的HDRP要使用额外的参数Primitive Length和Primitive Width了，这两个参数的目的是通过额外的缩放，将切线空间和贴图空间对应起来。\n这两个参数的意义应当是，贴图空间的xy分量每一个单位在物体空间的长度，这里我们记为uvScale。同时我们可以顺理成章地正式引入_ParallaxIntensity这个参数，它的含义应当是，贴图中颜色为0的点对应的物体空间的深度值。贴图空间转换到物体空间，只需要对xyz三个分量分别乘上uvScale.x，uvScale.y，和_ParallaxIntensity即可。_ParallaxIntensity这个参数我们可以作为材质球的一个输入进行控制，uvScale是一个跟模型相关的参数，我们可以在Geometry Shader中计算而得。\nuvScale的计算 如上面所属，uvScale指代的是贴图空间的xy分量每一个单位在物体空间的长度。对于两个顶点v0和v1，贴图空间的xy分量其实就是这两个顶点uv值的差，物体空间的长度其实就是两个顶点之间的距离，为了对应到贴图空间上，我们需要计算这段距离在切线和副切线上的投影长度，后者除以前者就是我们需要的uvScale了。由于构成三角形的三个顶点可能会存在某两个顶点之间uv的某个分量的变化率为0，导致我们计算uvScale的时候除以零，我们在检测到这个情况的时候使用第三个顶点即可。\n贴图空间变换 在获得了物体空间的切线、副切线和法线之后，为了构成贴图空间的三个基向量，我们需要对这个向量使用uvScale和_ParallaxIntensity进行缩放。这个缩放导致了我们按照以往的float3x3(tangentOS * uvScale.x, bitangentOS * uvScale.y, normalOS * _ParallaxIntensity)构成的矩阵不再是一个正交矩阵，它实际上是贴图空间到物体空间的变换矩阵的转置。因此将物体空间的视线viewOS转换到贴图空间viewTS时，我们要用这个矩阵的转置的逆左乘viewOS，将贴图空间的视线viewTS转换到物体空间viewOS时，我们要用这个矩阵的转置左乘viewTS。\n深度的获取 这个就相对来说比较简单了，我们在贴图空间步进的时候，可以知道我们在贴图空间步进的z方向的深度值len。而由于我们的viewTS会做除以z分量的归一化，我们只需要用归一化前的-viewTS乘上len再除以z分量，就能知道我们在贴图空间中总的步进的向量，将其转换到物体空间再转换到世界空间，和当前点的世界空间的坐标相加后再转换到裁剪空间，其z分量除以w分量就是我们需要的深度值了。\n具体的代码 这里只做了可行性的研究，应该有个方法能够简化计算矩阵的逆这一步操作。在计算世界空间的切线、副切线和法线的时候，可以不进行归一化，这样我们也就不需要先转换到物体空间再转换到贴图空间了。\nPOMShader.shader Shader \u0026#34;zznewclear13/POMShader\u0026#34; { Properties { [Toggle(OUTPUT_DEPTH)] _OutputDepth (\u0026#34;Output Depth\u0026#34;, Float) = 1 _BaseColor(\u0026#34;Base Color\u0026#34;, Color) = (1, 1, 1, 1) _MainTex (\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _HeightMap(\u0026#34;Height Map\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _NormalMap(\u0026#34;Normal Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _NormalIntensity(\u0026#34;Normal Intensity\u0026#34;, Range(0, 2)) = 1 _ParallaxIntensity (\u0026#34;Parallax Intensity\u0026#34;, Float) = 1 _ParallaxIteration (\u0026#34;Parallax Iteration\u0026#34;, Float) = 15 } HLSLINCLUDE #include \u0026#34;Packages/com.","title":"从视差映射、浮雕映射中获取正确的深度值"},{"content":"Screen Space Reflection While screen space reflection (SSR) is a well-known effect, this article aims to introduce a unique method for calculating screen space reflections \u0026ndash; one that I haven’t encountered online before.\nMany online tutorials cover screen space reflection already, and most of them follow a similar process: calculate the reflection direction in world space, use a mostly uniform step size to traverse the world space, and for each step, compute the normalized device coordinates (NDC). Then, compare the current depth with the depth value sampled from depth texture. If the ray depth is below the sampled depth, consider it a reflection intersection (hit) and sample the color value at that location. This method can yield visually pleasing reflection effects, but hardly anyone mentions the staggering number of iterations required. We’ll discuss this further shortly.\nAdditionally, achieving good reflection results for objects at varying distances often requires different step sizes, but few people delve into this consideration. Some slightly improved approaches involve binary search after ray hitting the scene to ensure smoother transitions between reflection colors. Others may prematurely terminate steps (known as early return) or interpolate reflection colors and environment reflection probe based on comparison between NDC coordinate and the [-1, 1] range.\nCurrently, the most effective screen space ray marching method involves precomputing a Hierarchical ZBuffer. By stepping into and out of different LODs, this approach achieves the same results with fewer iterations. However, Hierarchical ZBuffer is not a feature available in every projects.\nThe most valuable tutorial one can find online is Screen Space Ray Tracing by Morgan McGuire. He also wrote a paper about his algorithm. In his article, McGuire highlights why stepping in world space can be problematic. After undergoing perspective transformation, the step positions in world space may not vary significantly in screen space, leading to the need for more iterations to achieve desirable reflection effects. Also, McGuire presents an ingenious approach in his article. He calculates the coordinates of the starting and ending points in both clip space and screen space. By linearly interpolating the z coordinate in clip space, the 1/w coordinate in clip space, and the xy coordinates in screen space, he eliminates the matrix computations required during each step. Definitely worth using!\nThe goal of this article is to get the correct reflection color using as few iterations as possible within a single shader. Random sampling, blurring, and Fresnel effect are not within the scope of this article. We will focus solely on Windows platform DX11 shaders, which allows us to avoid extensive platform-specific code. The Unity version used for this article is Unity 2022.3.21f1. The final shader code will be provided at the end of the article.\nCalculation of Reflections Parameters The calculation of reflections typically relies on three essential parameters:\nMax Distance: This parameter considers reflections from objects within a certain range around the reflection point. Step Count: Increasing the number of steps results in more accurate reflections but also impacts performance. Thickness Params: In this article, an object’s default thickness is calculated as depth * _Thickness.y + _Thickness.x. This ensures that when a ray passes through behind an object, it is not considered an intersection. Comparion of Depth Value When considering what kind of depth value to compare during the stepping process, several factors come into play. We define the depth value we obtained from stepping as rayDepth and the depth value obtained from sampling as sampleDepth.\nBy directly sampling the depth texture, we obtain the depth value in normalized device coordinates. Therefore a straightforward approach is to compare these depths in NDC. When rayDepth \u0026lt; sampleDepth, the ray intersects with the scene.\nAlternatively, we can compare the actual depth values in view space. This approach allows us to specify a thickness value. If the depth difference exceeds this thickness, we consider the ray passing through behind an object without intersection. Specifically, when rayDepth \u0026gt; sampleDepth \u0026amp;\u0026amp; rayDepth \u0026lt; sampleDepth + thickness, the ray intersects with the scene.\nOne thing worth noting is the sampler used when sampling depth texture. Linear interpolation can mistakenly identify intersections at the edges of two faces with different depths, resulting in unwanted artifacts (small dots) on the screen. If available, using a separate texture to mark object edges can help exclude these intersection points. But in our shader, we will stick to a Point Clamp sampler.\nRay Marching Here’s the workflow breakdown:\nDefine k0 and k1 as the reciprocals of the w-components of clip space coordinates for the starting and ending points. Define q0 and q1 as the xyz-components of clip space coordinates for the starting and ending points. Define p0 and p1 as the xy-components of normalized device coordinates for the starting and ending points. Define w as a variable that linearly increases in (0, 1) based on _StepCount. For each step, update the value of w and use it to linearly interpolate the three sets of components (k, q, and p). Calculate rayDepth using q.z * k, sample the depth texture at p to obtain sampleDepth. If rayDepth \u0026lt; sampleDepth, the ray intersects with the scene, exit the loop and return p. Sample the color texture at p to obtain the reflection color. It looks like this (32 steps): Quite poor! The most noticeable issue is the stretching effect. There are primarily two reasons for this: First, we did not use thickness to determine whether the ray passes through behind an object, resulting in significant stretching below suspended objects. Second, we did not restrict positions outside the screen area, causing us to sample depth values from coordinates beyond the screen and return depth values at clampped positions.\nThickness Test To address the thickness issue mentioned earlier, we introduce a method for determining whether the stepping position is behind an object. This method relies on the linear depths from the camera: linearRayDepth and linearSampleDepth.\nAs previously discussed, we use linearSampleDepth * _Thickness.y + _Thickness.x as the thickness of an object in the scene. To determine if the ray passes through behind an object, we compare (linearRayDepth - linearSampleDepth - _Thickness.x) / linearSampleDepth with _Thickness.y. If the former is greater than the latter, it indicates that the ray passes through behind an object.\nfloat getThicknessDiff(float diff, float linearSampleDepth, float2 thicknessParams) { return (diff - thicknessParams.x) / linearSampleDepth; } The workflow now becomes:\nIf rayDepth \u0026lt; sampleDepth and thicknessDiff \u0026lt; _Thickness.y, the ray intersects with the scene, exit the loop and return p. It looks like this (32 steps): Frustum Clipping For point p1 that falls outside the screen space, two issues arise: First, sampling the depth texture beyond the screen range yields incorrect depth values. Second, it reduces the effective sampling count. To address this, we can restrict p1 within the screen space. Here’s a method for constraining the stepping endpoint within the view frustum. We define nf as the near and far clipping plane depths (positive values), define s as the slope of the view frustum in the horizontal and vertical directions (positive values). Numerically, s is given by float2(aspect * tan(fovy * 0.5f), tan(fovy * 0.5f)). Note that for ease of calculation, the z-components of from and to are positive.\n#define INFINITY 1e10 float3 frustumClip(float3 from, float3 to, float2 nf, float2 s) { float3 dir = to - from; float3 signDir = sign(dir); float nfSlab = signDir.z * (nf.y - nf.x) * 0.5f + (nf.y + nf.x) * 0.5f; float lenZ = (nfSlab - from.z) / dir.z; if (dir.z == 0.0f) lenZ = INFINITY; float2 ss = sign(dir.xy - s * dir.z) * s; float2 denom = ss * dir.z - dir.xy; float2 lenXY = (from.xy - ss * from.z) / denom; if (lenXY.x \u0026lt; 0.0f || denom.x == 0.0f) lenXY.x = INFINITY; if (lenXY.y \u0026lt; 0.0f || denom.y == 0.0f) lenXY.y = INFINITY; float len = min(min(1.0f, lenZ), min(lenXY.x, lenXY.y)); float3 clippedVS = from + dir * len; return clippedVS; } Actually I have wrote a shadertoy to demonstrate frustum clipping in 2D, interactable with mouse:\nFrustum Clip 2D\nThe workflow now becomes:\nFrustum clip the stepping endpoint to clippedPosVS, and transform it to clip space position endCS. It looks like this (32 steps): The scene is starting to exhibit some reflection, although there’s still room for improvement. The view frustum clipping has indeed reduced the number of pixels between steps, filling in some of the gaps. However, the reflected colors appear distorted.\nBinary Search In our previous step, although p is ensured to be at the intersected object, there is still some distance from the actual intersection point. To reduce this error, we can use binary search. Here’s how it works: We maintain two variables during stepping, w1 and w2, representing the w values in last two steps (w1 \u0026gt; w2). During each binary search iteration, we calculate w = 0.5f * (w1 + w2), if an intersection is detected, update w1 = w, otherwise, update w2 = w and proceed to the next iteration.\nThe workflow now becomes:\nFrustum clip the stepping endpoint to clippedPosVS, and transform it to clip space position endCS. Define k0 and k1 as the reciprocals of the w-components of clip space coordinates for the starting and ending points. Define q0 and q1 as the xyz-components of clip space coordinates for the starting and ending points. Define p0 and p1 as the xy-components of normalized device coordinates for the starting and ending points. Define w1 as a variable that linearly increases in (0, 1) based on _StepCount, initialize w1 and w2 with 0. For each step, w2 = w1, update the value of w1 and use it to linearly interpolate the three sets of components (k, q, and p). Calculate rayDepth using q.z * k, sample the depth texture at p to obtain sampleDepth. If rayDepth \u0026lt; sampleDepth and thicknessDiff \u0026lt; _Thickness.y, the ray intersects with the scene, exit the loop. Let w be the average of w1 and w2. Repeat 5, 6, and 7 to check whether an intersection occurs until the binary search loop ends. We update either w1 or w2 in each step depending on the result. Sample the color texture at p to obtain the reflection color. It looks like this (32 steps, 5 binary searches): The reflection effect now appears less distorted (particularly noticeable in the bottom left corner). However, there are still gaps between color segments due to our thickness test, which excludes potential intersections.\nPotential Intersections To compute potential intersections, let’s revisit the workflow where we do thickness test. When the ray passes through behind an object, if it is above the scene during last step, we can calculate the difference (thicknessDiff) between rayDepth and sampleDepth. If this value is smaller than the minimum difference (minThicknessDiff), we consider it a potential intersection. We update minThicknessDiff and record the current w1 and w2 for subsequent binary search.\nDuring binary search, if an actual intersection occurs, we follow the original code. If a potential intersection occurs, we also need to track thicknessDiff during binary search. We find the smallest thicknessDiff less than _Thickness.y, use current w to interpolate between p0 and p1 to obtain p, and finally use p to sample the color texture.\nThe workflow now becomes:\nFrustum clip the stepping endpoint to clippedPosVS, and transform it to clip space position endCS. Define k0 and k1 as the reciprocals of the w-components of clip space coordinates for the starting and ending points. Define q0 and q1 as the xyz-components of clip space coordinates for the starting and ending points. Define p0 and p1 as the xy-components of normalized device coordinates for the starting and ending points. Define w1 as a variable that linearly increases in (0, 1) based on _StepCount, initialize w1 and w2 with 0. For each step, w2 = w1, update the value of w1 and use it to linearly interpolate the three sets of components (k, q, and p). Calculate rayDepth using q.z * k, sample the depth texture at p to obtain sampleDepth. If rayDepth \u0026lt; sampleDepth and thicknessDiff \u0026lt; _Thickness.y, the ray intersects with the scene, exit the loop. Otherwise, if rayZ \u0026lt; sampleZ, thicknessDiff \u0026gt; _Thickness.y, and the previous ray was in front of the scene, compare thicknessDiff with the minimum value. If smaller, update the minimum value, record the current w1 and w2, and mark this as a potential intersection, continue looping. If an actual intersection occurs, let w be the average of w1 and w2. Repeat 5, 6, and 7 to check whether an intersection occurs until the binary search loop ends. We update either w1 or w2 in each step depending on the result. If a potential intersection occurs, repeat 5, 6, and 7 to check whether an intersection occurs, and use the smallest thicknessDiff and w to update p. Sample the color texture at p to obtain the reflection color. It looks like this (32 steps, 5 binary searches): And here is the result using 64 steps and 5 binary searches: ScreenSpaceReflection.shader /* // Copyright (c) 2024 zznewclear@gmail.com // // Permission is hereby granted, free of charge, to any person obtaining a copy // of this software and associated documentation files (the \u0026#34;Software\u0026#34;), to deal // in the Software without restriction, including without limitation the rights // to use, copy, modify, merge, publish, distribute, sublicense, and/or sell // copies of the Software, and to permit persons to whom the Software is // furnished to do so, subject to the following conditions: // // The above copyright notice and this permission notice shall be included in all // copies or substantial portions of the Software. // // THE SOFTWARE IS PROVIDED \u0026#34;AS IS\u0026#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR // IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, // FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE // AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER // LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, // OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE // SOFTWARE. */ Shader \u0026#34;zznewclear13/SSRShader\u0026#34; { Properties { [Toggle(USE_POTENTIAL_HIT)] _UsePotentialHit (\u0026#34;Use Potential Hit\u0026#34;, Float) = 1.0 [Toggle(USE_FRUSTUM_CLIP)] _UseFrustumClip (\u0026#34;Use Frustum Clip\u0026#34;, Float) = 1.0 [Toggle(USE_BINARY_SEARCH)] _UseBinarySearch (\u0026#34;Use Binary Search\u0026#34;, Float) = 1.0 [Toggle(USE_THICKNESS)] _UseThickness (\u0026#34;Use Thickness\u0026#34;, Float) = 1.0 _MaxDistance (\u0026#34;Max Distance\u0026#34;, Range(0.1, 100.0)) = 15.0 [int] _StepCount (\u0026#34;Step Count\u0026#34;, Float) = 32 _ThicknessParams (\u0026#34;Thickness Params\u0026#34;, Vector) = (0.1, 0.02, 0.0, 0.0) } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; #pragma shader_feature _ USE_POTENTIAL_HIT #pragma shader_feature _ USE_FRUSTUM_CLIP #pragma shader_feature _ USE_BINARY_SEARCH #pragma shader_feature _ USE_THICKNESS #define INFINITY 1e10 #define DEPTH_SAMPLER sampler_PointClamp Texture2D _CameraOpaqueTexture; Texture2D _CameraDepthTexture; CBUFFER_START(UnityPerMaterial) float _MaxDistance; int _StepCount; float2 _ThicknessParams; CBUFFER_END struct Attributes { float4 positionOS : POSITION; float3 normalOS : NORMAL; float2 texcoord : TEXCOORD0; }; struct Varyings { float4 positionCS : SV_POSITION; float3 positionWS : TEXCOORD0; float3 normalWS : TEXCOORD1; float2 uv : TEXCOORD2; float3 viewWS : TEXCOORD3; }; Varyings vert(Attributes input) { Varyings output = (Varyings)0; VertexPositionInputs vpi = GetVertexPositionInputs(input.positionOS.xyz); VertexNormalInputs vni = GetVertexNormalInputs(input.normalOS); output.positionCS = vpi.positionCS; output.positionWS = vpi.positionWS; output.normalWS = vni.normalWS; output.uv = input.texcoord; output.viewWS = GetCameraPositionWS() - vpi.positionWS; return output; } float3 frustumClip(float3 from, float3 to, float2 nf, float2 s) { float3 dir = to - from; float3 signDir = sign(dir); float nfSlab = signDir.z * (nf.y - nf.x) * 0.5f + (nf.y + nf.x) * 0.5f; float lenZ = (nfSlab - from.z) / dir.z; if (dir.z == 0.0f) lenZ = INFINITY; float2 ss = sign(dir.xy - s * dir.z) * s; float2 denom = ss * dir.z - dir.xy; float2 lenXY = (from.xy - ss * from.z) / denom; if (lenXY.x \u0026lt; 0.0f || denom.x == 0.0f) lenXY.x = INFINITY; if (lenXY.y \u0026lt; 0.0f || denom.y == 0.0f) lenXY.y = INFINITY; float len = min(min(1.0f, lenZ), min(lenXY.x, lenXY.y)); float3 clippedVS = from + dir * len; return clippedVS; } float getThicknessDiff(float diff, float linearSampleDepth, float2 thicknessParams) { return (diff - thicknessParams.x) / linearSampleDepth; } float4 frag(Varyings input) : SV_TARGET { float3 positionWS = input.positionWS; float3 normalWS = normalize(input.normalWS); float3 viewWS = normalize(input.viewWS); float3 reflWS = reflect(-viewWS, normalWS); float3 env = GlossyEnvironmentReflection(reflWS, 0.0f, 1.0f); float3 color = env; float3 originWS = positionWS; float3 endWS = positionWS + reflWS * _MaxDistance; #if defined(USE_FRUSTUM_CLIP) float3 originVS = mul(UNITY_MATRIX_V, float4(originWS, 1.0f)).xyz; float3 endVS = mul(UNITY_MATRIX_V, float4(endWS, 1.0f)).xyz; float3 flipZ = float3(1.0f, 1.0f, -1.0f); float3 clippedVS = frustumClip(originVS * flipZ, endVS * flipZ, _ProjectionParams.yz, float2(1.0f, -1.0f) / UNITY_MATRIX_P._m00_m11); clippedVS *= flipZ; float4 originCS = mul(UNITY_MATRIX_VP, float4(originWS, 1.0f)); float4 endCS = mul(UNITY_MATRIX_P, float4(clippedVS, 1.0f)); #else float4 originCS = mul(UNITY_MATRIX_VP, float4(originWS, 1.0f)); float4 endCS = mul(UNITY_MATRIX_VP, float4(endWS, 1.0f)); #endif float k0 = 1.0f / originCS.w; float k1 = 1.0f / endCS.w; float3 q0 = originCS.xyz; float3 q1 = endCS.xyz; float2 p0 = originCS.xy * float2(1.0f, -1.0f) * k0 * 0.5f + 0.5f; float2 p1 = endCS.xy * float2(1.0f, -1.0f) * k1 * 0.5f + 0.5f; #if defined(USE_POTENTIAL_HIT) float w1 = 0.0f; float w2 = 0.0f; bool hit = false; bool lastHit = false; bool potentialHit = false; float2 potentialW12 = float2(0.0f, 0.0f); float minPotentialHitPos = INFINITY; [unroll(64)] for (int i=0; i\u0026lt;_StepCount; ++i) { w2 = w1; w1 += 1.0f / float(_StepCount); float3 q = lerp(q0, q1, w1); float2 p = lerp(p0, p1, w1); float k = lerp(k0, k1, w1); float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r; float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams); float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams); float hitDiff = linearRayDepth - linearSampleDepth; float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams); if (hitDiff \u0026gt; 0.0f) { if (thicknessDiff \u0026lt; _ThicknessParams.y) { hit = true; break; } else if(!lastHit) { potentialHit = true; if (minPotentialHitPos \u0026gt; thicknessDiff) { minPotentialHitPos = thicknessDiff; potentialW12 = float2(w1, w2); } } } lastHit = hitDiff \u0026gt; 0.0f; } #else float w1 = 0.0f; float w2 = 0.0f; bool hit = false; [unroll(64)] for (int i=0; i\u0026lt;_StepCount; ++i) { w2 = w1; w1 += 1.0f / float(_StepCount); float3 q = lerp(q0, q1, w1); float2 p = lerp(p0, p1, w1); float k = lerp(k0, k1, w1); float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r; #if defined(USE_THICKNESS) float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams); float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams); float hitDiff = linearRayDepth - linearSampleDepth; float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams); if (hitDiff \u0026gt; 0.0f \u0026amp;\u0026amp; thicknessDiff \u0026lt; _ThicknessParams.y) { hit = true; break; } #else if (q.z * k \u0026lt; sampleDepth) { hit = true; break; } #endif } #endif #if defined(USE_POTENTIAL_HIT) if (hit || potentialHit) { if (!hit) { w1 = potentialW12.x; w2 = potentialW12.y; } bool realHit = false; float2 hitPos; float minThicknessDiff = _ThicknessParams.y; [unroll(5)] for (int i=0; i\u0026lt;5; ++i) { float w = 0.5f * (w1 + w2); float3 q = lerp(q0, q1, w); float2 p = lerp(p0, p1, w); float k = lerp(k0, k1, w); float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r; float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams); float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams); float hitDiff = linearRayDepth - linearSampleDepth; if (hitDiff \u0026gt; 0.0f) { w1 = w; if (hit) hitPos = p; } else { w2 = w; } float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams); float absThicknessDiff = abs(thicknessDiff); if (!hit \u0026amp;\u0026amp; absThicknessDiff \u0026lt; minThicknessDiff) { realHit = true; minThicknessDiff = thicknessDiff; hitPos = p; } } if (hit || realHit) color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f; } #elif defined(USE_BINARY_SEARCH) if (hit) { float2 hitPos; [unroll(5)] for (int i=0; i\u0026lt;5; ++i) { float w = 0.5f * (w1 + w2); float3 q = lerp(q0, q1, w); float2 p = lerp(p0, p1, w); float k = lerp(k0, k1, w); float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r; if (q.z * k \u0026lt; sampleDepth) { w1 = w; hitPos = p; } else { w2 = w; } } color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f; } #else if (hit) { float2 hitPos = lerp(p0, p1, w1); color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f; } #endif return float4(color, 1.0f); } ENDHLSL SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; } LOD 100 Pass { HLSLPROGRAM #pragma vertex vert #pragma fragment frag ENDHLSL } } } Future Optimization Currently, there is one aspect worth optimizing: controlling the overall step count based on the pixel distance between p0 and p1. We certainly don’t want to step 64 times for just 10 pixels. However, this is a relatively straightforward task, and I’ll leave it to someone with time to spare. As for random sampling, blurring, and Fresnel effects, let’s consider those when we really need them.\nPostscript This article was translated by Microsoft’s Copilot and I made a few adjustments. What an era we live in!\n","permalink":"https://zznewclear13.github.io/posts/screen-space-reflection-en/","summary":"Screen Space Reflection While screen space reflection (SSR) is a well-known effect, this article aims to introduce a unique method for calculating screen space reflections \u0026ndash; one that I haven’t encountered online before.\nMany online tutorials cover screen space reflection already, and most of them follow a similar process: calculate the reflection direction in world space, use a mostly uniform step size to traverse the world space, and for each step, compute the normalized device coordinates (NDC).","title":"Screen Space Reflection"},{"content":"屏幕空间反射 屏幕空间反射也是一个老生常谈的效果了，但正如本博客的宗旨，要从千篇一律中脱颖而出，这篇文章也将介绍与众不同的，至少我在网上没有见到过的计算屏幕空间反射的方法。\n网上有很多很多的屏幕空间反射的教程，绝大部分的流程是这样的：计算世界空间的反射方向，使用一个大部分情况下是统一的步长在世界空间中步进，对于每一次步进，计算标准化设备空间的坐标，将当前的深度和深度图进行比较，如果在深度图之后，认为发生了交叉，采样当前点的颜色值并返回。这种方法能看到很多很多看上去非常完美的反射效果，但几乎没有人会提及所需要的步进次数，因为它往往高得惊人，关于这点我们后续还会谈到。而且对于不同远近的物体，想要达到比较好反射效果，其需要的步长往往是不同的，也很少有人去做这方面的思考。稍好一些的会考虑在交叉之后做几次二分法查找，这样能够让一段一段的反射后的颜色带上下颠倒，使画面看上去更加连贯，后面也能看到对比。还有一些会考虑在计算标准化设备空间的坐标后，根据坐标和[-1, 1]之间的大小关系，提前结束步进或是对反射的颜色和环境反射进行插值。目前看来最好的步进方法，是预先计算Hierarchical ZBuffer，通过对更高LOD步进的方法，使用更少的步进次数达到同样的步进效果，但是Hierarchical ZBuffer并不是一个所有项目都能有的特性。\n网上能找到的最有用的教程，是Morgan McGuire写的Screen Space Ray Tracing。在他的这篇文章中也提到了为什么在世界空间中步进是不好的，因为世界空间步进的位置在经过透视变换后，很有可能在屏幕空间中没多大变化，也就导致了世界空间步进需要更多的次数来达到较好的反射效果。在这篇文章中展示了一个非常好的方法，计算裁剪空间和屏幕空间的起点和终点的坐标，通过对裁剪空间的z、裁剪空间的1/w、屏幕空间的xy进行线性插值，省去了每一次步进所需要的矩阵运算，十分值得使用。\n本文的目标是，在一个Shader中使用尽量少的步进次数得到正确的反射颜色。随机采样、模糊、菲涅尔效应之类的不在本文的考虑范围之内。本文仅考虑Windows平台下DX11的Shader，这样能省去很多的平台适配的代码，使用的Unity版本是Unity 2022.3.21f1，在文章的最后会附上最终的Shader代码。\n反射的计算 参数的选择 计算反射基本上只需要三个参数，一个是Max Distance，只考虑距离反射点一定范围内的物体带来的反射，一个是Step Count，更多的步进次数带来更精确的反射，同时也增加性能消耗，最后一个是Thickness Params，对于一个物体，默认其厚度为depth * _Thickness.y + _Thickness.x，这样当射线经过物体背面时不会被认为发生了交叉。\n深度比较 步进的时候比较什么深度也是一个值得思考的问题。将步进的深度记为rayDepth，将采样获得的深度记为sampleDepth，一个很简单的想法在标准化设备空间进行比较，因为直接采样深度图就能获取到标准化设备空间的深度值，当rayDepth \u0026lt; sampleDepth的时候，射线和场景发生了交叉。又或是对实际的深度进行比较，这样能够指定一个厚度，当深度的差大于厚度时，认为射线从场景物体的后面穿了过去并没有发生交叉，当rayDepth \u0026gt; sampleDepth \u0026amp;\u0026amp; rayDepth \u0026lt; sampleDepth + thickness的时候，射线和场景发生了交叉。此外裁剪空间的Z分量也能用来判断是否发生了交叉，这里不再赘述。深度图的采样方式则应该使用PointClamp的方式，使用线性插值的话在一前一后的两个面的边缘很可能会被认为发生了交叉，导致画面上有不少的小点，除非另外有一张标记物体边缘的贴图可以用来排除掉这部分的交叉点。\n光线步进 伪代码很简单：\n记k0、k1分别是步进起点和终点的裁剪空间坐标的w分量的倒数。 记q0、q1分别是步进起点和终点的裁剪空间坐标的xyz分量。 记p0、p1分别是步进起点和终点的标准化设备空间坐标的xy分量。 记w是一个在(0, 1)之间按照1.0f/_StepCount递增的变量。 对每一次步进，更新w的值，并对上面的三组分量线性插值得到k、q、p。 使用q.z * k获得rayDepth，使用p采样深度图获得sampleDepth。 如果rayDepth \u0026lt; sampleDepth，射线和场景发生了交叉，跳出循环，返回p。 使用p采样颜色图，获得反射的颜色。 效果是这样的（步进次数为32次）: 看上去非常糟糕，最明显的是拉扯的效果。它主要有两个产生的原因：一是我们并没有使用厚度来判断射线是否从物体的背面穿过，这导致了悬空的物体下方会有很长的拉扯；二是我们并没有对超出屏幕范围的位置进行限制，这导致了我们使用屏幕外的坐标采样深度图但返回了Clamp之后的深度值。\n厚度检测 为了解决上面的厚度问题，我们新增了一个方法由于判断步进的位置是否在物体后面。我们需要使用的是距离相机的线性深度linearRayDepth和linearSampleDepth。上文说到我们使用linearSampleDepth * _Thickness.y + _Thickness.x来作为一个场景中一个物体的厚度，我们只需要判断(linearRayDepth-linearSampleDepth-_Thickness.x) / linearSampleDepth和_Thickness.y的大小即可，如果前式大于后式，则表明射线从物体后面穿过。\nfloat getThicknessDiff(float diff, float linearSampleDepth, float2 thicknessParams) { return (diff - thicknessParams.x) / linearSampleDepth; } 伪代码变成了：\n如果rayZ \u0026lt; sampleZ且thicknessDiff \u0026lt; _Thickness.y，射线和场景发生了交叉，跳出循环，返回p。 效果是这样的（步进次数为32次）: 视锥体裁剪 对于超出屏幕空间的p1，会带来两个坏处，一是采样了超出范围的深度图得到了错误的深度值，二是减少了有效采样的次数。因此我们可以考虑将p1限制在屏幕空间内，这里新增了一个方法用于将步进终点限制在视锥体内部。我们记nf为近裁剪面深度和远裁剪面深度值（正数），s为视锥体的左右和上下的斜率（正数），s在数值上为float2(asepect * tan(fovy * 0.5f), tan(fovy * 0.5f)，注意为了方便计算，这里from和to的z分量为正数。下面的算法应该还能优化一些，不过已经够用了。\n事实上我还写了一个Shadertoy用来演示，使用鼠标进行交互：\nFrustum Clip 2D\n#define INFINITY 1e10 float3 frustumClip(float3 from, float3 to, float2 nf, float2 s) { float3 dir = to - from; float3 signDir = sign(dir); float nfSlab = signDir.z * (nf.y - nf.x) * 0.5f + (nf.y + nf.x) * 0.5f; float lenZ = (nfSlab - from.z) / dir.z; if (dir.z == 0.0f) lenZ = INFINITY; float2 ss = sign(dir.xy - s * dir.z) * s; float2 denom = ss * dir.z - dir.xy; float2 lenXY = (from.xy - ss * from.z) / denom; if (lenXY.x \u0026lt; 0.0f || denom.x == 0.0f) lenXY.x = INFINITY; if (lenXY.y \u0026lt; 0.0f || denom.y == 0.0f) lenXY.y = INFINITY; float len = min(min(1.0f, lenZ), min(lenXY.x, lenXY.y)); float3 clippedVS = from + dir * len; return clippedVS; } 伪代码变成了：\n将步进终点进行视锥体裁剪得到clippedPosVS，再进一步得到终点的裁剪空间坐标endCS。 效果是这样的（步进次数为32次）: 看上去有那么点反射的意思了，视锥体剔除一定程度地减少了每次步进的像素数，因此补上了一部分的窟窿。但是反射的颜色扭扭曲曲的。\n二分法查找 我们上一步获得的p虽然确保了在物体内部，但距离实际的交点仍有一部分的距离，我们可以通过二分法查找减少两者之间的误差。为了进行二分法查找，我们需要记录最后两次步进的w值，记为w1和w2（w1 \u0026gt; w2）。每次二分法时，取w = 0.5f * (w1 + w2)，如果检测到相交，则w1 = w，否则w2 = w，进入下一个循环。\n伪代码变成了：\n将步进终点进行视锥体裁剪得到clippedPosVS，再进一步得到终点的裁剪空间坐标endCS。 记k0、k1分别是步进起点和终点的裁剪空间坐标的w分量的倒数。 记q0、q1分别是步进起点和终点的裁剪空间坐标的xyz分量。 记p0、p1分别是步进起点和终点的标准化设备空间坐标的xy分量。 记w1是一个在(0, 1)之间按照1.0f/_StepCount递增的变量，w1和w2初始化为0。 对每一次步进，w2=w1，更新w1的值，并对上面的三组分量线性插值得到k、q、p。 使用q.z * k获得rayDepth，使用p采样深度图获得sampleDepth。 如果rayZ \u0026lt; sampleZ且thicknessDiff \u0026lt; _Thickness.y，射线和场景发生了交叉，跳出循环。 记w为w1和w2的平均数，按照567判断是否发生交叉，根据是否交叉更新w1或w2，直到结束二分法循环。 使用p采样颜色图，获得反射的颜色。 效果是这样的（步进次数为32次，二分法查找次数为5次）: 可以看到反射的效果看上去不那么扭扭曲曲的了（左下角尤为明显），但是两段颜色之间仍然有着空隙，这来自于我们的厚度测试，它将潜在的交叉排除在外了。\n潜在的交叉 为了计算潜在的交叉，我们需要回顾之前做厚度测试的代码。当射线穿过物体后面时，如果前一次射线还在物体前方，我们可以记录两者之间的差距thicknessDiff，如果它的值小于最小的差距minThicknessDiff，我们将其作为潜在的交叉，更新minThicknessDiff，并记录当前的w1和w2用于后续的二分法查找。在二分法时我们需要判断发生了交叉还是发生了潜在的交叉，如果发生了交叉，我们执行原有的代码，如果发生的是潜在的交叉，在二分法时我们也需要记录thicknessDiff，找到最小的小于_Thickness.y的thicknessDiff，使用当前w插值得到的p采样获得最终的颜色。\n伪代码变成了：\n将步进终点进行视锥体裁剪得到clippedPosVS，再进一步得到终点的裁剪空间坐标endCS。 记k0、k1分别是步进起点和终点的裁剪空间坐标的w分量的倒数。 记q0、q1分别是步进起点和终点的裁剪空间坐标的xyz分量。 记p0、p1分别是步进起点和终点的标准化设备空间坐标的xy分量。 记w1是一个在(0, 1)之间按照1.0f/_StepCount递增的变量，w1和w2初始化为0。 对每一次步进，w2=w1，更新w1的值，并对上面的三组分量线性插值得到k、q、p。 使用q.z * k获得rayDepth，使用p采样深度图获得sampleDepth。 如果rayZ \u0026lt; sampleZ且thicknessDiff \u0026lt; _Thickness.y，射线和场景发生了交叉，跳出循环。 否则如果rayZ \u0026lt; sampleZ且thicknessDiff \u0026gt; _Thickness.y且上一次射线在物体前方，将thicknessDiff和最小值进行比较，如果更小则更新最小值，记录此时的w1和w2，记为发生了潜在的交叉，继续循环。 如果发生了交叉，记w为w1和w2的平均数，按照567判断是否发生交叉，根据是否交叉更新w1或w2，直到结束二分法循环。 否则如果发生了潜在交叉，按照567判断是否发生交叉，使用最小的thicknessDiff更新p。 使用p采样颜色图，获得反射的颜色。 效果是这样的（步进次数为32次，二分法查找次数为5次）: 下图是步进次数为64次，二分法查找5次的效果： 最终的代码 /* // Copyright (c) 2024 zznewclear@gmail.com // // Permission is hereby granted, free of charge, to any person obtaining a copy // of this software and associated documentation files (the \u0026#34;Software\u0026#34;), to deal // in the Software without restriction, including without limitation the rights // to use, copy, modify, merge, publish, distribute, sublicense, and/or sell // copies of the Software, and to permit persons to whom the Software is // furnished to do so, subject to the following conditions: // // The above copyright notice and this permission notice shall be included in all // copies or substantial portions of the Software. // // THE SOFTWARE IS PROVIDED \u0026#34;AS IS\u0026#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR // IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, // FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE // AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER // LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, // OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE // SOFTWARE. */ Shader \u0026#34;zznewclear13/SSRShader\u0026#34; { Properties { [Toggle(USE_POTENTIAL_HIT)] _UsePotentialHit (\u0026#34;Use Potential Hit\u0026#34;, Float) = 1.0 [Toggle(USE_FRUSTUM_CLIP)] _UseFrustumClip (\u0026#34;Use Frustum Clip\u0026#34;, Float) = 1.0 [Toggle(USE_BINARY_SEARCH)] _UseBinarySearch (\u0026#34;Use Binary Search\u0026#34;, Float) = 1.0 [Toggle(USE_THICKNESS)] _UseThickness (\u0026#34;Use Thickness\u0026#34;, Float) = 1.0 _MaxDistance (\u0026#34;Max Distance\u0026#34;, Range(0.1, 100.0)) = 15.0 [int] _StepCount (\u0026#34;Step Count\u0026#34;, Float) = 32 _ThicknessParams (\u0026#34;Thickness Params\u0026#34;, Vector) = (0.1, 0.02, 0.0, 0.0) } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; #pragma shader_feature _ USE_POTENTIAL_HIT #pragma shader_feature _ USE_FRUSTUM_CLIP #pragma shader_feature _ USE_BINARY_SEARCH #pragma shader_feature _ USE_THICKNESS #define INFINITY 1e10 #define DEPTH_SAMPLER sampler_PointClamp Texture2D _CameraOpaqueTexture; Texture2D _CameraDepthTexture; CBUFFER_START(UnityPerMaterial) float _MaxDistance; int _StepCount; float2 _ThicknessParams; CBUFFER_END struct Attributes { float4 positionOS : POSITION; float3 normalOS : NORMAL; float2 texcoord : TEXCOORD0; }; struct Varyings { float4 positionCS : SV_POSITION; float3 positionWS : TEXCOORD0; float3 normalWS : TEXCOORD1; float2 uv : TEXCOORD2; float3 viewWS : TEXCOORD3; }; Varyings vert(Attributes input) { Varyings output = (Varyings)0; VertexPositionInputs vpi = GetVertexPositionInputs(input.positionOS.xyz); VertexNormalInputs vni = GetVertexNormalInputs(input.normalOS); output.positionCS = vpi.positionCS; output.positionWS = vpi.positionWS; output.normalWS = vni.normalWS; output.uv = input.texcoord; output.viewWS = GetCameraPositionWS() - vpi.positionWS; return output; } float3 frustumClip(float3 from, float3 to, float2 nf, float2 s) { float3 dir = to - from; float3 signDir = sign(dir); float nfSlab = signDir.z * (nf.y - nf.x) * 0.5f + (nf.y + nf.x) * 0.5f; float lenZ = (nfSlab - from.z) / dir.z; if (dir.z == 0.0f) lenZ = INFINITY; float2 ss = sign(dir.xy - s * dir.z) * s; float2 denom = ss * dir.z - dir.xy; float2 lenXY = (from.xy - ss * from.z) / denom; if (lenXY.x \u0026lt; 0.0f || denom.x == 0.0f) lenXY.x = INFINITY; if (lenXY.y \u0026lt; 0.0f || denom.y == 0.0f) lenXY.y = INFINITY; float len = min(min(1.0f, lenZ), min(lenXY.x, lenXY.y)); float3 clippedVS = from + dir * len; return clippedVS; } float getThicknessDiff(float diff, float linearSampleDepth, float2 thicknessParams) { return (diff - thicknessParams.x) / linearSampleDepth; } float4 frag(Varyings input) : SV_TARGET { float3 positionWS = input.positionWS; float3 normalWS = normalize(input.normalWS); float3 viewWS = normalize(input.viewWS); float3 reflWS = reflect(-viewWS, normalWS); float3 env = GlossyEnvironmentReflection(reflWS, 0.0f, 1.0f); float3 color = env; float3 originWS = positionWS; float3 endWS = positionWS + reflWS * _MaxDistance; #if defined(USE_FRUSTUM_CLIP) float3 originVS = mul(UNITY_MATRIX_V, float4(originWS, 1.0f)).xyz; float3 endVS = mul(UNITY_MATRIX_V, float4(endWS, 1.0f)).xyz; float3 flipZ = float3(1.0f, 1.0f, -1.0f); float3 clippedVS = frustumClip(originVS * flipZ, endVS * flipZ, _ProjectionParams.yz, float2(1.0f, -1.0f) / UNITY_MATRIX_P._m00_m11); clippedVS *= flipZ; float4 originCS = mul(UNITY_MATRIX_VP, float4(originWS, 1.0f)); float4 endCS = mul(UNITY_MATRIX_P, float4(clippedVS, 1.0f)); #else float4 originCS = mul(UNITY_MATRIX_VP, float4(originWS, 1.0f)); float4 endCS = mul(UNITY_MATRIX_VP, float4(endWS, 1.0f)); #endif float k0 = 1.0f / originCS.w; float k1 = 1.0f / endCS.w; float3 q0 = originCS.xyz; float3 q1 = endCS.xyz; float2 p0 = originCS.xy * float2(1.0f, -1.0f) * k0 * 0.5f + 0.5f; float2 p1 = endCS.xy * float2(1.0f, -1.0f) * k1 * 0.5f + 0.5f; #if defined(USE_POTENTIAL_HIT) float w1 = 0.0f; float w2 = 0.0f; bool hit = false; bool lastHit = false; bool potentialHit = false; float2 potentialW12 = float2(0.0f, 0.0f); float minPotentialHitPos = INFINITY; [unroll(64)] for (int i=0; i\u0026lt;_StepCount; ++i) { w2 = w1; w1 += 1.0f / float(_StepCount); float3 q = lerp(q0, q1, w1); float2 p = lerp(p0, p1, w1); float k = lerp(k0, k1, w1); float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r; float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams); float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams); float hitDiff = linearRayDepth - linearSampleDepth; float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams); if (hitDiff \u0026gt; 0.0f) { if (thicknessDiff \u0026lt; _ThicknessParams.y) { hit = true; break; } else if(!lastHit) { potentialHit = true; if (minPotentialHitPos \u0026gt; thicknessDiff) { minPotentialHitPos = thicknessDiff; potentialW12 = float2(w1, w2); } } } lastHit = hitDiff \u0026gt; 0.0f; } #else float w1 = 0.0f; float w2 = 0.0f; bool hit = false; [unroll(64)] for (int i=0; i\u0026lt;_StepCount; ++i) { w2 = w1; w1 += 1.0f / float(_StepCount); float3 q = lerp(q0, q1, w1); float2 p = lerp(p0, p1, w1); float k = lerp(k0, k1, w1); float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r; #if defined(USE_THICKNESS) float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams); float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams); float hitDiff = linearRayDepth - linearSampleDepth; float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams); if (hitDiff \u0026gt; 0.0f \u0026amp;\u0026amp; thicknessDiff \u0026lt; _ThicknessParams.y) { hit = true; break; } #else if (q.z * k \u0026lt; sampleDepth) { hit = true; break; } #endif } #endif #if defined(USE_POTENTIAL_HIT) if (hit || potentialHit) { if (!hit) { w1 = potentialW12.x; w2 = potentialW12.y; } bool realHit = false; float2 hitPos; float minThicknessDiff = _ThicknessParams.y; [unroll(5)] for (int i=0; i\u0026lt;5; ++i) { float w = 0.5f * (w1 + w2); float3 q = lerp(q0, q1, w); float2 p = lerp(p0, p1, w); float k = lerp(k0, k1, w); float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r; float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams); float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams); float hitDiff = linearRayDepth - linearSampleDepth; if (hitDiff \u0026gt; 0.0f) { w1 = w; if (hit) hitPos = p; } else { w2 = w; } float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams); float absThicknessDiff = abs(thicknessDiff); if (!hit \u0026amp;\u0026amp; absThicknessDiff \u0026lt; minThicknessDiff) { realHit = true; minThicknessDiff = thicknessDiff; hitPos = p; } } if (hit || realHit) color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f; } #elif defined(USE_BINARY_SEARCH) if (hit) { float2 hitPos; [unroll(5)] for (int i=0; i\u0026lt;5; ++i) { float w = 0.5f * (w1 + w2); float3 q = lerp(q0, q1, w); float2 p = lerp(p0, p1, w); float k = lerp(k0, k1, w); float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r; if (q.z * k \u0026lt; sampleDepth) { w1 = w; hitPos = p; } else { w2 = w; } } color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f; } #else if (hit) { float2 hitPos = lerp(p0, p1, w1); color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f; } #endif return float4(color, 1.0f); } ENDHLSL SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; } LOD 100 Pass { HLSLPROGRAM #pragma vertex vert #pragma fragment frag ENDHLSL } } } 优化的方向 目前还有一个值得优化的方向，就是根据p0和p1之间的像素距离控制总体的步进次数，总不至于对10个像素步进64次吧。不过这个就比较简单了，留给有空的人来做吧。至于随机采样、模糊和菲涅尔，等到真的用到的时候再去考虑吧。\n后记 2024简直就是开幕雷击，各种糟糕的事情接踵而至，有时候会有深深的无力感。可能只有写博客和Shadertoy才能给我带来最强的满足感吧，希望真的有人能从我的博客和Shadertoy中有所收获。本来是打算写一篇Contact Shadow的，但是发现屏幕空间ray marching其实并不是那么一件简单的事情，因此想先写完这个再继续我的Contact Shadow。说实在的我对这篇博客的质量还是比较满意的，打算再写个英文版的去投稿Graphics Programming weekly，拭目以待.gif。\n","permalink":"https://zznewclear13.github.io/posts/screen-space-reflection/","summary":"屏幕空间反射 屏幕空间反射也是一个老生常谈的效果了，但正如本博客的宗旨，要从千篇一律中脱颖而出，这篇文章也将介绍与众不同的，至少我在网上没有见到过的计算屏幕空间反射的方法。\n网上有很多很多的屏幕空间反射的教程，绝大部分的流程是这样的：计算世界空间的反射方向，使用一个大部分情况下是统一的步长在世界空间中步进，对于每一次步进，计算标准化设备空间的坐标，将当前的深度和深度图进行比较，如果在深度图之后，认为发生了交叉，采样当前点的颜色值并返回。这种方法能看到很多很多看上去非常完美的反射效果，但几乎没有人会提及所需要的步进次数，因为它往往高得惊人，关于这点我们后续还会谈到。而且对于不同远近的物体，想要达到比较好反射效果，其需要的步长往往是不同的，也很少有人去做这方面的思考。稍好一些的会考虑在交叉之后做几次二分法查找，这样能够让一段一段的反射后的颜色带上下颠倒，使画面看上去更加连贯，后面也能看到对比。还有一些会考虑在计算标准化设备空间的坐标后，根据坐标和[-1, 1]之间的大小关系，提前结束步进或是对反射的颜色和环境反射进行插值。目前看来最好的步进方法，是预先计算Hierarchical ZBuffer，通过对更高LOD步进的方法，使用更少的步进次数达到同样的步进效果，但是Hierarchical ZBuffer并不是一个所有项目都能有的特性。\n网上能找到的最有用的教程，是Morgan McGuire写的Screen Space Ray Tracing。在他的这篇文章中也提到了为什么在世界空间中步进是不好的，因为世界空间步进的位置在经过透视变换后，很有可能在屏幕空间中没多大变化，也就导致了世界空间步进需要更多的次数来达到较好的反射效果。在这篇文章中展示了一个非常好的方法，计算裁剪空间和屏幕空间的起点和终点的坐标，通过对裁剪空间的z、裁剪空间的1/w、屏幕空间的xy进行线性插值，省去了每一次步进所需要的矩阵运算，十分值得使用。\n本文的目标是，在一个Shader中使用尽量少的步进次数得到正确的反射颜色。随机采样、模糊、菲涅尔效应之类的不在本文的考虑范围之内。本文仅考虑Windows平台下DX11的Shader，这样能省去很多的平台适配的代码，使用的Unity版本是Unity 2022.3.21f1，在文章的最后会附上最终的Shader代码。\n反射的计算 参数的选择 计算反射基本上只需要三个参数，一个是Max Distance，只考虑距离反射点一定范围内的物体带来的反射，一个是Step Count，更多的步进次数带来更精确的反射，同时也增加性能消耗，最后一个是Thickness Params，对于一个物体，默认其厚度为depth * _Thickness.y + _Thickness.x，这样当射线经过物体背面时不会被认为发生了交叉。\n深度比较 步进的时候比较什么深度也是一个值得思考的问题。将步进的深度记为rayDepth，将采样获得的深度记为sampleDepth，一个很简单的想法在标准化设备空间进行比较，因为直接采样深度图就能获取到标准化设备空间的深度值，当rayDepth \u0026lt; sampleDepth的时候，射线和场景发生了交叉。又或是对实际的深度进行比较，这样能够指定一个厚度，当深度的差大于厚度时，认为射线从场景物体的后面穿了过去并没有发生交叉，当rayDepth \u0026gt; sampleDepth \u0026amp;\u0026amp; rayDepth \u0026lt; sampleDepth + thickness的时候，射线和场景发生了交叉。此外裁剪空间的Z分量也能用来判断是否发生了交叉，这里不再赘述。深度图的采样方式则应该使用PointClamp的方式，使用线性插值的话在一前一后的两个面的边缘很可能会被认为发生了交叉，导致画面上有不少的小点，除非另外有一张标记物体边缘的贴图可以用来排除掉这部分的交叉点。\n光线步进 伪代码很简单：\n记k0、k1分别是步进起点和终点的裁剪空间坐标的w分量的倒数。 记q0、q1分别是步进起点和终点的裁剪空间坐标的xyz分量。 记p0、p1分别是步进起点和终点的标准化设备空间坐标的xy分量。 记w是一个在(0, 1)之间按照1.0f/_StepCount递增的变量。 对每一次步进，更新w的值，并对上面的三组分量线性插值得到k、q、p。 使用q.z * k获得rayDepth，使用p采样深度图获得sampleDepth。 如果rayDepth \u0026lt; sampleDepth，射线和场景发生了交叉，跳出循环，返回p。 使用p采样颜色图，获得反射的颜色。 效果是这样的（步进次数为32次）: 看上去非常糟糕，最明显的是拉扯的效果。它主要有两个产生的原因：一是我们并没有使用厚度来判断射线是否从物体的背面穿过，这导致了悬空的物体下方会有很长的拉扯；二是我们并没有对超出屏幕范围的位置进行限制，这导致了我们使用屏幕外的坐标采样深度图但返回了Clamp之后的深度值。\n厚度检测 为了解决上面的厚度问题，我们新增了一个方法由于判断步进的位置是否在物体后面。我们需要使用的是距离相机的线性深度linearRayDepth和linearSampleDepth。上文说到我们使用linearSampleDepth * _Thickness.y + _Thickness.x来作为一个场景中一个物体的厚度，我们只需要判断(linearRayDepth-linearSampleDepth-_Thickness.x) / linearSampleDepth和_Thickness.y的大小即可，如果前式大于后式，则表明射线从物体后面穿过。\nfloat getThicknessDiff(float diff, float linearSampleDepth, float2 thicknessParams) { return (diff - thicknessParams.","title":"屏幕空间反射"},{"content":"Bloom辉光效果 一直都想做一个Bloom效果，Bloom是一个很简单的效果，几乎所有介绍后处理的教程里都会提到Bloom效果的制作，但是Bloom又是一个不那么简单的效果，大部分教程制作出来的Bloom看上去都不太好看。\n想要做好Bloom，首先得认识到什么是Bloom效果。Bloom是由于透镜不能完美地让光线聚焦与同一点而导致图像上的高亮区域的颜色向周围溢出的效果，和体积雾这样的由于多次散射和折射形成的溢出效果在原理上就不相同。在计算机图形学里往往使用多次模糊的方式来表现这种效果。\n而在讨论什么是好的Bloom之前，我们先来看看差的Bloom的效果。Matthew Gallant在他的文章Bloom Disasters中就给出了很多当时的极糟糕的Bloom效果的例子。可以看到Bloom很重要的一点是，Bloom之前的画面必须要是HDR的画面，如果整个画面被限制在01之间，那么白色的T恤和特别亮以至于看上去是白色的太阳带来的Bloom效果就会相同。在LearnOpenGL上有那么一篇文章，其中说到，为了模拟我们眼睛的工作原理，我们不对颜色进行阈值限制，而是直接对HDR画面进行模糊再和原HDR画面进行插值。我认为这是一种十分错误的方式。最合理的方式应当是，画面上的每个颜色确实会向周围溢出自己的颜色，但是更亮的颜色的溢出半径会更大，对于较暗的颜色，由于溢出半径小于半个像素宽，在最后的画面中就看不到颜色的溢出了。但是根据明度来控制溢出的半径是一件很复杂的事情（这和景深的原理是一样的，所以我到现在都没有掌握一个很好的景深的算法），因此我们在计算的时候通过仅模糊超出阈值的颜色来模拟这种效果。模糊半径也是一个决定Bloom质量的关键要素，如果模糊的半径比较小，看上去就像高光套了一个稍弱的圈一样，不够美观。\nJorge Jimenez在2014年Siggraph多的Advances in Real-Time Rendering课程上介绍了他为使命召唤现代战争所做的次世代后处理效果。他的PPT里介绍了使命召唤现代战争中运动模糊、散景、次表面散射、Bloom和阴影采样的做法，十分值得一看。本文在整体的算法上就使用了他介绍的方法，而采样则使用了Dual Kawase Blur的算法，可以看我之前的文章。\nBloom的算法 主流的Bloom算法都会使用一个阈值，第一个Pass提取出大于这个阈值的颜色（使用减法，这样能够和小于阈值的颜色形成自然的过渡），然后进行一系列的降采样升采样以减少采样的次数，最后将之前所有的升采样的模糊结果（就相当于是Mip的每一级）叠加到一开始的颜色上。为了减少最后一步采样所有的Mip等级带来的消耗，根据Jorge Jimenez的做法，我们会在每一步升采样时叠加当前Mip的颜色。但是最后叠加不是一个很好的处理方法，由于叠加了各个Mip的颜色，会导致原来高光的区域的亮度会被提高到原来的两倍甚至更多，不过我们之后的Tone Mapping能够一定程度上缓解这个问题。然后是对微小的高亮物体的处理，Jorge Jimenez使用了1/(1 + Luma)的方式进行加权处理，不过如果我们将Bloom移动到TAA之后，这个问题能够很好的解决掉。至于模糊，在我之前那么多文章的铺垫下，也就不是什么难点了。\n我自己在实现的时候，会在最后一步叠加Mip到最一开始的图像时，将模糊后的颜色除以所有的降采样次数，这样能够稍微弥补一下多个Mip带来的亮度剧烈增加的问题。事实上我也想过将因为阈值而丢失的亮度储存在透明通道里，和颜色一起参与模糊，在最后的时候加回之前丢失的亮度，最后和原始颜色线性插值，不过似乎不那么好做。\n这里可以对比一下Unity自带的Bloom和我的Bloom之间的效果差异。Unity第一个Pass预过滤会进行13次采样，之后每一次降采样分成横竖两个方向，横向9次采样，竖向5次采样，升采样则是在2次采样中线性插值。我的则是每次降采样进行5次采样，每次升采样进行8+1次采样，不需要分横竖采样。下图上边是Unity自带的Bloom，下边是我的Bloom，最后均使用Aces Tonemapping，场景里的大立方体的大小是相邻小立方体的1.5倍，而小立方体的亮度是相邻大立方体的1.5倍。我尽量地将参数调的差不多，Unity一共22个Draw最高Mip为7，我的一共17个Draw最高Mip为8，可以观察到Unity的会有稍微明显一点的Banding，中间亮度的中间大小的物体带来的Bloom比我的稍微大一些。\nUnity Default Bloom\nMy High Quality Bloom\nBloom的具体实现 HQBloomComputeShader.compute 这里有四个Kernel，HQBloomWeightedDownsample用于第一次降采样时减去阈值并加权进行模糊，HQBloomDownsample是和Dual Kawase Blur一样的降采样的模糊，HQBloomAdditiveUpsample是在Dual Kawase Blur的升采样的基础上和低一级的Mip叠加，HQBloomComposite则是将最低一级Mip和原始颜色进行混合。\n#pragma kernel HQBloomDownsample #pragma kernel HQBloomWeightedDownsample #pragma kernel HQBloomAdditiveUpsample #pragma kernel HQBloomComposite #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; Texture2D\u0026lt;float4\u0026gt; _SourceTexture; Texture2D\u0026lt;float4\u0026gt; _ColorTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTexture; SamplerState sampler_LinearClamp; float4 _SourceSize; float4 _TargetSize; float _Threshold; float _InvDownsampleCount; float _BloomIntensity; float3 applyThreshold(float3 color, out float luma) { luma = Luminance(color); return color * max(0.0f, luma - _Threshold); } float getLumaWeight(float luma) { return rcp(1.0f + luma); } float getLumaWeight(float3 color) { float luma = Luminance(color); return rcp(1.0f + luma); } float3 sampleSource(float2 center, float2 offset) { return _SourceTexture.SampleLevel(sampler_LinearClamp, center + offset, 0.0f).rgb; } [numthreads(8, 8, 1)] void HQBloomDownsample(uint3 id : SV_DispatchThreadID) { float2 uv = (float2(id.xy) + 0.5f) * _TargetSize.zw; float2 halfPixel = 0.5f * _TargetSize.zw; float3 c = sampleSource(uv, float2(0.0f, 0.0f)); float3 tl = sampleSource(uv, halfPixel * float2(-1.0f, +1.0f)); float3 tr = sampleSource(uv, halfPixel * float2(+1.0f, +1.0f)); float3 bl = sampleSource(uv, halfPixel * float2(-1.0f, -1.0f)); float3 br = sampleSource(uv, halfPixel * float2(+1.0f, -1.0f)); float3 color = (tl + tr + bl + br + c * 4.0f) / 8.0f; _RW_TargetTexture[id.xy] = float4(color, 1.0f); } [numthreads(8, 8, 1)] void HQBloomWeightedDownsample(uint3 id : SV_DispatchThreadID) { float2 uv = (float2(id.xy) + 0.5f) * _TargetSize.zw; float2 halfPixel = 0.5f * _TargetSize.zw; float lumac, lumatl, lumatr, lumabl, lumabr; float3 c = applyThreshold(sampleSource(uv, float2(0.0f, 0.0f)), lumac); float3 tl = applyThreshold(sampleSource(uv, halfPixel * float2(-1.0f, +1.0f)), lumatl); float3 tr = applyThreshold(sampleSource(uv, halfPixel * float2(+1.0f, +1.0f)), lumatr); float3 bl = applyThreshold(sampleSource(uv, halfPixel * float2(-1.0f, -1.0f)), lumabl); float3 br = applyThreshold(sampleSource(uv, halfPixel * float2(+1.0f, -1.0f)), lumabr); float3 wc = getLumaWeight(lumac); float3 wtl = getLumaWeight(lumatl); float3 wtr = getLumaWeight(lumatr); float3 wbl = getLumaWeight(lumabl); float3 wbr = getLumaWeight(lumabr); float3 colorSum = tl * wtl + tr * wtr + bl * wbl + br * wbr + c * wc * 4.0f; float3 weightSum = wtl + wtr + wbl + wbr + wc * 4.0f; float3 color = colorSum / weightSum; _RW_TargetTexture[id.xy] = float4(color, 1.0f); } [numthreads(8, 8, 1)] void HQBloomAdditiveUpsample(uint3 id : SV_DispatchThreadID) { float2 uv = (float2(id.xy) + 0.5f) * _TargetSize.zw; float2 onePixel = 1.0f * _TargetSize.zw; // float3 c = sampleSource(uv, float2(0.0f, 0.0f)); float3 t2 = sampleSource(uv, onePixel * float2(+0.0f, +2.0f)); float3 b2 = sampleSource(uv, onePixel * float2(+0.0f, -2.0f)); float3 l2 = sampleSource(uv, onePixel * float2(-2.0f, +0.0f)); float3 r2 = sampleSource(uv, onePixel * float2(+2.0f, +0.0f)); float3 tl = sampleSource(uv, onePixel * float2(-1.0f, +1.0f)); float3 tr = sampleSource(uv, onePixel * float2(+1.0f, +1.0f)); float3 bl = sampleSource(uv, onePixel * float2(-1.0f, -1.0f)); float3 br = sampleSource(uv, onePixel * float2(+1.0f, -1.0f)); float3 color = (t2 + b2 + l2 + r2 + 2.0f * (tl + tr + bl + br)) / 12.0f; float3 prevTarget = _RW_TargetTexture.Load(uint3(id.xy, 0)); _RW_TargetTexture[id.xy] = float4(color + prevTarget, 1.0f); } [numthreads(8, 8, 1)] void HQBloomComposite(uint3 id : SV_DispatchThreadID) { float2 uv = (float2(id.xy) + 0.5f) * _TargetSize.zw; float2 onePixel = 1.0f * _TargetSize.zw; // float3 c = sampleSource(uv, float2(0.0f, 0.0f)); float3 t2 = sampleSource(uv, onePixel * float2(+0.0f, +2.0f)); float3 b2 = sampleSource(uv, onePixel * float2(+0.0f, -2.0f)); float3 l2 = sampleSource(uv, onePixel * float2(-2.0f, +0.0f)); float3 r2 = sampleSource(uv, onePixel * float2(+2.0f, +0.0f)); float3 tl = sampleSource(uv, onePixel * float2(-1.0f, +1.0f)); float3 tr = sampleSource(uv, onePixel * float2(+1.0f, +1.0f)); float3 bl = sampleSource(uv, onePixel * float2(-1.0f, -1.0f)); float3 br = sampleSource(uv, onePixel * float2(+1.0f, -1.0f)); float3 color = (t2 + b2 + l2 + r2 + 2.0f * (tl + tr + bl + br)) / 12.0f; float3 colorTexture = _ColorTexture.Load(uint3(id.xy, 0)); float3 bloomColor = colorTexture + color * _BloomIntensity * _InvDownsampleCount; _RW_TargetTexture[id.xy] = float4(bloomColor, 1.0f); } HQBloom.cs 没啥好说的。\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenuForRenderPipeline(\u0026#34;Post-processing/HQ Bloom\u0026#34;, typeof(UniversalRenderPipeline))] public sealed class HQBloom : VolumeComponent, IPostProcessComponent { public BoolParameter isEnabled = new BoolParameter(false); public ClampedFloatParameter intensity = new ClampedFloatParameter(1.0f, 0.0f, 1.0f); public ClampedFloatParameter threshold = new ClampedFloatParameter(0.9f, 0.0f, 5.0f); public ClampedIntParameter downsampleCount = new ClampedIntParameter(7, 3, 10); public bool IsActive() { return isEnabled.value \u0026amp;\u0026amp; intensity.value \u0026gt; 0.0f; } public bool IsTileCompatible() { return false; } } } HQBloomRenderPass.cs 这个脚本相较于Dual Kawase Blur来说要稍微简单一点，因为Bloom效果往往只会调整其强弱，而不会调整半径，不需要在两个降采样之间再做线性插值了。\nusing System.Collections.Generic; namespace UnityEngine.Rendering.Universal { public class HQBloomRenderPass : ScriptableRenderPass { static readonly string passName = \u0026#34;HQ Bloom Render Pass\u0026#34;; private HQBloomRendererFeature.HQBloomSettings settings; private HQBloom hqBloom; private ComputeShader computeShader; static readonly string cameraColorTextureName = \u0026#34;_CameraColorAttachmentA\u0026#34;; static readonly int cameraColorTextureID = Shader.PropertyToID(cameraColorTextureName); RenderTargetIdentifier cameraColorIden; private Vector2Int textureSize; private RenderTextureDescriptor desc; public HQBloomRenderPass(HQBloomRendererFeature.HQBloomSettings settings) { profilingSampler = new ProfilingSampler(passName); this.settings = settings; renderPassEvent = settings.renderPassEvent; computeShader = settings.computeShader; cameraColorIden = new RenderTargetIdentifier(cameraColorTextureID); } public void Setup(HQBloom hqBloom) { this.hqBloom = hqBloom; } public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor) { textureSize = new Vector2Int(cameraTextureDescriptor.width, cameraTextureDescriptor.height); desc = cameraTextureDescriptor; desc.enableRandomWrite = true; desc.msaaSamples = 1; desc.depthBufferBits = 0; } private Vector4 GetTextureSizeParams(Vector2Int size) { return new Vector4(size.x, size.y, 1.0f / size.x, 1.0f / size.y); } private void DoHQBloomDownsample(CommandBuffer cmd, RenderTargetIdentifier sourceid, RenderTargetIdentifier targetid, Vector2Int sourceSize, Vector2Int targetSize, bool firstDownsample, ComputeShader computeShader) { if (!computeShader) return; string kernelName = firstDownsample ? \u0026#34;HQBloomWeightedDownsample\u0026#34; : \u0026#34;HQBloomDownsample\u0026#34;; int kernelID = computeShader.FindKernel(kernelName); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_SourceTexture\u0026#34;, sourceid); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_RW_TargetTexture\u0026#34;, targetid); cmd.SetComputeVectorParam(computeShader, \u0026#34;_SourceSize\u0026#34;, GetTextureSizeParams(sourceSize)); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TargetSize\u0026#34;, GetTextureSizeParams(targetSize)); cmd.SetComputeFloatParam(computeShader, \u0026#34;_Threshold\u0026#34;, hqBloom.threshold.value); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)targetSize.x / x), Mathf.CeilToInt((float)targetSize.y / y), 1); } private void DoHQBloomAdditiveUpsample(CommandBuffer cmd, RenderTargetIdentifier sourceid, RenderTargetIdentifier targetid, Vector2Int sourceSize, Vector2Int targetSize, ComputeShader computeShader) { if (!computeShader) return; string kernelName = \u0026#34;HQBloomAdditiveUpsample\u0026#34;; int kernelID = computeShader.FindKernel(kernelName); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_SourceTexture\u0026#34;, sourceid); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_RW_TargetTexture\u0026#34;, targetid); cmd.SetComputeVectorParam(computeShader, \u0026#34;_SourceSize\u0026#34;, GetTextureSizeParams(sourceSize)); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TargetSize\u0026#34;, GetTextureSizeParams(targetSize)); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)targetSize.x / x), Mathf.CeilToInt((float)targetSize.y / y), 1); } private void DoHQloomComposite(CommandBuffer cmd, RenderTargetIdentifier sourceid, RenderTargetIdentifier colorid, RenderTargetIdentifier targetid, Vector2Int sourceSize, Vector2Int targetSize, ComputeShader computeShader) { if (!computeShader) return; string kernelName = \u0026#34;HQBloomComposite\u0026#34;; int kernelID = computeShader.FindKernel(kernelName); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_SourceTexture\u0026#34;, sourceid); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_ColorTexture\u0026#34;, colorid); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_RW_TargetTexture\u0026#34;, targetid); cmd.SetComputeVectorParam(computeShader, \u0026#34;_SourceSize\u0026#34;, GetTextureSizeParams(sourceSize)); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TargetSize\u0026#34;, GetTextureSizeParams(targetSize)); cmd.SetComputeFloatParam(computeShader, \u0026#34;_InvDownsampleCount\u0026#34;, 1.0f / hqBloom.downsampleCount.value); cmd.SetComputeFloatParam(computeShader, \u0026#34;_BloomIntensity\u0026#34;, hqBloom.intensity.value); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)targetSize.x / x), Mathf.CeilToInt((float)targetSize.y / y), 1); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(); using (new ProfilingScope(cmd, profilingSampler)) { List\u0026lt;int\u0026gt; rtIDs = new List\u0026lt;int\u0026gt;(); List\u0026lt;Vector2Int\u0026gt; rtSizes = new List\u0026lt;Vector2Int\u0026gt;(); RenderTextureDescriptor tempDesc = desc; string bloomRT = \u0026#34;_BloomRT\u0026#34;; int bloomRTID = Shader.PropertyToID(bloomRT); cmd.GetTemporaryRT(bloomRTID, tempDesc); rtIDs.Add(bloomRTID); rtSizes.Add(textureSize); Vector2Int lastSize = textureSize; int lastID = cameraColorTextureID; int downsampleCount = hqBloom.downsampleCount.value; for (int i = 0; i \u0026lt; downsampleCount; i++) { string rtName = \u0026#34;_BloomRT\u0026#34; + i.ToString(); int rtID = Shader.PropertyToID(rtName); Vector2Int rtSize = new Vector2Int((lastSize.x + 1) / 2, (lastSize.y + 1) / 2); tempDesc.width = rtSize.x; tempDesc.height = rtSize.y; cmd.GetTemporaryRT(rtID, tempDesc); rtIDs.Add(rtID); rtSizes.Add(rtSize); DoHQBloomDownsample(cmd, lastID, rtID, lastSize, rtSize, i == 0, computeShader); lastID = rtID; lastSize = rtSize; } for (int i = downsampleCount; i \u0026gt;= 1; i--) { int sourceID = rtIDs[i]; Vector2Int sourceSize = rtSizes[i]; int targetID = rtIDs[i-1]; Vector2Int targetSize = rtSizes[i-1]; if(i == 1) { DoHQloomComposite(cmd, sourceID, cameraColorIden, targetID, sourceSize, targetSize, computeShader); cmd.Blit(targetID, cameraColorIden); cmd.ReleaseTemporaryRT(targetID); } else { DoHQBloomAdditiveUpsample(cmd, sourceID, targetID, sourceSize, targetSize, computeShader); } cmd.ReleaseTemporaryRT(sourceID); } } context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } } } HQBloomRendererFeature.cs 没啥好说的。\nnamespace UnityEngine.Rendering.Universal { public class HQBloomRendererFeature : ScriptableRendererFeature { [System.Serializable] public class HQBloomSettings { public RenderPassEvent renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; public ComputeShader computeShader; } public HQBloomSettings settings = new HQBloomSettings(); private HQBloomRenderPass hqBloomRenderPass; public override void Create() { hqBloomRenderPass = new HQBloomRenderPass(settings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { HQBloom HQBloom = VolumeManager.instance.stack.GetComponent\u0026lt;HQBloom\u0026gt;(); if (HQBloom != null \u0026amp;\u0026amp; HQBloom.IsActive()) { hqBloomRenderPass.Setup(HQBloom); renderer.EnqueuePass(hqBloomRenderPass); } } } } ","permalink":"https://zznewclear13.github.io/posts/unity-high-quality-bloom/","summary":"Bloom辉光效果 一直都想做一个Bloom效果，Bloom是一个很简单的效果，几乎所有介绍后处理的教程里都会提到Bloom效果的制作，但是Bloom又是一个不那么简单的效果，大部分教程制作出来的Bloom看上去都不太好看。\n想要做好Bloom，首先得认识到什么是Bloom效果。Bloom是由于透镜不能完美地让光线聚焦与同一点而导致图像上的高亮区域的颜色向周围溢出的效果，和体积雾这样的由于多次散射和折射形成的溢出效果在原理上就不相同。在计算机图形学里往往使用多次模糊的方式来表现这种效果。\n而在讨论什么是好的Bloom之前，我们先来看看差的Bloom的效果。Matthew Gallant在他的文章Bloom Disasters中就给出了很多当时的极糟糕的Bloom效果的例子。可以看到Bloom很重要的一点是，Bloom之前的画面必须要是HDR的画面，如果整个画面被限制在01之间，那么白色的T恤和特别亮以至于看上去是白色的太阳带来的Bloom效果就会相同。在LearnOpenGL上有那么一篇文章，其中说到，为了模拟我们眼睛的工作原理，我们不对颜色进行阈值限制，而是直接对HDR画面进行模糊再和原HDR画面进行插值。我认为这是一种十分错误的方式。最合理的方式应当是，画面上的每个颜色确实会向周围溢出自己的颜色，但是更亮的颜色的溢出半径会更大，对于较暗的颜色，由于溢出半径小于半个像素宽，在最后的画面中就看不到颜色的溢出了。但是根据明度来控制溢出的半径是一件很复杂的事情（这和景深的原理是一样的，所以我到现在都没有掌握一个很好的景深的算法），因此我们在计算的时候通过仅模糊超出阈值的颜色来模拟这种效果。模糊半径也是一个决定Bloom质量的关键要素，如果模糊的半径比较小，看上去就像高光套了一个稍弱的圈一样，不够美观。\nJorge Jimenez在2014年Siggraph多的Advances in Real-Time Rendering课程上介绍了他为使命召唤现代战争所做的次世代后处理效果。他的PPT里介绍了使命召唤现代战争中运动模糊、散景、次表面散射、Bloom和阴影采样的做法，十分值得一看。本文在整体的算法上就使用了他介绍的方法，而采样则使用了Dual Kawase Blur的算法，可以看我之前的文章。\nBloom的算法 主流的Bloom算法都会使用一个阈值，第一个Pass提取出大于这个阈值的颜色（使用减法，这样能够和小于阈值的颜色形成自然的过渡），然后进行一系列的降采样升采样以减少采样的次数，最后将之前所有的升采样的模糊结果（就相当于是Mip的每一级）叠加到一开始的颜色上。为了减少最后一步采样所有的Mip等级带来的消耗，根据Jorge Jimenez的做法，我们会在每一步升采样时叠加当前Mip的颜色。但是最后叠加不是一个很好的处理方法，由于叠加了各个Mip的颜色，会导致原来高光的区域的亮度会被提高到原来的两倍甚至更多，不过我们之后的Tone Mapping能够一定程度上缓解这个问题。然后是对微小的高亮物体的处理，Jorge Jimenez使用了1/(1 + Luma)的方式进行加权处理，不过如果我们将Bloom移动到TAA之后，这个问题能够很好的解决掉。至于模糊，在我之前那么多文章的铺垫下，也就不是什么难点了。\n我自己在实现的时候，会在最后一步叠加Mip到最一开始的图像时，将模糊后的颜色除以所有的降采样次数，这样能够稍微弥补一下多个Mip带来的亮度剧烈增加的问题。事实上我也想过将因为阈值而丢失的亮度储存在透明通道里，和颜色一起参与模糊，在最后的时候加回之前丢失的亮度，最后和原始颜色线性插值，不过似乎不那么好做。\n这里可以对比一下Unity自带的Bloom和我的Bloom之间的效果差异。Unity第一个Pass预过滤会进行13次采样，之后每一次降采样分成横竖两个方向，横向9次采样，竖向5次采样，升采样则是在2次采样中线性插值。我的则是每次降采样进行5次采样，每次升采样进行8+1次采样，不需要分横竖采样。下图上边是Unity自带的Bloom，下边是我的Bloom，最后均使用Aces Tonemapping，场景里的大立方体的大小是相邻小立方体的1.5倍，而小立方体的亮度是相邻大立方体的1.5倍。我尽量地将参数调的差不多，Unity一共22个Draw最高Mip为7，我的一共17个Draw最高Mip为8，可以观察到Unity的会有稍微明显一点的Banding，中间亮度的中间大小的物体带来的Bloom比我的稍微大一些。\nUnity Default Bloom\nMy High Quality Bloom\nBloom的具体实现 HQBloomComputeShader.compute 这里有四个Kernel，HQBloomWeightedDownsample用于第一次降采样时减去阈值并加权进行模糊，HQBloomDownsample是和Dual Kawase Blur一样的降采样的模糊，HQBloomAdditiveUpsample是在Dual Kawase Blur的升采样的基础上和低一级的Mip叠加，HQBloomComposite则是将最低一级Mip和原始颜色进行混合。\n#pragma kernel HQBloomDownsample #pragma kernel HQBloomWeightedDownsample #pragma kernel HQBloomAdditiveUpsample #pragma kernel HQBloomComposite #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; Texture2D\u0026lt;float4\u0026gt; _SourceTexture; Texture2D\u0026lt;float4\u0026gt; _ColorTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTexture; SamplerState sampler_LinearClamp; float4 _SourceSize; float4 _TargetSize; float _Threshold; float _InvDownsampleCount; float _BloomIntensity; float3 applyThreshold(float3 color, out float luma) { luma = Luminance(color); return color * max(0.","title":"Unity的高质量的Bloom效果"},{"content":"阅读前提示 由于本文使用的贴图均为LearnOpenGL网站上的贴图，其法线贴图和一般Unity或Unreal引擎中的法线贴图的Y分量是相反的，因此在计算世界坐标的bitangent的时候会额外再乘上一个sign，在正常情况下是不需要的。\n视差效果 在三角形面数比较受限的情况下，往往会考虑使用一张高度图，通过视差的计算去渲染出一种3D的效果（虽然现在直接用曲面细分Tessellation似乎是一种更普遍的且更有效的方法）。有两种计算视差的方法，一种叫做Parallax Occlusion Mapping，先假定高度的层数，然后对每一层计算出合适的位置和颜色，从而达到3D效果；另一种叫做Cone Step Mapping，是根据高度图预先计算出每个点对于其他所有像素的最大的圆锥张角（有点像AO），根据圆锥张角快速步进，最后使用二分法计算出最终的交点的颜色。第一种方法有一个比较大的缺点，就是在视角比较接近平面的时候，如果采样次数不是很高，就会看到一层一层的效果，可以通过对最后一次计算深度进行线性插值在一定程度上减轻一层一层的问题；第二种方法的缺点是，当采样次数较小时，产生的图像会有一定程度的扭曲，但不会有一层一层的感觉，此外相较于第一种会有一个优点，较细物体不会被跳过。在GPU Gems 3中提到了一种Cone Step Mapping的优化，叫做Relaxed Cone Step Mapping，相较于之前计算最大张角的方式，这种优化通过确保通过圆锥的射线与圆锥内部的高度图至多只有一个交点，减少了一开始圆锥步进的次数。本文就主要使用这种方法进行计算，也许将圆锥的顶部放在比当前高度图更深的位置能够更加减少步进的次数，不过我稍微尝试了一下好像效果并不是特别理想。\nParallax Occlusion Mapping可以在Learn OpenGL里找到介绍和优化方案，Shadertoy上也有开源的代码可以参考。UE5中有一个叫Get Relief!的插件，可以用来快速生成Relaxed Cone Step Mapping的预计算的贴图，也提供了渲染的Shader。这个插件的作者Daniel Elliott也在GDC2023上分享了制作的思路，如果链接打不开的话这里还有一个GDC Vault的链接。\n本文使用的贴图可以在Learn OpenGL中给出的下载链接中找到。为了看上去舒服一些，这里对displacement贴图的颜色进行了反向。\n下图是两种视差做法的比较，左边是Parallax Occlusion Mapping，右边是Relaxed Cone Step Mapping，两者的采样次数是相同的，可以看到POM在较极限的情况下会有分层感而RCSM会有扭曲。RCSM使用的贴图也放在下面了，R通道是高度图，G通道是圆锥的张角。本文使用的是Unity 2021.3.19f1c1。\n生成预计算的贴图 和Parallax Occlusion Mapping直接使用深度图不同的是，Cone Step Mapping需要预先计算出一张圆锥张角的图，圆锥的张角可以使用圆锥底的半径除以圆锥的高来表示，记为coneRatio。本文中使用的是高度图，但实际计算中会使用1减去高度值，对应的是从模型表面到实际高度的深度值。由于深度值只会在01之间，uv也只会在01之间，因此对于最深的点，其最大的圆锥张角不会大于1。\n“确保通过圆锥的射线与圆锥内部的高度图至多只有一个交点”，对于圆锥顶部的currentPos和圆锥底部的rayStartPos（这个圆锥是一个倒立的圆锥，其底部和模型表面相平），可以采样一个目标点cachedPos，当cachedPos的深度小于currentPos的深度时，沿着cachedPos - rayStartPos的方向移动cachedPos的位置并一直采样所有像素samplePos，直到samplePos的深度值小于cachedPos（即射线穿过高度图并穿出），根据samplePos和currentPos就能计算出一个圆锥的张角coneRatio。循环所有的像素就能得到最小的圆锥张角了。\n为了减少单次计算的消耗，本文会先将整张图片分成NxN大小的区域，在一次循环中会计算所有像素对于这NxN大小的区域的圆锥张角，循环所有的区域就能得到最后的圆锥张角了。同时只需要让N等于THREAD_GROUP_SIZE，就能使用group shared memory仅通过一次采样缓存这些区域的深度值。再有就是Early Exit的优化，当cachedPos在贴图外部，当cachedPos的深度大于currentPos的深度，当cachedPos的圆锥张角大于当前最小的圆锥张角，在这些情况下可以直接结束向外步进的循环。更多的优化方法也都能在Get Relief!的分享中找到。\n具体的代码 RCSMComputeShader.compute 用于生成Relaxed Cone Step Mapping的贴图。PreProcessMain用于处理最一开始的深度图，预先设置最大的coneRatio为1。Early Exit是减少运算时间的关键。\n#pragma kernel PreProcessMain #pragma kernel RCSMMain #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; Texture2D\u0026lt;float4\u0026gt; _SourceTex; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTex; SamplerState sampler_LinearClamp; float4 _TextureSize; float2 _CacheOffset; #define THREAD_GROUP_SIZE 16u [numthreads(8, 8, 1)] void PreProcessMain(uint3 id : SV_DispatchThreadID) { uint2 tempID = uint2(id.x, _TextureSize.y - 1.0f - id.y); float sourceTex = _SourceTex.Load(uint3(tempID, 0)).r; _RW_TargetTex[id.xy] = float4(sourceTex, 1.0f, 0.0f, 0.0f); } float3 LoadPos(uint2 coord) { return float3((coord + 0.5f) * _TextureSize.zw, 1.0f - _SourceTex.Load(uint3(coord, 0)).r); } float3 LoadPos(uint2 coord, out float coneRatio) { float2 sourceTex = _SourceTex.Load(uint3(coord, 0)).rg; coneRatio = sourceTex.y; return float3((coord + 0.5f) * _TextureSize.zw, 1.0f - sourceTex.x); } float3 SamplePos(float2 uv) { return float3(uv, 1.0f - _SourceTex.SampleLevel(sampler_LinearClamp, uv, 0.0f).r); } const static uint CACHED_POS_SIZE = THREAD_GROUP_SIZE * THREAD_GROUP_SIZE; groupshared float3 cachedPos[CACHED_POS_SIZE]; void SetCachedPos(float3 pos, uint index) { cachedPos[index] = pos; } float3 GetCachedPos(uint index) { return cachedPos[index]; } void CachePos(uint2 cacheStartPos, uint cacheIndex) { uint2 offset = uint2(cacheIndex % THREAD_GROUP_SIZE, cacheIndex / THREAD_GROUP_SIZE); uint2 sampleCoord = cacheStartPos + offset; float3 pos = LoadPos(sampleCoord); SetCachedPos(pos, cacheIndex); } [numthreads(THREAD_GROUP_SIZE, THREAD_GROUP_SIZE, 1)] void RCSMMain(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { uint2 cacheStartPos = uint2(_CacheOffset)*THREAD_GROUP_SIZE; CachePos(cacheStartPos, groupIndex); GroupMemoryBarrierWithGroupSync(); float coneRatio; float3 currentPos = LoadPos(dispatchThreadID.xy, coneRatio); float3 rayStartPos = float3(currentPos.xy, 0.0f); const int steps = 128; for (uint cacheIndex = 0; cacheIndex \u0026lt; CACHED_POS_SIZE; cacheIndex++) { uint2 offset = uint2(cacheIndex % THREAD_GROUP_SIZE, cacheIndex / THREAD_GROUP_SIZE); uint2 sampleCoord = cacheStartPos + offset; if (any(float2(sampleCoord) \u0026gt;= _TextureSize.xy)) continue; if (length((int2(sampleCoord.xy) - int2(dispatchThreadID.xy)) * _TextureSize.zw) \u0026gt; coneRatio * currentPos.z) continue; if (all(sampleCoord == dispatchThreadID.xy)) continue; float3 cachedPos = GetCachedPos(cacheIndex); float3 dir = cachedPos - rayStartPos; float dirXYLength = length(dir.xy); float3 normalizedDir = dir / dirXYLength; float stepLength = 1.414 * _TextureSize.z; for (int j = 0; j \u0026lt; steps; j++) { cachedPos += stepLength * normalizedDir; if (any(cachedPos.xy \u0026gt;= 1.0f) || any(cachedPos.xy \u0026lt;= 0.0f)) break; if (cachedPos.z \u0026gt; currentPos.z) break; if (length(cachedPos.xy - currentPos.xy) / (currentPos.z - cachedPos.z) \u0026gt; coneRatio) break; float3 samplePos = SamplePos(cachedPos.xy); if (samplePos.z \u0026gt; currentPos.z) continue; float tempConeRatio = length(samplePos.xy - currentPos.xy) / (currentPos.z - samplePos.z); if (tempConeRatio \u0026lt; coneRatio) { coneRatio = tempConeRatio; } } } _RW_TargetTex[dispatchThreadID.xy] = float4(1.0f - currentPos.z, coneRatio, 0.0f, 1.0f); } RelaxedConeStepMappingGenerator.cs 需要注意的是这里保存的格式是tga，如果是存成jpg的话会有压缩的问题。此外还要注意深度图和预计算的贴图储存的不是颜色值，因此不能勾选srgb，coneRatio也不太适合MipMap。\nusing System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEditor; using Unity.EditorCoroutines.Editor; using System.IO; public class RelaxedConeStepMappingGenerator : EditorWindow { private ComputeShader computeShader; private Texture2D texture; private string savePath = \u0026#34;Assets/ParallaxMapping/rcsm\u0026#34;; private static readonly string suffix = \u0026#34;.tga\u0026#34;; private Vector2Int textureSize; private RenderTexture[] rts = new RenderTexture[2]; private EditorCoroutine editorCoroutine; Rect rect { get { return new Rect(20.0f, 20.0f, position.width - 40.0f, position.height - 10.0f); } } private void EnsureRTs() { foreach (var rt in rts) { if (rt != null) rt.Release(); } rts = new RenderTexture[2]; } private void EnsureRT(ref RenderTexture rt, int width, int height) { if(rt == null || rt.width != width || rt.height != height) { if(rt != null) rt.Release(); RenderTextureDescriptor desc = new RenderTextureDescriptor { width = width, height = height, volumeDepth = 1, dimension = UnityEngine.Rendering.TextureDimension.Tex2D, depthBufferBits = 0, msaaSamples = 1, graphicsFormat = UnityEngine.Experimental.Rendering.GraphicsFormat.R8G8B8A8_UNorm, enableRandomWrite = true }; rt = new RenderTexture(desc); if (!rt.IsCreated()) rt.Create(); } } [MenuItem(\u0026#34;zznewclear13/Relaxed Cone Step Mapping Generator\u0026#34;)] public static void Init() { RelaxedConeStepMappingGenerator window = GetWindow\u0026lt;RelaxedConeStepMappingGenerator\u0026gt;(\u0026#34;Relaxed Cone Step Mapping Generator\u0026#34;); window.Show(); window.Repaint(); window.Focus(); } private void OnGUI() { using (new GUILayout.AreaScope(rect)) { computeShader = (ComputeShader)EditorGUILayout.ObjectField(\u0026#34;Compute Shader\u0026#34;, computeShader, typeof(ComputeShader), false); texture = (Texture2D)EditorGUILayout.ObjectField(\u0026#34;Texture\u0026#34;, texture, typeof(Texture2D), false); savePath = EditorGUILayout.TextField(\u0026#34;Save Path\u0026#34;, savePath); using (new EditorGUI.DisabledGroupScope(!computeShader || !texture)) { if(GUILayout.Button(\u0026#34;Generate!\u0026#34;, new GUILayoutOption[] { GUILayout.Height(30.0f) })) { GenerateRCSM(); } } } } private static Vector4 GetTextureSize(Vector2Int textureSize) { return new Vector4(textureSize.x, textureSize.y, 1.0f / textureSize.x, 1.0f / textureSize.y); } private void PreProcess(RenderTexture target) { int kernelID = computeShader.FindKernel(\u0026#34;PreProcessMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); computeShader.SetVector(\u0026#34;_TextureSize\u0026#34;, GetTextureSize(textureSize)); computeShader.SetTexture(kernelID, \u0026#34;_SourceTex\u0026#34;, texture); computeShader.SetTexture(kernelID, \u0026#34;_RW_TargetTex\u0026#34;, target); computeShader.Dispatch(kernelID, Mathf.CeilToInt(textureSize.x / x), Mathf.CeilToInt(textureSize.y / y), 1); } private void ComputeRCSM(Vector2Int offset, RenderTexture source, RenderTexture target) { int kernelID = computeShader.FindKernel(\u0026#34;RCSMMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); computeShader.SetVector(\u0026#34;_TextureSize\u0026#34;, GetTextureSize(textureSize)); computeShader.SetVector(\u0026#34;_CacheOffset\u0026#34;, new Vector2(offset.x, offset.y)); computeShader.SetTexture(kernelID, \u0026#34;_SourceTex\u0026#34;, source); computeShader.SetTexture(kernelID, \u0026#34;_RW_TargetTex\u0026#34;, target); computeShader.Dispatch(kernelID, Mathf.CeilToInt(textureSize.x / x), Mathf.CeilToInt(textureSize.y / y), 1); } private IEnumerator DispatchCompute() { PreProcess(rts[0]); yield return null; int kernelID = computeShader.FindKernel(\u0026#34;RCSMMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); Vector2Int dispatchCount = new Vector2Int(Mathf.CeilToInt(textureSize.x / x), Mathf.CeilToInt(textureSize.y / y)); int fromID = 0; bool cancel = false; for (int i = 0; i \u0026lt; dispatchCount.x; i++) { for (int j = 0; j \u0026lt; dispatchCount.y; j++) { ComputeRCSM(new Vector2Int(i, j), rts[fromID], rts[1 - fromID]); fromID = 1 - fromID; yield return null; } cancel = EditorUtility.DisplayCancelableProgressBar(\u0026#34;In Progress...\u0026#34;, i + \u0026#34;/\u0026#34; + dispatchCount.x, (float)i / dispatchCount.x); if (cancel) break; } EditorUtility.ClearProgressBar(); if (!cancel) SaveRenderTextureToFile(rts[fromID]); } private void GenerateRCSM() { textureSize = new Vector2Int(texture.width, texture.height); EnsureRTs(); EnsureRT(ref rts[0], textureSize.x, textureSize.y); EnsureRT(ref rts[1], textureSize.x, textureSize.y); Stop(); editorCoroutine = EditorCoroutineUtility.StartCoroutine(DispatchCompute(), this); } private void SaveRenderTextureToFile(RenderTexture rt) { RenderTexture prev = RenderTexture.active; RenderTexture.active = rt; Texture2D toSave = new Texture2D(textureSize.x, textureSize.y, TextureFormat.ARGB32, false, true); toSave.ReadPixels(new Rect(0.0f, 0.0f, textureSize.x, textureSize.y), 0, 0); byte[] bytes = toSave.EncodeToTGA(); FileStream fs = File.OpenWrite(savePath + suffix); fs.Write(bytes); fs.Close(); AssetDatabase.Refresh(); TextureImporter ti = (TextureImporter)AssetImporter.GetAtPath(savePath + suffix); ti.mipmapEnabled = false; ti.sRGBTexture = false; ti.SaveAndReimport(); Texture2D tempTexture = AssetDatabase.LoadAssetAtPath\u0026lt;Texture2D\u0026gt;(savePath + suffix); EditorGUIUtility.PingObject(tempTexture); RenderTexture.active = prev; } private void Stop() { if (editorCoroutine != null) EditorCoroutineUtility.StopCoroutine(editorCoroutine); } private void OnDestroy() { foreach (var rt in rts) { if (rt != null) rt.Release(); } rts = new RenderTexture[2]; } } RCSMVisualizeShader.shader 计算Parallax的地方分成两个循环，第一个循环通过coneRatio和深度值进行光线步进直到采样点在高度图内部，第二个循环通过二分法获得较为准确的uv。\nShader \u0026#34;zznewclear13/RCSMVisualizeShader\u0026#34; { Properties { _BaseColor(\u0026#34;Base Color\u0026#34;, Color) = (1, 1, 1, 1) _MainTex (\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _RCSMTex(\u0026#34;RCSM Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _NormalMap(\u0026#34;Normal Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _NormalIntensity(\u0026#34;Normal Intensity\u0026#34;, Range(0, 2)) = 1 _ParallaxIntensity(\u0026#34;Parallax Intensity\u0026#34;, Float) = 1 _ParallaxIteration(\u0026#34;Parallax Iteration\u0026#34;, Float) = 15 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; sampler2D _MainTex; sampler2D _NormalMap; sampler2D _RCSMTex; CBUFFER_START(UnityPerMaterial) float4 _BaseColor; float _NormalIntensity; float _ParallaxIntensity; float _ParallaxIteration; CBUFFER_END struct Attributes { float4 positionOS : POSITION; float3 normalOS : NORMAL; float4 tangentOS : TANGENT; float2 texcoord : TEXCOORD0; }; struct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; float4 tbnView[3] : TEXCOORD1; }; Varyings vert(Attributes input) { Varyings output = (Varyings)0; VertexPositionInputs vpi = GetVertexPositionInputs(input.positionOS.xyz); VertexNormalInputs vni = GetVertexNormalInputs(input.normalOS, input.tangentOS); float3 cameraOS = mul(UNITY_MATRIX_I_M, float4(GetCameraPositionWS(), 1.0f)).xyz; float sign = (input.tangentOS.w \u0026gt; 0.0 ? 1.0 : -1.0) * GetOddNegativeScale(); float3 bitangent = cross(input.normalOS, input.tangentOS.xyz) * sign; float3x3 tbnMat = float3x3(input.tangentOS.xyz, bitangent, input.normalOS); float3 viewTS = mul(tbnMat, cameraOS - input.positionOS.xyz); output.positionCS = vpi.positionCS; output.uv = input.texcoord; output.tbnView[0] = float4(vni.tangentWS, viewTS.x); output.tbnView[1] = float4(vni.bitangentWS * sign, viewTS.y); output.tbnView[2] = float4(vni.normalWS, viewTS.z); return output; } float2 sampleRCSM(float2 uv) { float2 rcsm = tex2D(_RCSMTex, uv).xy; return float2(1.0f - rcsm.x, rcsm.y); } float getStepLength(float rayRatio, float coneRatio, float rayHeight, float sampleHeight) { float totalRatio = rayRatio / coneRatio + 1.0f; return (sampleHeight - rayHeight) / totalRatio; } float2 parallax(float2 uv, float3 view) { view.xy = -view.xy * _ParallaxIntensity; float3 samplePos = float3(uv, 0.0f); float2 rcsm = sampleRCSM(samplePos.xy); float rayRatio = length(view.xy); float coneRatio = rcsm.y; float rayHeight = samplePos.z; float sampleHeight = rcsm.x; float stepLength = getStepLength(rayRatio, coneRatio, rayHeight, sampleHeight); [unroll(30)] for (int i = 0; i \u0026lt; _ParallaxIteration; ++i) { samplePos += stepLength * view; rcsm = sampleRCSM(samplePos.xy); coneRatio = rcsm.y; rayHeight = samplePos.z; sampleHeight = rcsm.x; if (sampleHeight \u0026lt;= rayHeight) break; stepLength = getStepLength(rayRatio, coneRatio, rayHeight, sampleHeight); } stepLength *= 0.5f; samplePos -= stepLength * view; [unroll] for (int j = 0; j \u0026lt; 5; ++j) { rcsm = sampleRCSM(samplePos.xy); stepLength *= 0.5f; if (samplePos.z \u0026gt;= rcsm.x) { samplePos -= stepLength * view; } else if(samplePos.z \u0026lt; rcsm.x) { samplePos += stepLength * view; } } return samplePos.xy; } float4 frag(Varyings input) : SV_TARGET { float3 viewTS = normalize(float3(input.tbnView[0].w, input.tbnView[1].w, input.tbnView[2].w)); float3 tangentWS = normalize(input.tbnView[0].xyz); float3 bitangentWS = normalize(input.tbnView[1].xyz); float3 normalWS = normalize(input.tbnView[2].xyz); float z = max(abs(viewTS.z), 1e-5) * (viewTS.z \u0026gt;= 0.0f ? 1.0f : -1.0f); float2 uv = parallax(input.uv, viewTS / z); float4 mainTex = tex2D(_MainTex, uv) * _BaseColor; float3 normalTS = normalize(UnpackNormalScale(tex2D(_NormalMap, uv), _NormalIntensity)); float3 n = normalize(normalTS.x * tangentWS + normalTS.y * bitangentWS + normalTS.z * normalWS); Light mainLight = GetMainLight(); float ndotl = max(0.0f, dot(n, mainLight.direction)); float3 color = mainTex.rgb * mainLight.color * ndotl; float alpha = mainTex.a; return float4(color, alpha); } ENDHLSL SubShader { Tags{ \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34;} Blend SrcAlpha OneMinusSrcAlpha ZWrite Off Cull Back Pass { HLSLPROGRAM #pragma vertex vert #pragma fragment frag ENDHLSL } } } POMShader.shader 很大程度地参考了normal vs parallax的计算方式。\nShader \u0026#34;zznewclear13/POMShader\u0026#34; { Properties { _BaseColor(\u0026#34;Base Color\u0026#34;, Color) = (1, 1, 1, 1) _MainTex (\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _HeightMap(\u0026#34;Height Map\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _NormalMap(\u0026#34;Normal Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _NormalIntensity(\u0026#34;Normal Intensity\u0026#34;, Range(0, 2)) = 1 _ParallaxIntensity (\u0026#34;Parallax Intensity\u0026#34;, Float) = 1 _ParallaxIteration (\u0026#34;Parallax Iteration\u0026#34;, Float) = 15 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; sampler2D _MainTex; sampler2D _HeightMap; sampler2D _NormalMap; CBUFFER_START(UnityPerMaterial) float4 _BaseColor; float _NormalIntensity; float _ParallaxIntensity; float _ParallaxIteration; CBUFFER_END struct Attributes { float4 positionOS : POSITION; float3 normalOS : NORMAL; float4 tangentOS : TANGENT; float2 texcoord : TEXCOORD0; }; struct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; float4 tbnView[3] : TEXCOORD1; }; Varyings vert(Attributes input) { Varyings output = (Varyings)0; VertexPositionInputs vpi = GetVertexPositionInputs(input.positionOS.xyz); VertexNormalInputs vni = GetVertexNormalInputs(input.normalOS, input.tangentOS); float3 cameraOS = mul(UNITY_MATRIX_I_M, float4(GetCameraPositionWS(), 1.0f)).xyz; float sign = (input.tangentOS.w \u0026gt; 0.0 ? 1.0 : -1.0) * GetOddNegativeScale(); float3 bitangent = cross(input.normalOS, input.tangentOS.xyz) * sign; float3x3 tbnMat = float3x3(input.tangentOS.xyz, bitangent, input.normalOS); float3 viewTS = mul(tbnMat, cameraOS - input.positionOS.xyz); output.positionCS = vpi.positionCS; output.uv = input.texcoord; output.tbnView[0] = float4(vni.tangentWS, viewTS.x); output.tbnView[1] = float4(vni.bitangentWS * sign, viewTS.y); output.tbnView[2] = float4(vni.normalWS, viewTS.z); return output; } float sampleHeight(float2 uv) { return 1.0f - tex2D(_HeightMap, uv).r; } float2 parallax(float2 uv, float3 view) { float numLayers = _ParallaxIteration; float layerDepth = 1.0f / numLayers; float2 p = view.xy * _ParallaxIntensity; float2 deltaUVs = p / numLayers; float texd = sampleHeight(uv); float d = 0.0f; [unroll(30)] for (; d \u0026lt; texd; d += layerDepth) { uv -= deltaUVs; texd = sampleHeight(uv); } float2 lastUVs = uv + deltaUVs; float after = texd - d; float before = sampleHeight(lastUVs) - d + layerDepth; float w = after / (after - before); return lerp(uv, lastUVs, w); } float4 frag(Varyings input) : SV_TARGET { float3 viewTS = normalize(float3(input.tbnView[0].w, input.tbnView[1].w, input.tbnView[2].w)); float3 tangentWS = normalize(input.tbnView[0].xyz); float3 bitangentWS = normalize(input.tbnView[1].xyz); float3 normalWS = normalize(input.tbnView[2].xyz); float z = max(abs(viewTS.z), 1e-5) * (viewTS.z \u0026gt;= 0.0f ? 1.0f : -1.0f); float2 uv = parallax(input.uv, viewTS / z); float4 mainTex = tex2D(_MainTex, uv) * _BaseColor; float3 normalTS = normalize(UnpackNormalScale(tex2D(_NormalMap, uv), _NormalIntensity)); float3 n = normalize(normalTS.x * tangentWS + normalTS.y * bitangentWS + normalTS.z * normalWS); Light mainLight = GetMainLight(); float ndotl = max(0.0f, dot(n, mainLight.direction)); float3 color = mainTex.rgb * mainLight.color * ndotl; float alpha = mainTex.a; return float4(color, alpha); } ENDHLSL SubShader { Tags{ \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34;} Blend SrcAlpha OneMinusSrcAlpha ZWrite Off Cull Back Pass { HLSLPROGRAM #pragma vertex vert #pragma fragment frag ENDHLSL } } } 后记 夏天真的好热，热到头脑都不是很清醒了，感觉RCSM应该要比POM好很多才对，在自己的测试中也只稍微好了一些些，当然也有可能是我哪里没算对了。。。不过蛮奇怪的GPU Gems 3发表于2007年，直到今天我也没看到别的Unity上实现RCSM的文章或者github仓库，UE5也只有Get Relief这么一个插件。是因为曲面细分实在太好用了的原因吗？想起之前看最后生还者2的技术分享，里面大量地使用了高度图，难道主要是用来做多种材质的混合而不是做视差效果吗？\n","permalink":"https://zznewclear13.github.io/posts/relaxed-cone-step-mapping-in-unity/","summary":"阅读前提示 由于本文使用的贴图均为LearnOpenGL网站上的贴图，其法线贴图和一般Unity或Unreal引擎中的法线贴图的Y分量是相反的，因此在计算世界坐标的bitangent的时候会额外再乘上一个sign，在正常情况下是不需要的。\n视差效果 在三角形面数比较受限的情况下，往往会考虑使用一张高度图，通过视差的计算去渲染出一种3D的效果（虽然现在直接用曲面细分Tessellation似乎是一种更普遍的且更有效的方法）。有两种计算视差的方法，一种叫做Parallax Occlusion Mapping，先假定高度的层数，然后对每一层计算出合适的位置和颜色，从而达到3D效果；另一种叫做Cone Step Mapping，是根据高度图预先计算出每个点对于其他所有像素的最大的圆锥张角（有点像AO），根据圆锥张角快速步进，最后使用二分法计算出最终的交点的颜色。第一种方法有一个比较大的缺点，就是在视角比较接近平面的时候，如果采样次数不是很高，就会看到一层一层的效果，可以通过对最后一次计算深度进行线性插值在一定程度上减轻一层一层的问题；第二种方法的缺点是，当采样次数较小时，产生的图像会有一定程度的扭曲，但不会有一层一层的感觉，此外相较于第一种会有一个优点，较细物体不会被跳过。在GPU Gems 3中提到了一种Cone Step Mapping的优化，叫做Relaxed Cone Step Mapping，相较于之前计算最大张角的方式，这种优化通过确保通过圆锥的射线与圆锥内部的高度图至多只有一个交点，减少了一开始圆锥步进的次数。本文就主要使用这种方法进行计算，也许将圆锥的顶部放在比当前高度图更深的位置能够更加减少步进的次数，不过我稍微尝试了一下好像效果并不是特别理想。\nParallax Occlusion Mapping可以在Learn OpenGL里找到介绍和优化方案，Shadertoy上也有开源的代码可以参考。UE5中有一个叫Get Relief!的插件，可以用来快速生成Relaxed Cone Step Mapping的预计算的贴图，也提供了渲染的Shader。这个插件的作者Daniel Elliott也在GDC2023上分享了制作的思路，如果链接打不开的话这里还有一个GDC Vault的链接。\n本文使用的贴图可以在Learn OpenGL中给出的下载链接中找到。为了看上去舒服一些，这里对displacement贴图的颜色进行了反向。\n下图是两种视差做法的比较，左边是Parallax Occlusion Mapping，右边是Relaxed Cone Step Mapping，两者的采样次数是相同的，可以看到POM在较极限的情况下会有分层感而RCSM会有扭曲。RCSM使用的贴图也放在下面了，R通道是高度图，G通道是圆锥的张角。本文使用的是Unity 2021.3.19f1c1。\n生成预计算的贴图 和Parallax Occlusion Mapping直接使用深度图不同的是，Cone Step Mapping需要预先计算出一张圆锥张角的图，圆锥的张角可以使用圆锥底的半径除以圆锥的高来表示，记为coneRatio。本文中使用的是高度图，但实际计算中会使用1减去高度值，对应的是从模型表面到实际高度的深度值。由于深度值只会在01之间，uv也只会在01之间，因此对于最深的点，其最大的圆锥张角不会大于1。\n“确保通过圆锥的射线与圆锥内部的高度图至多只有一个交点”，对于圆锥顶部的currentPos和圆锥底部的rayStartPos（这个圆锥是一个倒立的圆锥，其底部和模型表面相平），可以采样一个目标点cachedPos，当cachedPos的深度小于currentPos的深度时，沿着cachedPos - rayStartPos的方向移动cachedPos的位置并一直采样所有像素samplePos，直到samplePos的深度值小于cachedPos（即射线穿过高度图并穿出），根据samplePos和currentPos就能计算出一个圆锥的张角coneRatio。循环所有的像素就能得到最小的圆锥张角了。\n为了减少单次计算的消耗，本文会先将整张图片分成NxN大小的区域，在一次循环中会计算所有像素对于这NxN大小的区域的圆锥张角，循环所有的区域就能得到最后的圆锥张角了。同时只需要让N等于THREAD_GROUP_SIZE，就能使用group shared memory仅通过一次采样缓存这些区域的深度值。再有就是Early Exit的优化，当cachedPos在贴图外部，当cachedPos的深度大于currentPos的深度，当cachedPos的圆锥张角大于当前最小的圆锥张角，在这些情况下可以直接结束向外步进的循环。更多的优化方法也都能在Get Relief!的分享中找到。\n具体的代码 RCSMComputeShader.compute 用于生成Relaxed Cone Step Mapping的贴图。PreProcessMain用于处理最一开始的深度图，预先设置最大的coneRatio为1。Early Exit是减少运算时间的关键。\n#pragma kernel PreProcessMain #pragma kernel RCSMMain #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; Texture2D\u0026lt;float4\u0026gt; _SourceTex; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTex; SamplerState sampler_LinearClamp; float4 _TextureSize; float2 _CacheOffset; #define THREAD_GROUP_SIZE 16u [numthreads(8, 8, 1)] void PreProcessMain(uint3 id : SV_DispatchThreadID) { uint2 tempID = uint2(id.","title":"在Unity里实现松散圆锥步进Relaxed Cone Step Mapping"},{"content":"2023年6月3日修订 发现还是在降采样升采样后进行线性插值来获取中间程度的模糊效果比较好，所以后面的代码也都改过来了，不过理论上的部分倒是没必要改。顺便也花了点时间写了一个Shadertoy作为演示：\nDual Kawase Blur\n写这篇文章的原因 网上已经有了很多很多的双Kawase模糊的现成的案例，但是存在以下几个问题：1. 绝大部分的文章都只给了代码，没有相应的解释，至多会给一张直接从Arm的pdf截取的图示，而这张图示画了一堆方框和符号，却没有说明这些图案代表的含义。2. 绝大部分的文章通过修改采样的距离来控制模糊的程度，这个的缺点我们后续再谈。3. 绝大部分的文章并没有考虑模糊程度从0开始逐渐增大的动态过程，使用降采样和升采样往往会破坏整个画面的连续性。\n如果只是想要获得一个模糊的画面，只需要做几次降采样和升采样就能完成了，但我希望能有一个连续地逐渐地变模糊的过程，因此我开始了量化双Kawase模糊的想法。\n双Kawase模糊(Dual Kawase Blur) 双Kawase模糊是2015年Arm在Kawase模糊的基础上提出的一种通过降采样和升采样来快速且高效地进行高质量大半径模糊的一种方法，具体的pdf可以从这里找到。\n这里是一张双Kawase模糊的图示，表示了双Kawase模糊在降采样和升采样时的操作。细的黑线对应的格子是原始的像素（或是升采样后的像素），粗的黑线对应的格子是降采样后的像素。叉对应的是当前模糊的像素，圆对应的是当前模糊的像素所需要采样点。粉色对应的是降采样时的模糊的像素和采样点，绿色对应的是升采样时的模糊的像素和采样点。\n从这张图中也可以看到双Kawase模糊利用双线性采样来节省采样数的操作。在降采样时实际采样了当前像素周围一共十六个像素的颜色；在升采样时实际采样了当前像素周围一共十三个像素的颜色。而如果在做降采样时，对于奇数个像素除以二向下取整，或者是在降采样时使用了不恰当的偏移（比如1.5倍的偏移），会导致降采样的采样点落在原始像素的中心，这时即使使用了双线性采样，也只等价于采样一个像素。\n因此为了让每一个像素都能对模糊做到应用的贡献，为了达到比较好的模糊效果，我们这里限制双Kawase模糊的采样偏移为一倍（也就是严格按照采样点进行最优的双线性采样）。而通过多次降采样和升采样达到合适的模糊半径。\n量化双Kawase模糊 降采样和升采样有一个缺点，就是只要发生了降采样和升采样，就必然会带来模糊。这时有两种方法，一种是在原始分辨率下通过消耗更大的方式进行加权模糊来逼近双Kawase模糊配合降采样带来的模糊；另一种是在零次和一次双Kawase之间线性插值得到一张介于两者之间模糊程度的图像。综合两者来看，线性插值得到的效果更为平滑，效果上稍“错误”一些，但完全在可接受的范围内。\n我这边写了一个小小的脚本，去计算原始分辨率下值为1的像素点，在经过一次双Kawase模糊后，其他像素的值。通过多项式拟合这些模糊后的值，就能利用这些值来逼近双Kawase模糊的效果了。我这边对8x8的像素做了计算（实际上模糊的核心应该更大一些，不过我懒得改之前的代码了）。计算出的权重如下：\n0.0003255208\t0.001464844\t0.003092448\t0.004231771\t0.004231771\t0.003092448\t0.001464844\t0.0003255208\t0.001464844\t0.004882813\t0.009440104\t0.01204427\t0.01074219\t0.007486979\t0.004231771\t0.001464844\t0.004394531\t0.01334635\t0.02311198\t0.02701823\t0.0218099\t0.01334635\t0.007486979\t0.003092448\t0.01009115\t0.02571615\t0.03808594\t0.04329427\t0.03678386\t0.0218099\t0.01074219\t0.004231771\t0.01529948\t0.03222656\t0.04069011\t0.04589844\t0.04329427\t0.02701823\t0.01204427\t0.004231771\t0.01416016\t0.0296224\t0.03678386\t0.04069011\t0.03808594\t0.02311198\t0.009440104\t0.003092448\t0.007324219\t0.01985677\t0.0296224\t0.03222656\t0.02571615\t0.01334635\t0.004882813\t0.001464844\t0.001627604\t0.007324219\t0.01416016\t0.01529948\t0.01009115\t0.004394531\t0.001464844\t0.0003255208\t然后我把它丢进了Excel强行进行了一波运算并手动调整了一下，得出了下面这个拟合的公式（x范围大致在[-4, 4]之间，值并未归一化）：\n// Approximate dual kawase blur with 4th degree polynomial. // -3.5 \u0026lt;= x \u0026lt;= 3.5 (idealy) float getWeight(float x) { return 0.1356f * x * x * x * x - 0.06748 * x * x * x - 4.693656 * x * x + 0.9954208 * x + 45.57338; } 具体的操作 接下来我们就能根据拟合的公式来进行几乎连续的双Kawase模糊了。通过计算以二为底的对数，可以知道我们需要进行多少次降采样，而其小数部分则代表着进行逼近下一次双Kawase模糊的程度，对应模糊的offset值。\nDualKawaseBlurComputeShader.compute 这里就懒得对双Kawase模糊那一部分写group shared memory的优化了。THREAD_GROUP_SIZE需要是BLUR_RADIUS的四倍以上，不然缓存的时候每个像素需要采更多的样。BLUR_RADIUS实际上是4但这边写了5，是因为需要额外预留一个像素进行手动的双线性采样。\n#pragma kernel KawaseDownSample #pragma kernel KawaseUpSample #pragma kernel KawaseLinear Texture2D\u0026lt;float4\u0026gt; _SourceTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTexture; SamplerState sampler_LinearClamp; float4 _SourceSize; float4 _TargetSize; float _Offset; float3 sampleSource(float2 center, float2 offset) { return _SourceTexture.SampleLevel(sampler_LinearClamp, center + offset * _Offset, 0.0f).rgb; } [numthreads(8,8,1)] void KawaseDownSample(uint3 id : SV_DispatchThreadID) { float2 uv = (float2(id.xy) + 0.5f) * _TargetSize.zw; float2 halfPixel = 0.5f * _TargetSize.zw; float3 c = sampleSource(uv, float2(0.0f, 0.0f)); float3 tl = sampleSource(uv, halfPixel * float2(-1.0f, +1.0f)); float3 tr = sampleSource(uv, halfPixel * float2(+1.0f, +1.0f)); float3 bl = sampleSource(uv, halfPixel * float2(-1.0f, -1.0f)); float3 br = sampleSource(uv, halfPixel * float2(+1.0f, -1.0f)); float3 color = (tl + tr + bl + br + c * 4.0f) / 8.0f; _RW_TargetTexture[id.xy] = float4(color, 1.0f); } [numthreads(8, 8, 1)] void KawaseUpSample(uint3 id : SV_DispatchThreadID) { float2 uv = (float2(id.xy) + 0.5f) * _TargetSize.zw; float2 onePixel = 1.0f * _TargetSize.zw; // float3 c = sampleSource(uv, float2(0.0f, 0.0f)); float3 t2 = sampleSource(uv, onePixel * float2(+0.0f, +2.0f)); float3 b2 = sampleSource(uv, onePixel * float2(+0.0f, -2.0f)); float3 l2 = sampleSource(uv, onePixel * float2(-2.0f, +0.0f)); float3 r2 = sampleSource(uv, onePixel * float2(+2.0f, +0.0f)); float3 tl = sampleSource(uv, onePixel * float2(-1.0f, +1.0f)); float3 tr = sampleSource(uv, onePixel * float2(+1.0f, +1.0f)); float3 bl = sampleSource(uv, onePixel * float2(-1.0f, -1.0f)); float3 br = sampleSource(uv, onePixel * float2(+1.0f, -1.0f)); float3 color = (t2 + b2 + l2 + r2 + 2.0f * (tl + tr + bl + br)) / 12.0f; _RW_TargetTexture[id.xy] = float4(color, 1.0f); } [numthreads(8, 8, 1)] void KawaseLinear(uint3 id : SV_DispatchThreadID) { half3 sourceTex = _SourceTexture.Load(uint3(id.xy, 0)).rgb; half3 blurredTex = _RW_TargetTexture.Load(uint3(id.xy, 0)).rgb; half3 color = lerp(sourceTex, blurredTex, _Offset); _RW_TargetTexture[id.xy] = float4(color, 1.0f); } DualKawaseBlurRenderPass.cs 这里需要对最后一次降采样做一次拟合的操作，然后还要注意一次都不进行降采样时也需要进行拟合。使用的Unity版本是2021.3.19f1c1，URP版本是12.1.10，因此会有_CameraColorAttachmentA这样奇怪的名字。\nusing System.Collections.Generic; namespace UnityEngine.Rendering.Universal { public class DualKawaseBlurRenderPass : ScriptableRenderPass { static readonly string passName = \u0026#34;Circular Blur Render Pass\u0026#34;; private DualKawaseBlurRendererFeature.DualKawaseBlurSettings settings; private DualKawaseBlur dualKawaseBlur; private ComputeShader computeShader; static readonly string cameraColorTextureName = \u0026#34;_CameraColorAttachmentA\u0026#34;; static readonly int cameraColorTextureID = Shader.PropertyToID(cameraColorTextureName); private RenderTargetIdentifier cameraColorIden; private Vector2Int textureSize; private RenderTextureDescriptor desc; public DualKawaseBlurRenderPass(DualKawaseBlurRendererFeature.DualKawaseBlurSettings settings) { profilingSampler = new ProfilingSampler(passName); this.settings = settings; renderPassEvent = settings.renderPassEvent; computeShader = settings.computeShader; cameraColorIden = new RenderTargetIdentifier(cameraColorTextureID); } public void Setup(DualKawaseBlur dualKawaseBlur) { this.dualKawaseBlur = dualKawaseBlur; } public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor) { textureSize = new Vector2Int(cameraTextureDescriptor.width, cameraTextureDescriptor.height); desc = cameraTextureDescriptor; desc.enableRandomWrite = true; desc.msaaSamples = 1; desc.depthBufferBits = 0; } private Vector4 GetTextureSizeParams(Vector2Int size) { return new Vector4(size.x, size.y, 1.0f / size.x, 1.0f / size.y); } private void DoKawaseSample(CommandBuffer cmd, RenderTargetIdentifier sourceid, RenderTargetIdentifier targetid, Vector2Int sourceSize, Vector2Int targetSize, float offset, bool downSample, ComputeShader computeShader) { if (!computeShader) return; string kernelName = downSample ? \u0026#34;KawaseDownSample\u0026#34; : \u0026#34;KawaseUpSample\u0026#34;; int kernelID = computeShader.FindKernel(kernelName); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_SourceTexture\u0026#34;, sourceid); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_RW_TargetTexture\u0026#34;, targetid); cmd.SetComputeVectorParam(computeShader, \u0026#34;_SourceSize\u0026#34;, GetTextureSizeParams(sourceSize)); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TargetSize\u0026#34;, GetTextureSizeParams(targetSize)); cmd.SetComputeFloatParam(computeShader, \u0026#34;_Offset\u0026#34;, offset); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)targetSize.x / x), Mathf.CeilToInt((float)targetSize.y / y), 1); } private void DoKawaseLinear(CommandBuffer cmd, RenderTargetIdentifier sourceid, RenderTargetIdentifier targetid, Vector2Int sourceSize, float offset, ComputeShader computeShader) { if (!computeShader) return; string kernelName = \u0026#34;KawaseLinear\u0026#34;; int kernelID = computeShader.FindKernel(kernelName); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_SourceTexture\u0026#34;, sourceid); cmd.SetComputeTextureParam(computeShader, kernelID, \u0026#34;_RW_TargetTexture\u0026#34;, targetid); cmd.SetComputeVectorParam(computeShader, \u0026#34;_SourceSize\u0026#34;, GetTextureSizeParams(sourceSize)); cmd.SetComputeFloatParam(computeShader, \u0026#34;_Offset\u0026#34;, offset); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)sourceSize.x / x), Mathf.CeilToInt((float)sourceSize.y / y), 1); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(); using (new ProfilingScope(cmd, profilingSampler)) { List\u0026lt;int\u0026gt; rtIDs = new List\u0026lt;int\u0026gt;(); List\u0026lt;Vector2Int\u0026gt; rtSizes = new List\u0026lt;Vector2Int\u0026gt;(); RenderTextureDescriptor tempDesc = desc; string kawaseRT = \u0026#34;_KawaseRT\u0026#34;; int kawaseRTID = Shader.PropertyToID(kawaseRT); cmd.GetTemporaryRT(kawaseRTID, tempDesc); rtIDs.Add(kawaseRTID); rtSizes.Add(textureSize); float downSampleAmount = Mathf.Log(dualKawaseBlur.GetRadius() + 1.0f) / 0.693147181f; int downSampleCount = Mathf.FloorToInt(downSampleAmount); float offsetRatio = downSampleAmount - (float)downSampleCount; Vector2Int lastSize = textureSize; int lastID = cameraColorTextureID; for (int i = 0; i \u0026lt;= downSampleCount; i++) { string rtName = \u0026#34;_KawaseRT\u0026#34; + i.ToString(); int rtID = Shader.PropertyToID(rtName); Vector2Int rtSize = new Vector2Int((lastSize.x + 1) / 2, (lastSize.y + 1) / 2); tempDesc.width = rtSize.x; tempDesc.height = rtSize.y; cmd.GetTemporaryRT(rtID, tempDesc); rtIDs.Add(rtID); rtSizes.Add(rtSize); DoKawaseSample(cmd, lastID, rtID, lastSize, rtSize, 1.0f, true, computeShader); lastSize = rtSize; lastID = rtID; } if(downSampleCount == 0) { DoKawaseSample(cmd, rtIDs[1], rtIDs[0], rtSizes[1], rtSizes[0], 1.0f, false, computeShader); DoKawaseLinear(cmd, cameraColorIden, rtIDs[0], rtSizes[0], offsetRatio, computeShader); } else { string intermediateRTName = \u0026#34;_KawaseRT\u0026#34; + (downSampleCount + 1).ToString(); int intermediateRTID = Shader.PropertyToID(intermediateRTName); Vector2Int intermediateRTSize = rtSizes[downSampleCount]; tempDesc.width = intermediateRTSize.x; tempDesc.height = intermediateRTSize.y; cmd.GetTemporaryRT(intermediateRTID, tempDesc); for (int i = downSampleCount+1; i \u0026gt;= 1; i--) { int sourceID = rtIDs[i]; Vector2Int sourceSize = rtSizes[i]; int targetID = i == (downSampleCount + 1) ? intermediateRTID : rtIDs[i - 1]; Vector2Int targetSize = rtSizes[i - 1]; DoKawaseSample(cmd, sourceID, targetID, sourceSize, targetSize, 1.0f, false, computeShader); if (i == (downSampleCount + 1)) { DoKawaseLinear(cmd, rtIDs[i - 1], intermediateRTID, targetSize, offsetRatio, computeShader); int tempID = intermediateRTID; intermediateRTID = rtIDs[i - 1]; rtIDs[i - 1] = tempID; } cmd.ReleaseTemporaryRT(sourceID); } cmd.ReleaseTemporaryRT(intermediateRTID); } cmd.Blit(kawaseRTID, cameraColorIden); cmd.ReleaseTemporaryRT(kawaseRTID); } context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } } } DualKawaseBlur.cs 没啥好说的\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenuForRenderPipeline(\u0026#34;Post-processing/Dual Kawase Blur\u0026#34;, typeof(UniversalRenderPipeline))] public class DualKawaseBlur : VolumeComponent, IPostProcessComponent { public BoolParameter isEnabled = new BoolParameter(false); public ClampedFloatParameter maxRadius = new ClampedFloatParameter(32.0f, 0.0f, 255.0f); public ClampedFloatParameter intensity = new ClampedFloatParameter(0.0f, 0.0f, 1.0f); public float GetRadius() { return maxRadius.value * intensity.value; } public bool IsActive() { return isEnabled.value \u0026amp;\u0026amp; intensity.value \u0026gt; 0.0f; } public bool IsTileCompatible() { return false; } } } DualKawaseBlurRendererFeature.cs 也没啥好说的。\nusing System.Collections; namespace UnityEngine.Rendering.Universal { public class DualKawaseBlurRendererFeature : ScriptableRendererFeature { [System.Serializable] public class DualKawaseBlurSettings { public RenderPassEvent renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; public ComputeShader computeShader; } public DualKawaseBlurSettings settings = new DualKawaseBlurSettings(); private DualKawaseBlurRenderPass dualKawaseBlurRenderPass; public override void Create() { dualKawaseBlurRenderPass = new DualKawaseBlurRenderPass(settings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { DualKawaseBlur dualKawaseBlur = VolumeManager.instance.stack.GetComponent\u0026lt;DualKawaseBlur\u0026gt;(); if (dualKawaseBlur != null \u0026amp;\u0026amp; dualKawaseBlur.IsActive()) { dualKawaseBlurRenderPass.Setup(dualKawaseBlur); renderer.EnqueuePass(dualKawaseBlurRenderPass); } } } } 后记 迅速地写完了这篇博客，其实有价值的东西并不是太多。主要是在网上搜索了半天，只有几乎千篇一律的代码和千篇一律的示意图，并没有很详细的解释，因此自己做了一些研究。不得不说Dual Kawase Blur做得还是很聪明的，最后模糊的效果也看不出什么明显的方形的痕迹。\n","permalink":"https://zznewclear13.github.io/posts/almost-continuous-dual-kawase-blur/","summary":"2023年6月3日修订 发现还是在降采样升采样后进行线性插值来获取中间程度的模糊效果比较好，所以后面的代码也都改过来了，不过理论上的部分倒是没必要改。顺便也花了点时间写了一个Shadertoy作为演示：\nDual Kawase Blur\n写这篇文章的原因 网上已经有了很多很多的双Kawase模糊的现成的案例，但是存在以下几个问题：1. 绝大部分的文章都只给了代码，没有相应的解释，至多会给一张直接从Arm的pdf截取的图示，而这张图示画了一堆方框和符号，却没有说明这些图案代表的含义。2. 绝大部分的文章通过修改采样的距离来控制模糊的程度，这个的缺点我们后续再谈。3. 绝大部分的文章并没有考虑模糊程度从0开始逐渐增大的动态过程，使用降采样和升采样往往会破坏整个画面的连续性。\n如果只是想要获得一个模糊的画面，只需要做几次降采样和升采样就能完成了，但我希望能有一个连续地逐渐地变模糊的过程，因此我开始了量化双Kawase模糊的想法。\n双Kawase模糊(Dual Kawase Blur) 双Kawase模糊是2015年Arm在Kawase模糊的基础上提出的一种通过降采样和升采样来快速且高效地进行高质量大半径模糊的一种方法，具体的pdf可以从这里找到。\n这里是一张双Kawase模糊的图示，表示了双Kawase模糊在降采样和升采样时的操作。细的黑线对应的格子是原始的像素（或是升采样后的像素），粗的黑线对应的格子是降采样后的像素。叉对应的是当前模糊的像素，圆对应的是当前模糊的像素所需要采样点。粉色对应的是降采样时的模糊的像素和采样点，绿色对应的是升采样时的模糊的像素和采样点。\n从这张图中也可以看到双Kawase模糊利用双线性采样来节省采样数的操作。在降采样时实际采样了当前像素周围一共十六个像素的颜色；在升采样时实际采样了当前像素周围一共十三个像素的颜色。而如果在做降采样时，对于奇数个像素除以二向下取整，或者是在降采样时使用了不恰当的偏移（比如1.5倍的偏移），会导致降采样的采样点落在原始像素的中心，这时即使使用了双线性采样，也只等价于采样一个像素。\n因此为了让每一个像素都能对模糊做到应用的贡献，为了达到比较好的模糊效果，我们这里限制双Kawase模糊的采样偏移为一倍（也就是严格按照采样点进行最优的双线性采样）。而通过多次降采样和升采样达到合适的模糊半径。\n量化双Kawase模糊 降采样和升采样有一个缺点，就是只要发生了降采样和升采样，就必然会带来模糊。这时有两种方法，一种是在原始分辨率下通过消耗更大的方式进行加权模糊来逼近双Kawase模糊配合降采样带来的模糊；另一种是在零次和一次双Kawase之间线性插值得到一张介于两者之间模糊程度的图像。综合两者来看，线性插值得到的效果更为平滑，效果上稍“错误”一些，但完全在可接受的范围内。\n我这边写了一个小小的脚本，去计算原始分辨率下值为1的像素点，在经过一次双Kawase模糊后，其他像素的值。通过多项式拟合这些模糊后的值，就能利用这些值来逼近双Kawase模糊的效果了。我这边对8x8的像素做了计算（实际上模糊的核心应该更大一些，不过我懒得改之前的代码了）。计算出的权重如下：\n0.0003255208\t0.001464844\t0.003092448\t0.004231771\t0.004231771\t0.003092448\t0.001464844\t0.0003255208\t0.001464844\t0.004882813\t0.009440104\t0.01204427\t0.01074219\t0.007486979\t0.004231771\t0.001464844\t0.004394531\t0.01334635\t0.02311198\t0.02701823\t0.0218099\t0.01334635\t0.007486979\t0.003092448\t0.01009115\t0.02571615\t0.03808594\t0.04329427\t0.03678386\t0.0218099\t0.01074219\t0.004231771\t0.01529948\t0.03222656\t0.04069011\t0.04589844\t0.04329427\t0.02701823\t0.01204427\t0.004231771\t0.01416016\t0.0296224\t0.03678386\t0.04069011\t0.03808594\t0.02311198\t0.009440104\t0.003092448\t0.007324219\t0.","title":"几乎连续的双Kawase模糊"},{"content":"圆形模糊 圆形模糊，在Photoshop里又称镜头模糊(Lens Blur)，和景深结合在一起的时候被称作散景(Bokeh)，是指在摄影时失焦的区域产生的和光圈的形状一致的模糊效果，五边形八边形或是圆形都有可能。\n在计算机图形学中实现景深效果基本上有两种方法：第一种也是最常用的，通过黄金率生成一系列的采样点，使得其形状接近想要模糊的形状，这种方法需要很多很多的采样点，基本上找到的都是60次以上的采样次数，由于采样点的分布不一定正好在像素点中心，也不能轻易地使用Group Shared Memory进行优化，事实上大的模糊半径很可能导致Group Shared Memory的大小不够；另一种是针对于特殊的模糊形状，比如正六边形，可以使用三次（MRT的话可以认为是两次）1D的模糊来组合而成，可以在Colin Barré-Brisebois的博客Hexagonal Bokeh Blur Revisited中看到详细的说明，值得一提的是他此前也在EA工作过（看来EA是真的很喜欢散景啊）。\nEA的渲染工程师Kleber Garcia在2018年的GDC演讲Circular Separable Convolution Depth of Field中提到了通过复数的运算来实现圆形模糊的算法，其背后的数学我这里就不再赘述了，感兴趣的话可以参考Circularly symmetric convolution and lens blu这篇文章。圆形模糊的参数的生成的代码可以在Kleber Garcia的公开仓库里找到。Kleber Garcia本人也在Shadertoy上写了具体的圆形模糊的代码。\nCircular Dof\n由于景深效果相对来说比较复杂，这里就只考虑对整个屏幕施加相同程度的圆形模糊效果。\n具体的实现方法 其实大部分和之前的高斯模糊没有什么差别。在分离卷积圆形模糊的算法中，圆形的效果是通过多个Filter叠加而成的，每个Filter对应实部和虚部两个参数。以本文为例，本文使用了两个Filter，对于一个Filter的一个颜色分量，需要储存实部虚部两个数据，总体就需要2x3x2=12个通道，使用三张R16G16B16A16_SFloat就能储存所有的数据。在Kleber Garcia的演讲中他还提到了，可以使用bracket的方法，将颜色的卷积数据储存到另一张图中，这样中间的Filter的结果就会落在[0, 1]的范围内，就能使用R8G8A8B8来储存了，可以节省一半的带宽（但是颜色的卷积数据不也要一张32位的图吗，这里我没太懂，感觉优化了但又没那么优化，索性就没那么做）。\n整体的操作是：1. 采样源图片，对每个Filter和每个颜色分量计算实部和虚部的值，水平累加后储存到中间贴图中；2. 采样中间贴图，对每个Filter和每个颜色分量计算实部和虚部的值，竖直累加后乘上各自的权重就得到最终的颜色了。由于我没有使用bracket的方法，Filter中会有负值存在，在仅使用两个Filter且半径较大且像素颜色过亮的时候，由于banding的存在会使最终的颜色出现负值，解决方法是在读取颜色的时候做一次Clamp或者是ToneMapping到合理范围。\nCircularBlurFilterGenerator.cs 改写自Kleber Garcia的公开仓库。\n/* Copyright 2023 zznewclear13 (zznewclear@gmail.com) Copyright 2016 Kleber A Garcia (kecho_garcia@hotmail.com) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u0026#34;Software\u0026#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u0026#34;AS IS\u0026#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\u001d */ using UnityEngine; public static class CircularBlurFilterGenerator { public struct Filter { public Vector2 kernelWeight; public Vector2[] circularKernels; } public struct KernelData { public Filter[] filters; } private static Vector4 KernelFunction(float x, Vector4 C) { float real = Mathf.Cos(x * x * C.x) * Mathf.Exp(x * x * C.y); float imaginary = Mathf.Sin(x * x * C.x) * Mathf.Exp(x * x * C.y); float realWeight = C.z; float imaginaryWeight = C.w; return new Vector4(real, imaginary, realWeight, imaginaryWeight); } // 1 \u0026lt;= components \u0026lt;= 5 // best: 0.2f \u0026lt;= transition \u0026lt;= 0.4f public static KernelData GenerateFilter(float radius, int components = 2, float transition = 0.2f, bool logKernel = false) { int sampleRadius = Mathf.CeilToInt(radius); int kernelSize = 2 * sampleRadius + 1; KernelData kernelData = new KernelData(); if (transition \u0026lt;= -1.0f) { Debug.Log(\u0026#34;Invalid transition bandwidth. Must be greater than -1 and preferably positive.\u0026#34;); return kernelData; } Vector4[] P; switch (components) { case 1: P = new Vector4[] { new Vector4( 1.624835f, -0.862325f, 0.767583f, 1.862321f ) }; break; case 2: P = new Vector4[] { new Vector4( 5.268909f, -0.886528f, 0.411259f, -0.548794f ), new Vector4(1.558213f, -1.960518f, 0.513282f, 4.561110f ) }; break; case 3: P = new Vector4[] { new Vector4( 5.043495f, -2.176490f, 1.621035f, -2.105439f ), new Vector4( 9.027613f, -1.019306f, -0.280860f, -0.162882f ), new Vector4( 1.597273f, -2.815110f, -0.366471f, 10.300301f ) }; break; case 4: P = new Vector4[] { new Vector4( 1.553635f, -4.338459f, -5.767909f, 46.164397f ), new Vector4( 4.693183f, -3.839993f, 9.795391f, 15.227561f ), new Vector4( 8.178137f, -2.791880f, -3.048324f, 0.302959f ), new Vector4( 12.328289f, -1.342190f, 0.010001f, 0.244650f ) }; break; case 5: P = new Vector4[] { new Vector4( 1.685979f, -4.892608f, -22.356787f, 85.912460f ), new Vector4( 4.998496f, -4.711870f, 35.918936f , -28.875618f ), new Vector4( 8.244168f, -4.052795f, -13.212253f, -1.578428f ), new Vector4( 11.900859f, -2.929212f, 0.507991f, 1.816328f ), new Vector4( 16.116382f, -1.512961f, 0.138051f, -0.010000f ) }; break; default: Debug.Log(\u0026#34;Invalid component count. Must be [1-5].\u0026#34;); return kernelData; } Vector4[,] kernels = new Vector4[components, kernelSize]; float totalBandWidth = 1.0f + transition; for (int i = 0; i \u0026lt; components; i++) { Vector4 C = P[i]; for (int j = -sampleRadius; j \u0026lt; sampleRadius + 1; j++) { kernels[i, j + sampleRadius] = KernelFunction(totalBandWidth * j / radius, C); } } // normalize kernels float accum = 0.0f; for (int i = 0; i \u0026lt; components; i++) { for (int j = 0; j \u0026lt; kernelSize; j++) { Vector4 v = kernels[i, j]; for (int k = 0; k \u0026lt; kernelSize; k++) { Vector4 w = kernels[i, k]; accum += v.z * (v.x * w.x - v.y * w.y) + v.w * (v.x * w.y + v.y * w.x); } } } float normConstant = 1.0f / Mathf.Sqrt(accum); Vector4[,] kernelsNormalized = new Vector4[components, kernelSize]; for (int i = 0; i \u0026lt; components; i++) { for (int j = 0; j \u0026lt; kernelSize; j++) { Vector4 v = kernels[i, j]; Vector4 vn = new Vector4(normConstant * v.x, normConstant * v.y, 0.0f, 0.0f); kernelsNormalized[i, j] = vn; } } // bracket the kernel so we maximize precision. This means figureout a Offset and a Scale Vector2[] scales = new Vector2[components]; Vector2[] offsets = new Vector2[components]; for (int i = 0; i \u0026lt; components; i++) { Vector2 minVector = new Vector2(kernelsNormalized[i, 0].x, kernelsNormalized[i, 0].y); for (int j = 1; j \u0026lt; kernelSize; j++) { minVector = new Vector2(Mathf.Min(minVector.x, kernelsNormalized[i, j].x), Mathf.Min(minVector.y, kernels[i, j].y)); } offsets[i] = minVector; } for (int i = 0; i \u0026lt; components; i++) { Vector2 offset = offsets[i]; Vector2 scale = new Vector2(0f, 0f); for (int j = 0; j \u0026lt; kernelSize; j++) { Vector4 v = kernelsNormalized[i, j]; float realScale = v.x - offset.x; float immScale = v.y - offset.y; scale += new Vector2(realScale, immScale); } scales[i] = scale; } Vector4[,] finalKernels = new Vector4[components, kernelSize]; for (int i = 0; i \u0026lt; components; i++) { Vector2 offset = offsets[i]; Vector2 scale = scales[i]; for (int j = 0; j \u0026lt; kernelSize; j++) { Vector4 v = kernelsNormalized[i, j]; float realScale = v.x - offset.x; float immScale = v.y - offset.y; finalKernels[i, j] = new Vector4(v.x, v.y, realScale / scale.x, immScale / scale.y); } } Vector2[] componentWeights = new Vector2[components]; for (int i = 0; i \u0026lt; components; i++) { Vector4 comp = P[i]; componentWeights[i] = new Vector2(comp.z, comp.w); } Filter[] filters = new Filter[components]; for (int i = 0; i \u0026lt; components; i++) { Vector2[] circularKernels = new Vector2[kernelSize]; for (int j = 0; j \u0026lt; kernelSize; j++) { Vector4 v = finalKernels[i, j]; circularKernels[j] = new Vector2(v.x, v.y); } Vector2 kernelWeight = componentWeights[i]; filters[i] = new Filter { kernelWeight = kernelWeight, circularKernels = circularKernels }; } kernelData.filters = filters; if(logKernel) { Debug.Log(Log(radius, finalKernels, componentWeights, offsets, scales)); } return kernelData; } private static readonly string[] syntax = new string[] { \u0026#34;uint\u0026#34;, \u0026#34;float\u0026#34;, \u0026#34;static const\u0026#34;, \u0026#34;{\u0026#34;, \u0026#34;};\u0026#34; }; private static string Log(float radius, Vector4[,] finalKernels, Vector2[] componentWeights, Vector2[] offsets, Vector2[] scales) { string logStr = \u0026#34;\u0026#34;; logStr += string.Format(\u0026#34;{0} {1} KERNEL_RADIUS = {2};\\n\u0026#34;, syntax[2], syntax[0], radius); int kernelSize = Mathf.CeilToInt(radius) * 2 + 1; logStr += string.Format(\u0026#34;{0} {1} KERNEL_COUNT = {2};\\n\u0026#34;, syntax[2], syntax[0], kernelSize); int component = componentWeights.Length; for (int i = 0; i \u0026lt; component; i++) { Vector2 o = offsets[i]; Vector2 s = scales[i]; Vector2 comp = componentWeights[i]; logStr += string.Format(\u0026#34;{0} {1}4 Kernel{2}BracketsRealXY_ImZW = {1}4({3},{4},{5},{6});\\n\u0026#34;, syntax[2], syntax[1], i, o.x, s.x, o.y, s.y); logStr += string.Format(\u0026#34;{0} {1}2 Kernel{2}Weights_RealX_ImY = {1}2({3},{4});\\n\u0026#34;, syntax[2], syntax[1], i, comp.x, comp.y); logStr += string.Format(\u0026#34;{0} {1}4 Kernel{2}_RealX_ImY_RealZ_ImW[] = {3}\\n\u0026#34;, syntax[2], syntax[1], i, syntax[3]); for (int j = 0; j \u0026lt; kernelSize; j++) { Vector4 val = finalKernels[i, j]; logStr += string.Format(\u0026#34;\\t{0}4(/*XY: Non Bracketed*/{1},{2},/*Bracketed WZ:*/{3},{4}){5}\\n\u0026#34;, syntax[1], val.x, val.y, val.z, val.w, (j \u0026lt; kernelSize - 1) ? \u0026#34;,\u0026#34; : \u0026#34;\u0026#34;); } logStr += string.Format(\u0026#34;{0}\\n\u0026#34;, syntax[4]); } return logStr; } } CircularBlurComputeShader.compute 我只使用了两个Filter，且将其权重和参数合并到了一个StructuredBuffer\u0026lt;float2\u0026gt;中，每一个Filter就需要根据一定的偏移量去获取到权重和参数的数据。同时又因为只使用了两个Filter，一个颜色分量会有两组实部和虚部，刚好构成一个四通道的数据，对应到一张贴图上。这个Compute Shader很大程度地参考了Kleber Garcia的Circular Dof。\n#pragma kernel CircularH #pragma kernel CircularV Texture2D\u0026lt;float4\u0026gt; _ColorTexture; Texture2D\u0026lt;float4\u0026gt; _RTexture; Texture2D\u0026lt;float4\u0026gt; _GTexture; Texture2D\u0026lt;float4\u0026gt; _BTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_RTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_GTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_BTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_CompositeTexture; float4 _TextureSize; int _SampleRadius; StructuredBuffer\u0026lt;float2\u0026gt; _CircularKernels; #define COLOR_THRESHOLD 5.0f #define CIRCULAR_BLUR_MAX_RADIUS 128 #define THREAD_GROUP_SIZE 256 const static int CACHED_COLOR_SIZE = THREAD_GROUP_SIZE + CIRCULAR_BLUR_MAX_RADIUS * 2; groupshared half3 cachedColor[CACHED_COLOR_SIZE]; void SetCachedColor(half3 color, int index) { cachedColor[index] = color; } half3 GetCachedColor(int threadPos) { return cachedColor[threadPos + CIRCULAR_BLUR_MAX_RADIUS]; } void CacheColor(int2 groupCacheStartPos, int cacheIndex) { int2 texturePos = groupCacheStartPos + int2(cacheIndex, 0); texturePos = clamp(texturePos, 0, _TextureSize.xy - 1.0f); half3 color = _ColorTexture.Load(uint3(texturePos, 0)).rgb; color = clamp(color, 0.0f, COLOR_THRESHOLD); SetCachedColor(color, cacheIndex); } struct RGBComp { half4 rComp; half4 gComp; half4 bComp; }; groupshared RGBComp cachedComp[CACHED_COLOR_SIZE]; void SetCachedComp(RGBComp comp, int index) { cachedComp[index] = comp; } RGBComp GetCachedComp(int threadPos) { return cachedComp[threadPos + CIRCULAR_BLUR_MAX_RADIUS]; } void CacheComp(int2 groupCacheStartPos, int cacheIndex) { int2 texturePos = groupCacheStartPos + int2(0, cacheIndex); texturePos = clamp(texturePos, 0, _TextureSize.xy - 1.0f); half4 rComp = _RTexture.Load(uint3(texturePos, 0)); half4 gComp = _GTexture.Load(uint3(texturePos, 0)); half4 bComp = _BTexture.Load(uint3(texturePos, 0)); RGBComp rgbComp = (RGBComp)0; rgbComp.rComp = rComp; rgbComp.gComp = gComp; rgbComp.bComp = bComp; SetCachedComp(rgbComp, cacheIndex); } //(Pr+Pi)*(Qr+Qi) = (Pr*Qr+Pr*Qi+Pi*Qr-Pi*Qi) float2 multComplex(float2 p, float2 q) { return float2(p.x * q.x - p.y * q.y, p.x * q.y + p.y * q.x); } [numthreads(THREAD_GROUP_SIZE,1,1)] void CircularH(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { int2 threadGroupSize = int2(THREAD_GROUP_SIZE, 1); int2 groupCacheStartPos = groupID.xy * threadGroupSize - int2(CIRCULAR_BLUR_MAX_RADIUS, 0); int cacheIndex = groupIndex * 2; if (cacheIndex \u0026lt; CACHED_COLOR_SIZE - 1) { CacheColor(groupCacheStartPos, cacheIndex); CacheColor(groupCacheStartPos, cacheIndex + 1); } GroupMemoryBarrierWithGroupSync(); int sampleRadius = _SampleRadius; int threadPos = groupIndex; half4 redSum = 0.0f; half4 greenSum = 0.0f; half4 blueSum = 0.0f; for (int i = -sampleRadius; i \u0026lt;=sampleRadius; ++i) { half3 color = GetCachedColor(threadPos + i); float2 kernel0 = _CircularKernels[1 + i + sampleRadius]; float2 kernel1 = _CircularKernels[1 + 2 * sampleRadius + 1 + 1 + i + sampleRadius]; redSum += color.r * float4(kernel0, kernel1); greenSum += color.g * float4(kernel0, kernel1); blueSum += color.b * float4(kernel0, kernel1); } _RW_RTexture[dispatchThreadID.xy] = redSum; _RW_GTexture[dispatchThreadID.xy] = greenSum; _RW_BTexture[dispatchThreadID.xy] = blueSum; } [numthreads(1,THREAD_GROUP_SIZE,1)] void CircularV(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { int2 threadGroupSize = int2(1, THREAD_GROUP_SIZE); int2 groupCacheStartPos = groupID.xy * threadGroupSize - int2(0, CIRCULAR_BLUR_MAX_RADIUS); int cacheIndex = groupIndex * 2; if (cacheIndex \u0026lt; CACHED_COLOR_SIZE - 1) { CacheComp(groupCacheStartPos, cacheIndex); CacheComp(groupCacheStartPos, cacheIndex + 1); } GroupMemoryBarrierWithGroupSync(); int sampleRadius = _SampleRadius; int threadPos = groupIndex; half4 redSum = 0.0f; half4 greenSum = 0.0f; half4 blueSum = 0.0f; for (int i = -sampleRadius; i \u0026lt;= sampleRadius; ++i) { RGBComp comp = GetCachedComp(threadPos + i); float2 kernel0 = _CircularKernels[1 + i + sampleRadius]; float2 kernel1 = _CircularKernels[1 + 2 * sampleRadius + 1 + 1 + i + sampleRadius]; redSum.xy += multComplex(comp.rComp.xy, kernel0); redSum.zw += multComplex(comp.rComp.zw, kernel1); greenSum.xy += multComplex(comp.gComp.xy, kernel0); greenSum.zw += multComplex(comp.gComp.zw, kernel1); blueSum.xy += multComplex(comp.bComp.xy, kernel0); blueSum.zw += multComplex(comp.bComp.zw, kernel1); } float2 weight0 = _CircularKernels[0]; float2 weight1 = _CircularKernels[1 + 2 * sampleRadius + 1]; half r = dot(redSum.xy, weight0) + dot(redSum.zw, weight1); half g = dot(greenSum.xy, weight0) + dot(greenSum.zw, weight1); half b = dot(blueSum.xy, weight0) + dot(blueSum.zw, weight1); _RW_CompositeTexture[dispatchThreadID.xy] = half4(r, g, b, 1.0f); } CircularBlur.cs 这里需要保证radius的最大值不大于Compute Shader里的CIRCULAR_BLUR_MAX_RADIUS。\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenuForRenderPipeline(\u0026#34;Post-processing/Circular Blur\u0026#34;, typeof(UniversalRenderPipeline))] public class CircularBlur : VolumeComponent, IPostProcessComponent { public BoolParameter isEnabled = new BoolParameter(false); public ClampedFloatParameter radius = new ClampedFloatParameter(0.0f, 0.0f, 128.0f); public bool IsActive() { return isEnabled.value \u0026amp;\u0026amp; radius.value \u0026gt; 0.0f; } public bool IsTileCompatible() { return false; } } } CircularBlurRenderPass.cs 和高斯模糊的Render Pass大同小异。\nusing System.Collections.Generic; namespace UnityEngine.Rendering.Universal { public class CircularBlurRenderPass : ScriptableRenderPass { static readonly string passName = \u0026#34;Circular Blur Render Pass\u0026#34;; private CircularBlurRendererFeature.CircularBlurSettings settings; private CircularBlur circularBlur; private ComputeShader computeShader; static readonly string cameraColorTextureName = \u0026#34;_CameraColorAttachmentA\u0026#34;; static readonly int cameraColorTextureID = Shader.PropertyToID(cameraColorTextureName); RenderTargetIdentifier cameraColorIden; static readonly string circularBlurTextureRName = \u0026#34;_CircularBlurTextureR\u0026#34;; static readonly int circularBlurTextureRID = Shader.PropertyToID(circularBlurTextureRName); RenderTargetIdentifier circularBlurTextureRIden; static readonly string circularBlurTextureGName = \u0026#34;_CircularBlurTextureG\u0026#34;; static readonly int circularBlurTextureGID = Shader.PropertyToID(circularBlurTextureGName); RenderTargetIdentifier circularBlurTextureGIden; static readonly string circularBlurTextureBName = \u0026#34;_CircularBlurTextureB\u0026#34;; static readonly int circularBlurTextureBID = Shader.PropertyToID(circularBlurTextureBName); RenderTargetIdentifier circularBlurTextureBIden; static readonly string compositeTextureName = \u0026#34;_CompositeTexture\u0026#34;; static readonly int compositeTextureID = Shader.PropertyToID(compositeTextureName); RenderTargetIdentifier compositeTextureIden; CircularBlurFilterGenerator.KernelData kernelData; private ComputeBuffer computeBuffer; private Vector2Int textureSize; private float lastRadius = 0.0f; static readonly string HorizontalKernelName = \u0026#34;CircularH\u0026#34;; static readonly string VerticalKernelName = \u0026#34;CircularV\u0026#34;; static readonly int _ColorTexture = Shader.PropertyToID(\u0026#34;_ColorTexture\u0026#34;); static readonly int _RTexture = Shader.PropertyToID(\u0026#34;_RTexture\u0026#34;); static readonly int _GTexture = Shader.PropertyToID(\u0026#34;_GTexture\u0026#34;); static readonly int _BTexture = Shader.PropertyToID(\u0026#34;_BTexture\u0026#34;); static readonly int _RW_RTexture = Shader.PropertyToID(\u0026#34;_RW_RTexture\u0026#34;); static readonly int _RW_GTexture = Shader.PropertyToID(\u0026#34;_RW_GTexture\u0026#34;); static readonly int _RW_BTexture = Shader.PropertyToID(\u0026#34;_RW_BTexture\u0026#34;); static readonly int _RW_CompositeTexture = Shader.PropertyToID(\u0026#34;_RW_CompositeTexture\u0026#34;); static readonly int _TextureSize = Shader.PropertyToID(\u0026#34;_TextureSize\u0026#34;); static readonly int _SampleRadius = Shader.PropertyToID(\u0026#34;_SampleRadius\u0026#34;); static readonly int _CircularKernels = Shader.PropertyToID(\u0026#34;_CircularKernels\u0026#34;); public CircularBlurRenderPass(CircularBlurRendererFeature.CircularBlurSettings settings) { profilingSampler = new ProfilingSampler(passName); this.settings = settings; renderPassEvent = settings.renderPassEvent; computeShader = settings.computeShader; cameraColorIden = new RenderTargetIdentifier(cameraColorTextureID); circularBlurTextureRIden = new RenderTargetIdentifier(circularBlurTextureRID); circularBlurTextureGIden = new RenderTargetIdentifier(circularBlurTextureGID); circularBlurTextureBIden = new RenderTargetIdentifier(circularBlurTextureBID); compositeTextureIden = new RenderTargetIdentifier(compositeTextureID); } public void Setup(CircularBlur circularBlur) { this.circularBlur = circularBlur; } private void EnsureComputeBuffer(int count, int stride) { if (computeBuffer == null || computeBuffer.count != count || computeBuffer.stride != stride) { if (computeBuffer != null) { computeBuffer.Release(); } computeBuffer = new ComputeBuffer(count, stride, ComputeBufferType.Structured); } } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { if(lastRadius != circularBlur.radius.value) { kernelData = CircularBlurFilterGenerator.GenerateFilter(circularBlur.radius.value, 2, 0.2f, false); int count = kernelData.filters.Length * (1 + kernelData.filters[0].circularKernels.Length); EnsureComputeBuffer(count, 2 * sizeof(float)); List\u0026lt;Vector2\u0026gt; data = new List\u0026lt;Vector2\u0026gt;(); foreach (var filter in kernelData.filters) { data.Add(filter.kernelWeight); data.AddRange(filter.circularKernels); } computeBuffer.SetData(data); lastRadius = circularBlur.radius.value; } } public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor) { textureSize = new Vector2Int(cameraTextureDescriptor.width, cameraTextureDescriptor.height); RenderTextureDescriptor desc = cameraTextureDescriptor; desc.enableRandomWrite = true; desc.msaaSamples = 1; desc.depthBufferBits = 0; desc.graphicsFormat = Experimental.Rendering.GraphicsFormat.R16G16B16A16_SFloat; cmd.GetTemporaryRT(circularBlurTextureRID, desc); cmd.GetTemporaryRT(circularBlurTextureGID, desc); cmd.GetTemporaryRT(circularBlurTextureBID, desc); cmd.GetTemporaryRT(compositeTextureID, desc); } private Vector4 GetTextureSizeParams(Vector2Int size) { return new Vector4(size.x, size.y, 1.0f / size.x, 1.0f / size.y); } private void DoCircularBlur(CommandBuffer cmd, RenderTargetIdentifier colorid, RenderTargetIdentifier rid, RenderTargetIdentifier gid, RenderTargetIdentifier bid, RenderTargetIdentifier compositeid, ComputeShader computeShader) { if (!computeShader) return; { int kernelID = computeShader.FindKernel(HorizontalKernelName); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, kernelID, _ColorTexture, colorid); cmd.SetComputeTextureParam(computeShader, kernelID, _RW_RTexture, rid); cmd.SetComputeTextureParam(computeShader, kernelID, _RW_GTexture, gid); cmd.SetComputeTextureParam(computeShader, kernelID, _RW_BTexture, bid); cmd.SetComputeBufferParam(computeShader, kernelID, _CircularKernels, computeBuffer); cmd.SetComputeVectorParam(computeShader, _TextureSize, GetTextureSizeParams(textureSize)); cmd.SetComputeIntParam(computeShader, _SampleRadius, Mathf.CeilToInt(circularBlur.radius.value)); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)textureSize.x / x), Mathf.CeilToInt((float)textureSize.y / y), 1); } { int kernelID = computeShader.FindKernel(VerticalKernelName); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, kernelID, _RTexture, rid); cmd.SetComputeTextureParam(computeShader, kernelID, _GTexture, gid); cmd.SetComputeTextureParam(computeShader, kernelID, _BTexture, bid); cmd.SetComputeTextureParam(computeShader, kernelID, _RW_CompositeTexture, compositeid); cmd.SetComputeBufferParam(computeShader, kernelID, _CircularKernels, computeBuffer); cmd.SetComputeVectorParam(computeShader, _TextureSize, GetTextureSizeParams(textureSize)); cmd.SetComputeIntParam(computeShader, _SampleRadius, Mathf.CeilToInt(circularBlur.radius.value)); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)textureSize.x / x), Mathf.CeilToInt((float)textureSize.y / y), 1); } cmd.Blit(compositeid, cameraColorIden); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(); using (new ProfilingScope(cmd, profilingSampler)) { DoCircularBlur(cmd, cameraColorIden, circularBlurTextureRIden, circularBlurTextureGIden, circularBlurTextureBIden, compositeTextureIden, computeShader); } context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } public override void FrameCleanup(CommandBuffer cmd) { cmd.ReleaseTemporaryRT(circularBlurTextureRID); cmd.ReleaseTemporaryRT(circularBlurTextureGID); cmd.ReleaseTemporaryRT(circularBlurTextureBID); cmd.ReleaseTemporaryRT(compositeTextureID); } public void Dispose() { if (computeBuffer != null) { computeBuffer.Release(); computeBuffer = null; } } } } CircularBlurRendererFeature.cs 和高斯模糊的Renderer Feature如出一辙。\nnamespace UnityEngine.Rendering.Universal { public class CircularBlurRendererFeature : ScriptableRendererFeature { [System.Serializable] public class CircularBlurSettings { public RenderPassEvent renderPassEvent; public ComputeShader computeShader; } public CircularBlurSettings settings = new CircularBlurSettings(); private CircularBlurRenderPass circularBlurRenderPass; public override void Create() { circularBlurRenderPass = new CircularBlurRenderPass(settings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { CircularBlur circularBlur = VolumeManager.instance.stack.GetComponent\u0026lt;CircularBlur\u0026gt;(); if (circularBlur != null \u0026amp;\u0026amp; circularBlur.IsActive()) { circularBlurRenderPass.Setup(circularBlur); renderer.EnqueuePass(circularBlurRenderPass); } } protected override void Dispose(bool disposing) { circularBlurRenderPass.Dispose(); base.Dispose(disposing); } } } 后记 太棒了太棒了，迅速地写完了EA的Circular Blur！本来想配一张夕阳下的椰树剪影和海面波光粼粼反光的场景，对其进行圆形模糊的，发现水的Shader还要自己写，Unity的默认材质球的Smoothness还要使用Albedo或者Metallic图的Alpha通道，导致MegaScans的素材还不能直接用，就暂时放弃了。等写完景深之后再整这些吧，顺便一提为了让封面图能有那个小小的散景的圆形，我偷偷地另外打了一盏灯。下一步，景深！！！\n","permalink":"https://zznewclear13.github.io/posts/unity-two-pass-circular-blur/","summary":"圆形模糊 圆形模糊，在Photoshop里又称镜头模糊(Lens Blur)，和景深结合在一起的时候被称作散景(Bokeh)，是指在摄影时失焦的区域产生的和光圈的形状一致的模糊效果，五边形八边形或是圆形都有可能。\n在计算机图形学中实现景深效果基本上有两种方法：第一种也是最常用的，通过黄金率生成一系列的采样点，使得其形状接近想要模糊的形状，这种方法需要很多很多的采样点，基本上找到的都是60次以上的采样次数，由于采样点的分布不一定正好在像素点中心，也不能轻易地使用Group Shared Memory进行优化，事实上大的模糊半径很可能导致Group Shared Memory的大小不够；另一种是针对于特殊的模糊形状，比如正六边形，可以使用三次（MRT的话可以认为是两次）1D的模糊来组合而成，可以在Colin Barré-Brisebois的博客Hexagonal Bokeh Blur Revisited中看到详细的说明，值得一提的是他此前也在EA工作过（看来EA是真的很喜欢散景啊）。\nEA的渲染工程师Kleber Garcia在2018年的GDC演讲Circular Separable Convolution Depth of Field中提到了通过复数的运算来实现圆形模糊的算法，其背后的数学我这里就不再赘述了，感兴趣的话可以参考Circularly symmetric convolution and lens blu这篇文章。圆形模糊的参数的生成的代码可以在Kleber Garcia的公开仓库里找到。Kleber Garcia本人也在Shadertoy上写了具体的圆形模糊的代码。\nCircular Dof\n由于景深效果相对来说比较复杂，这里就只考虑对整个屏幕施加相同程度的圆形模糊效果。\n具体的实现方法 其实大部分和之前的高斯模糊没有什么差别。在分离卷积圆形模糊的算法中，圆形的效果是通过多个Filter叠加而成的，每个Filter对应实部和虚部两个参数。以本文为例，本文使用了两个Filter，对于一个Filter的一个颜色分量，需要储存实部虚部两个数据，总体就需要2x3x2=12个通道，使用三张R16G16B16A16_SFloat就能储存所有的数据。在Kleber Garcia的演讲中他还提到了，可以使用bracket的方法，将颜色的卷积数据储存到另一张图中，这样中间的Filter的结果就会落在[0, 1]的范围内，就能使用R8G8A8B8来储存了，可以节省一半的带宽（但是颜色的卷积数据不也要一张32位的图吗，这里我没太懂，感觉优化了但又没那么优化，索性就没那么做）。\n整体的操作是：1. 采样源图片，对每个Filter和每个颜色分量计算实部和虚部的值，水平累加后储存到中间贴图中；2. 采样中间贴图，对每个Filter和每个颜色分量计算实部和虚部的值，竖直累加后乘上各自的权重就得到最终的颜色了。由于我没有使用bracket的方法，Filter中会有负值存在，在仅使用两个Filter且半径较大且像素颜色过亮的时候，由于banding的存在会使最终的颜色出现负值，解决方法是在读取颜色的时候做一次Clamp或者是ToneMapping到合理范围。\nCircularBlurFilterGenerator.cs 改写自Kleber Garcia的公开仓库。\n/* Copyright 2023 zznewclear13 (zznewclear@gmail.com) Copyright 2016 Kleber A Garcia (kecho_garcia@hotmail.com) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u0026#34;Software\u0026#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.","title":"Unity两个Pass的圆形模糊"},{"content":"将近两年之后再回过头来制作高斯模糊 虽然两年前已经写过了使用Group Shared Memory加速高斯模糊这篇文章了。但当时写的时候仍有一些遗憾的地方，由于使用的是长度为17的静态的高斯模糊的数组（实际上只有9个权重），虽然在一定程度上能够达到任意调节高斯模糊的程度的效果，但在较低程度的高斯模糊时，是通过手动线性插值找到合适的采样颜色，且一定会有17次的颜色和权重的运算；而在较高程度的高斯模糊时，由于仅有十七个有效的颜色点，会有明显的采样次数不足的瑕疵。\n而这两年之间我也曾考虑使用不同的方法来制作一个既能满足很高程度的高斯模糊，又能兼顾很小程度的高斯模糊，性能上也相对高效，且使用同一套通用的代码，的高斯模糊效果。下面便是我之前在Shadertoy上写的通过随机采样和历史混合的高斯模糊效果。\nStochastic Gaussian Blur\n但随机带来的噪声和历史混合带来的限制，决定了这种方法终究不能真正地使用在项目中，于是我又开始回到了使用Compute Shader和Group Shared Memory来计算高斯模糊效果的老路子上。不同的是，这次我使用了Compute Buffer把高斯模糊的参数传给Shader，这样就能确保范围内的每一个采样点都能够对最后的颜色产生应有的贡献。\n正态分布(Normal Distribution) 和之前不同的是，这次我们要先从正态分布入手，从正态分布的特性来考虑我们的计算方式。正态分布的概率密度函数(probability density function)如下所示： $$ f(x) = \\frac 1 {\\sigma \\sqrt{2 \\pi}} e^{- \\frac 1 2 (\\frac {x-\\mu} \\sigma)^2} $$ 使用正态分布对信号进行过滤，被称作高斯滤波器(Gaussian Filter)。我们在使用的时候会把\\(\\mu\\)设成0，这样永远是最中心的信号带来最大的贡献。但是这个概率密度函数的\\(x\\)的范围是\\((-\\infin, \\infin)\\)，我们不可能对所有的信号都进行采样，于是我们一般对\\(3\\sigma\\)范围内的信号进行采样，对1D的正态分布来说，\\((-3\\sigma, 3\\sigma)\\)占据了约99.7%的面积。因此我们往往使用三倍的\\(sigma\\)作为采样的半径，事实上在2D的时候，可能需要更大的采样半径才能消除明显的采样半径过小的瑕疵。\n有一点值得一提的是，虽然我并不会具体的微积分的计算，但据我所知先后执行两个\\(\\sigma\\)值分别为\\(x\\)和\\(y\\)高斯模糊，等价于执行一次\\(\\sigma\\)值为\\(\\sqrt {x^2+y^2}\\)的高斯模糊。\n另一个有趣的点是，在普通的模糊操作是我们往往会用降采样再升采样的方式来减少采样的次数。对于半分辨率的线性1D降采样和升采样，中心像素保留了\\(\\frac 3 8\\)的之前像素的信息，我们可以找到那么一个\\(\\sigma\\)的值使得其在\\((-0.5, 0.5)\\)之间的面积约等于\\(\\frac 3 8\\)，这样我们就能说我们通过线性降采样和升采样做到了近似对应\\(\\sigma\\)的高斯模糊的效果。可惜这个\\(\\sigma\\)不太好算，有Group Shared Memory也没有必要去做额外的降采样和升采样了。\n在本文中，会通过横竖两个1D高斯滤波器来等效一个2D的高斯滤波器，使用Group Shared Memory的话，倒是一个2D的高斯滤波器效率更高一些，不过为了后续的扩展性，本文拆成了两个滤波器。\n具体的实现方法 剩下的就和之前大同小异了，为了确保每个像素只会进行至多两次采样，需要限制高斯模糊的最大半径GAUSSIAN_BLUR_MAX_RADIUS为THREAD_GROUP_SIZE的一半。而为了2D的高斯模糊在比较极端的情况下也能有比较好的效果，我的高斯模糊的半径会是\\(\\sigma\\)的3.8倍向上取整。\nGaussianBlurComputeShader.compute 这是一个横竖两次高斯模糊的Compute Shader，通过Group Shared Memory优化了原本高斯模糊的每个像素的采样操作（至多两次）。最大模糊半径为128个像素。\n#pragma kernel GaussianH #pragma kernel GaussianV Texture2D\u0026lt;float4\u0026gt; _SourceTex; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTex; StructuredBuffer\u0026lt;float\u0026gt; _GaussianWeights; float4 _TextureSize; #define GAUSSIAN_BLUR_MAX_RADIUS 128 #define THREAD_GROUP_SIZE 256 const static int CACHED_COLOR_SIZE = THREAD_GROUP_SIZE +GAUSSIAN_BLUR_MAX_RADIUS*2; groupshared half3 cachedColor[CACHED_COLOR_SIZE]; void SetCachedColor(half3 color, int index) { cachedColor[index] = color; } half3 GetCachedColor(int threadPos) { return cachedColor[threadPos + GAUSSIAN_BLUR_MAX_RADIUS]; } void CacheColor(int2 groupCacheStartPos, int cacheIndex, int isHorizontal) { int2 texturePos = groupCacheStartPos + cacheIndex * int2(isHorizontal, 1 - isHorizontal); texturePos = clamp(texturePos, 0, _TextureSize.xy - 1.0f); half3 color = _SourceTex.Load(uint3(texturePos, 0)).rgb; SetCachedColor(color, cacheIndex); } half3 Gaussian(uint3 groupID, uint3 groupThreadID, uint groupIndex, uint3 dispatchThreadID, int isHorizontal) { int2 direction = int2(isHorizontal, 1 - isHorizontal); int2 theadGroupSize = (THREAD_GROUP_SIZE - 1) * direction + 1; int2 groupCacheStartPos = groupID.xy * theadGroupSize - GAUSSIAN_BLUR_MAX_RADIUS * direction; int cacheIndex = groupIndex * 2; if (cacheIndex \u0026lt; CACHED_COLOR_SIZE - 1) { CacheColor(groupCacheStartPos, cacheIndex, isHorizontal); CacheColor(groupCacheStartPos, cacheIndex+1, isHorizontal); } GroupMemoryBarrierWithGroupSync(); int sampleRadius = int(_GaussianWeights[0]); uint loadCacheIndex = groupIndex; int threadPos = loadCacheIndex; half3 sumColor = 0.0f; half sumWeight = 0.0f; for (int i=-sampleRadius; i\u0026lt;=sampleRadius; ++i) { half3 color = GetCachedColor(threadPos + i); half weight = _GaussianWeights[abs(i)+1]; sumColor += color * weight; sumWeight += weight; } return sumColor / sumWeight; } [numthreads(THREAD_GROUP_SIZE,1,1)] void GaussianH(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { half3 color = Gaussian(groupID, groupThreadID, groupIndex, dispatchThreadID, 1); _RW_TargetTex[dispatchThreadID.xy] = half4(color, 1.0f); } [numthreads(1, THREAD_GROUP_SIZE,1)] void GaussianV(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { half3 color = Gaussian(groupID, groupThreadID, groupIndex, dispatchThreadID, 0); _RW_TargetTex[dispatchThreadID.xy] = half4(color, 1.0f); } GausianBlur.cs 这里设置了采样半径为\\(\\sigma\\)的3.8倍，即使在有很亮的光斑的情况下，也能有很好的高斯模糊的效果（不好的话就再提高sigma的大小）。会通过\\(\\sigma\\)计算出采样半径和每个像素的权重。\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenuForRenderPipeline(\u0026#34;Post-processing/Gaussian Blur\u0026#34;, typeof(UniversalRenderPipeline))] public sealed class GaussianBlur : VolumeComponent, IPostProcessComponent { static float sigmaRadiusRatio = 3.8f; public BoolParameter isEnabled = new BoolParameter(false); public ClampedFloatParameter sigma = new ClampedFloatParameter(0.0f, 0.0f, Mathf.Floor(128.0f / sigmaRadiusRatio)); public bool IsActive() { return isEnabled.value \u0026amp;\u0026amp; sigma.value \u0026gt; 0.0f; } public bool IsTileCompatible() { return false; } private static float INV_SQRT_2PI = 0.3989422804f; private static float Gaussian(float sigma, float x) { float invSigma = 1.0f / sigma; return INV_SQRT_2PI * invSigma * Mathf.Exp(-0.5f * x * x * invSigma * invSigma); } public static int SigmaToRadius(float sigma) { return Mathf.CeilToInt(sigma * sigmaRadiusRatio); } public static float[] GetGaussianWeights(float sigma) { int length = SigmaToRadius(sigma); float[] weights = new float[length+1]; weights[0] = (float)length; for (int i = 0; i \u0026lt; length; i++) { weights[i+1] = Gaussian(sigma, (float)i); } return weights; } } } GaussianBlurRenderPass.cs 这里我谷歌了一下，找到了一个比较合适的避免Unity说我没有释放Compute Buffer的办法。很朴实无华的横竖两次高斯模糊，如果Camera Color Attachment能够支持随机读写的话，就能再节省一次Blit。我使用的是Unity 2021.3.19f1c1，Unity很奇怪地给Camera Color Attachment命名为“_CameraColorAttachementA”，不过没什么大碍。\nnamespace UnityEngine.Rendering.Universal { public class GaussianBlurRenderPass : ScriptableRenderPass { static readonly string passName = \u0026#34;Gaussian Blur Render Pass\u0026#34;; private GaussianBlurRendererFeature.GaussianBlurSettings settings; private GaussianBlur gaussianBlur; private ComputeShader computeShader; static readonly string cameraColorTextureName = \u0026#34;_CameraColorAttachmentA\u0026#34;; static readonly int cameraColorTextureID = Shader.PropertyToID(cameraColorTextureName); RenderTargetIdentifier cameraColorIden; static readonly string gaussianBlurTextureOneName = \u0026#34;_GaussianBlurTextureOne\u0026#34;; static readonly int gaussianBlurTextureOneID = Shader.PropertyToID(gaussianBlurTextureOneName); RenderTargetIdentifier gaussianBlurTextureOneIden; static readonly string gaussianBlurTextureTwoName = \u0026#34;_GaussianBlurTextureTwo\u0026#34;; static readonly int gaussianBlurTextureTwoID = Shader.PropertyToID(gaussianBlurTextureTwoName); RenderTargetIdentifier gaussianBlurTextureTwoIden; private ComputeBuffer computeBuffer; private Vector2Int textureSize; private float[] weights; static readonly string HorizontalKernelName = \u0026#34;GaussianH\u0026#34;; static readonly string VerticalKernelName = \u0026#34;GaussianV\u0026#34;; static readonly int _SourceTex = Shader.PropertyToID(\u0026#34;_SourceTex\u0026#34;); static readonly int _RW_TargetTex = Shader.PropertyToID(\u0026#34;_RW_TargetTex\u0026#34;); static readonly int _GaussianWeights = Shader.PropertyToID(\u0026#34;_GaussianWeights\u0026#34;); static readonly int _TextureSize = Shader.PropertyToID(\u0026#34;_TextureSize\u0026#34;); public GaussianBlurRenderPass(GaussianBlurRendererFeature.GaussianBlurSettings settings) { profilingSampler = new ProfilingSampler(passName); this.settings = settings; renderPassEvent = settings.renderPassEvent; computeShader = settings.computeShader; cameraColorIden = new RenderTargetIdentifier(cameraColorTextureID); gaussianBlurTextureOneIden = new RenderTargetIdentifier(gaussianBlurTextureOneID); gaussianBlurTextureTwoIden = new RenderTargetIdentifier(gaussianBlurTextureTwoID); } public void Setup(GaussianBlur gaussianBlur) { this.gaussianBlur = gaussianBlur; } private void EnsureComputeBuffer(int count, int stride) { if(computeBuffer == null || computeBuffer.count != count || computeBuffer.stride != stride) { if(computeBuffer != null) { computeBuffer.Release(); } computeBuffer = new ComputeBuffer(count, stride, ComputeBufferType.Structured); } } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { weights = GaussianBlur.GetGaussianWeights(gaussianBlur.sigma.value); int count = weights.Length; EnsureComputeBuffer(count, 4); computeBuffer.SetData(weights); } public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor) { textureSize = new Vector2Int(cameraTextureDescriptor.width, cameraTextureDescriptor.height); RenderTextureDescriptor desc = cameraTextureDescriptor; desc.enableRandomWrite = true; desc.msaaSamples = 1; desc.depthBufferBits = 0; cmd.GetTemporaryRT(gaussianBlurTextureOneID, desc); cmd.GetTemporaryRT(gaussianBlurTextureTwoID, desc); } private Vector4 GetTextureSizeParams(Vector2Int size) { return new Vector4(size.x, size.y, 1.0f / size.x, 1.0f / size.y); } private void DoGaussianBlur(CommandBuffer cmd, RenderTargetIdentifier colorid, RenderTargetIdentifier oneid, RenderTargetIdentifier twoid, ComputeShader computeShader) { if (!computeShader) return; { int kernelID = computeShader.FindKernel(HorizontalKernelName); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, kernelID, _SourceTex, colorid); cmd.SetComputeTextureParam(computeShader, kernelID, _RW_TargetTex, oneid); cmd.SetComputeBufferParam(computeShader, kernelID, _GaussianWeights, computeBuffer); cmd.SetComputeVectorParam(computeShader, _TextureSize, GetTextureSizeParams(textureSize)); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)textureSize.x / x), Mathf.CeilToInt((float)textureSize.y / y), 1); } { int kernelID = computeShader.FindKernel(VerticalKernelName); computeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, kernelID, _SourceTex, oneid); cmd.SetComputeTextureParam(computeShader, kernelID, _RW_TargetTex, twoid); cmd.SetComputeBufferParam(computeShader, kernelID, _GaussianWeights, computeBuffer); cmd.SetComputeVectorParam(computeShader, _TextureSize, GetTextureSizeParams(textureSize)); cmd.DispatchCompute(computeShader, kernelID, Mathf.CeilToInt((float)textureSize.x / x), Mathf.CeilToInt((float)textureSize.y / y), 1); } cmd.Blit(twoid, colorid); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(); using (new ProfilingScope(cmd, profilingSampler)) { DoGaussianBlur(cmd, cameraColorIden, gaussianBlurTextureOneIden, gaussianBlurTextureTwoIden, computeShader); } context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } public override void FrameCleanup(CommandBuffer cmd) { cmd.ReleaseTemporaryRT(gaussianBlurTextureOneID); cmd.ReleaseTemporaryRT(gaussianBlurTextureTwoID); } public void Dispose() { if (computeBuffer != null) { computeBuffer.Release(); computeBuffer = null; } } } } GaussianBlurRendererFeature.cs 没啥好说的了，加了一个Dispose方法来及时释放Compute Buffer。\nnamespace UnityEngine.Rendering.Universal { public class GaussianBlurRendererFeature : ScriptableRendererFeature { [System.Serializable] public class GaussianBlurSettings { public RenderPassEvent renderPassEvent; public ComputeShader computeShader; } public GaussianBlurSettings settings = new GaussianBlurSettings(); private GaussianBlurRenderPass gaussianBlurRenderPass; public override void Create() { gaussianBlurRenderPass = new GaussianBlurRenderPass(settings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { GaussianBlur gaussianBlur = VolumeManager.instance.stack.GetComponent\u0026lt;GaussianBlur\u0026gt;(); if(gaussianBlur != null \u0026amp;\u0026amp; gaussianBlur.IsActive()) { gaussianBlurRenderPass.Setup(gaussianBlur); renderer.EnqueuePass(gaussianBlurRenderPass); } } protected override void Dispose(bool disposing) { gaussianBlurRenderPass.Dispose(); base.Dispose(disposing); } } } 后记 2023年了怎么还在做高斯模糊啊喂，明明都写了不知道多少遍了。下一个目标是EA之前做过演讲的Circular Blur（虽然也写了很多遍固定大小的了）。什么时候才能重拾勇气去算景深呢呜呜呜。\n","permalink":"https://zznewclear13.github.io/posts/gaussian-blur-revisited/","summary":"将近两年之后再回过头来制作高斯模糊 虽然两年前已经写过了使用Group Shared Memory加速高斯模糊这篇文章了。但当时写的时候仍有一些遗憾的地方，由于使用的是长度为17的静态的高斯模糊的数组（实际上只有9个权重），虽然在一定程度上能够达到任意调节高斯模糊的程度的效果，但在较低程度的高斯模糊时，是通过手动线性插值找到合适的采样颜色，且一定会有17次的颜色和权重的运算；而在较高程度的高斯模糊时，由于仅有十七个有效的颜色点，会有明显的采样次数不足的瑕疵。\n而这两年之间我也曾考虑使用不同的方法来制作一个既能满足很高程度的高斯模糊，又能兼顾很小程度的高斯模糊，性能上也相对高效，且使用同一套通用的代码，的高斯模糊效果。下面便是我之前在Shadertoy上写的通过随机采样和历史混合的高斯模糊效果。\nStochastic Gaussian Blur\n但随机带来的噪声和历史混合带来的限制，决定了这种方法终究不能真正地使用在项目中，于是我又开始回到了使用Compute Shader和Group Shared Memory来计算高斯模糊效果的老路子上。不同的是，这次我使用了Compute Buffer把高斯模糊的参数传给Shader，这样就能确保范围内的每一个采样点都能够对最后的颜色产生应有的贡献。\n正态分布(Normal Distribution) 和之前不同的是，这次我们要先从正态分布入手，从正态分布的特性来考虑我们的计算方式。正态分布的概率密度函数(probability density function)如下所示： $$ f(x) = \\frac 1 {\\sigma \\sqrt{2 \\pi}} e^{- \\frac 1 2 (\\frac {x-\\mu} \\sigma)^2} $$ 使用正态分布对信号进行过滤，被称作高斯滤波器(Gaussian Filter)。我们在使用的时候会把\\(\\mu\\)设成0，这样永远是最中心的信号带来最大的贡献。但是这个概率密度函数的\\(x\\)的范围是\\((-\\infin, \\infin)\\)，我们不可能对所有的信号都进行采样，于是我们一般对\\(3\\sigma\\)范围内的信号进行采样，对1D的正态分布来说，\\((-3\\sigma, 3\\sigma)\\)占据了约99.7%的面积。因此我们往往使用三倍的\\(sigma\\)作为采样的半径，事实上在2D的时候，可能需要更大的采样半径才能消除明显的采样半径过小的瑕疵。\n有一点值得一提的是，虽然我并不会具体的微积分的计算，但据我所知先后执行两个\\(\\sigma\\)值分别为\\(x\\)和\\(y\\)高斯模糊，等价于执行一次\\(\\sigma\\)值为\\(\\sqrt {x^2+y^2}\\)的高斯模糊。\n另一个有趣的点是，在普通的模糊操作是我们往往会用降采样再升采样的方式来减少采样的次数。对于半分辨率的线性1D降采样和升采样，中心像素保留了\\(\\frac 3 8\\)的之前像素的信息，我们可以找到那么一个\\(\\sigma\\)的值使得其在\\((-0.5, 0.5)\\)之间的面积约等于\\(\\frac 3 8\\)，这样我们就能说我们通过线性降采样和升采样做到了近似对应\\(\\sigma\\)的高斯模糊的效果。可惜这个\\(\\sigma\\)不太好算，有Group Shared Memory也没有必要去做额外的降采样和升采样了。\n在本文中，会通过横竖两个1D高斯滤波器来等效一个2D的高斯滤波器，使用Group Shared Memory的话，倒是一个2D的高斯滤波器效率更高一些，不过为了后续的扩展性，本文拆成了两个滤波器。\n具体的实现方法 剩下的就和之前大同小异了，为了确保每个像素只会进行至多两次采样，需要限制高斯模糊的最大半径GAUSSIAN_BLUR_MAX_RADIUS为THREAD_GROUP_SIZE的一半。而为了2D的高斯模糊在比较极端的情况下也能有比较好的效果，我的高斯模糊的半径会是\\(\\sigma\\)的3.8倍向上取整。\nGaussianBlurComputeShader.compute 这是一个横竖两次高斯模糊的Compute Shader，通过Group Shared Memory优化了原本高斯模糊的每个像素的采样操作（至多两次）。最大模糊半径为128个像素。\n#pragma kernel GaussianH #pragma kernel GaussianV Texture2D\u0026lt;float4\u0026gt; _SourceTex; RWTexture2D\u0026lt;float4\u0026gt; _RW_TargetTex; StructuredBuffer\u0026lt;float\u0026gt; _GaussianWeights; float4 _TextureSize; #define GAUSSIAN_BLUR_MAX_RADIUS 128 #define THREAD_GROUP_SIZE 256 const static int CACHED_COLOR_SIZE = THREAD_GROUP_SIZE +GAUSSIAN_BLUR_MAX_RADIUS*2; groupshared half3 cachedColor[CACHED_COLOR_SIZE]; void SetCachedColor(half3 color, int index) { cachedColor[index] = color; } half3 GetCachedColor(int threadPos) { return cachedColor[threadPos + GAUSSIAN_BLUR_MAX_RADIUS]; } void CacheColor(int2 groupCacheStartPos, int cacheIndex, int isHorizontal) { int2 texturePos = groupCacheStartPos + cacheIndex * int2(isHorizontal, 1 - isHorizontal); texturePos = clamp(texturePos, 0, _TextureSize.","title":"再议高斯模糊"},{"content":"缺氧的瓦片渲染的特点 很可惜我没有在RenderDoc里截到缺氧的帧，不过我还是能从渲染表现上来分析一下缺氧的瓦片渲染的特点。经过一段时间的游玩和从下面这张图中可以看到，缺氧的游戏逻辑是把整个2D的地图分成一个一个格子，每个格子记录了气体、液体、固体和建筑物的信息。气体只是一个扭曲的Shader，液体渲染和计算比较复杂，这里暂时不考虑，建筑物中的墙和管线虽然也有程序化生成再渲染的效果，但和场景中资源类型的固体格子是硬相接的关系，这里也不考虑。本文的研究重点放在资源类型的固体格子的渲染上（不包括这些格子的程序化生成）。\n资源类型的固体格子（这里就简称瓦片了）的特点如下：\n有多种类型的瓦片 仅在不同类型的瓦片相接时会有黑色的描边 瓦片之间会有排序，优先级高的瓦片会更多地扩张 瓦片之间黑色的描边呈现周期性规律 模仿这种渲染的思路 最简单的思路肯定就是在CPU中计算每一个瓦片应当有的形态，然后找到对应的贴图，把瓦片在GPU中绘制出来了。但是这样子做的话就失去了本文的意义，也太过无趣了。我想的是尽量多地用GPU来计算每个瓦片的形态，同时使用Instancing的方式，绘制每一个瓦片。\n第一个问题是，不规则的瓦片应当如何绘制。如果是正方形的瓦片，能够很轻易地使用一个Quad和纹理来绘制，但是不规则的瓦片，势必会使用透明度混合的方式来绘制，这时对应的模型就会超出瓦片的游戏逻辑上的位置。因此，我想的是绘制的Quad的数量是瓦片实际数量的两倍加一，如下图所示：\n在这张图中，ABC代表了不同类型的瓦片，左边是游戏游玩的时候逻辑上的瓦片分布，ABC是相接的，右边是在渲染的时候的瓦片的分布，在原有瓦片中间插入新的瓦片，专门用来渲染接缝。对于2号瓦片，其左上角右上角右下角左下角（顺时针的顺序）分别是ABCC，决定了这是一块三块相接的瓦片；对于1号瓦片，对应的编号是AACC（通过一些对2取模取余的运算可以排除掉B），决定了这是一块两块相接的瓦片；而对于3和4号瓦片，其编号为CCCC，决定了这两块是没有接缝的瓦片。这时我们又考虑到了瓦片之间优先级的关系，假设C\u0026gt;B\u0026gt;A，则AACC和AABB的接缝应当是相同的，ABCC和BCCA是旋转了九十度的关系。考虑到必定会有一个瓦片处于最低优先级，我们只需要将最低优先级的瓦片固定到左上角，讨论剩下三个瓦片的优先级与顺序即可。循着这个思路，我们可以把所有可能的接缝画在一张图上，这张图的RGBA通道记录了瓦片的优先级（R优先级最低，A优先级最高，接缝我使用了一个统一的灰色以便后续渲染），图片如下所示，为了比较容易观察，我对A通道做了反向，且对应的在下方标注了优先级顺序。同时我们还对应的写好一个函数用于根据优先级顺序找到对应的接缝类型从而在渲染时找到接缝在图上的位置（见ONITileRender.hlsl中的GetMode(uint a, uint b, uint c)）。\n由于会有优先级的比较，不可避免地会在GPU中进行排序，使用MergeSort的话，4个元素会有5次比较，由于我们还需要获得每个瓦片在四个瓦片中排序的序号，这里就硬写了手动比较，6次比较和MergeSort的5次也差不太多。我们绘制的图上仅有最低优先级瓦片在左上角的情况，因此我们还需要找到最低优先级瓦片初始的序号，从而在渲染时旋转我们的接缝图（这里就体现了我们使用顺时针编号的优势，方便了旋转的操作，如果是左上角右上角左下角右下角的顺序，就不太好旋转了）。\n知道了每一个接缝图的旋转，我们还需要为其每一个部分（通道）渲染不同的贴图。这里使用了DrawProceduralIndirect来进行Instancing的渲染，DrawCall数量会和瓦片类型的数量一样多。对于一种瓦片，需要渲染的总瓦片数相当于是这类瓦片的图形向外扩展一个瓦片的数量，我们可以通过判断左上右上右下左下的瓦片类型来轻易地判断当前瓦片是否应该和目标瓦片类型一起渲染。我们会使用一个数据数量为瓦片类型数量*(2*地图宽高+1)的StructuredBuffer来统计所有应当绘制的瓦片（实际使用的大小不会大于4*(2*地图宽+1)*(2*地图高+1)）。同时我们会使用一个数据数量为瓦片类型数量*5的ByteAddressBuffer来统计每种瓦片类型Instancing时需要的参数。\n本文中的岩石的2D无缝贴图来自OpenGameArt.org\n具体的代码和相关的解释 由于会用到CommandBuffer进行瓦片的绘制，我就把相关的代码放到Universal RP的Package里了。CPU代码，ONITileRenderManager.cs放在Packages/com.unity.render-pipelines.universal/Runtime/Overrides/下，ONITileRendererFeature.cs放在Packages/com.unity.render-pipelines.universal/Runtime/RendererFeature/下，ONITileRenderPass.cs放在Packages/com.unity.render-pipelines.universal/Runtime/Passes/下；GPU代码，ONITileRender.hlsl，ONITileComputeShader.compute和ONITileRenderShader.shader放在Packages/com.unity.render-pipelines.universal/Shaders/ONITile/下。\nONITileRenderManager用于地图的设置、计算和Buffer的获取。ONITileRendererFeature和ONITileRenderPass用于在Unity URP中渲染瓦片，ONITileComputeShader用于瓦片相关的计算，ONITileRenderShader用于瓦片的渲染。\nONITileRenderManager.cs 这里尤其需要注意每个Buffer的大小。在这个脚本里使用Compute Shader做了三件事：1. 对地图每一个点生成一个随机数作为瓦片类型；2. 从地图中计算每一种瓦片类型需要绘制的数量、位置、解封类型、旋转和应当采样的通道；3. 把ByteAddressBuffer中的数据复制到IndirectArgumentBuffer里。事实上我感觉ComputeShader.Dispatch应该做成一个异步的方法，不过这个调用频率不高，就这样好了。\nusing UnityEngine; [ExecuteInEditMode] public class ONITileRenderManager : MonoBehaviour { [HideInInspector] public static ONITileRenderManager Instance { get; private set; } public ComputeShader oniTileComputeShader; public int tileTypeCount = 4; public Vector2Int tileCount = new Vector2Int(16, 16); public Vector2 tileSize = Vector2.one; public Vector3 tileStartPos; public Vector2 randomSeed; public Texture[] mainTextures = new Texture[] {}; public Vector4 mainTextureST = new Vector4(1.0f, 1.0f, 0.0f, 0.0f); private Vector2Int tileCountExt; public Vector2Int TileCountExt { get { return tileCountExt; } } private Vector4 textureSize; public Vector4 TextureSize { get { return textureSize; } } private Vector4 textureSizeExt; public Vector4 TextureSizeExt { get { return textureSizeExt; } } private ComputeBuffer computeBuffer; public ComputeBuffer ComputeBuffer { get { return computeBuffer; } } private ComputeBuffer argBuffer; public ComputeBuffer ArgBuffer { get { return argBuffer; } } private ComputeBuffer counterBuffer; public ComputeBuffer CounterBuffer { get { return counterBuffer; } } private RenderTexture tileRenderTexture; private RenderTexture tileRenderTextureExt; private bool hasValidBuffer = false; public bool HasValidBuffer { get { return hasValidBuffer; } } struct PerTileProperty { public Vector2Int coord; public uint mode; public uint rotation; public uint channel; } private void EnsureRenderTexture(ref RenderTexture rt, int width, int height) { if (rt == null || rt.width != width || rt.height != height) { if (rt != null) RenderTexture.ReleaseTemporary(rt); RenderTextureDescriptor desc = new RenderTextureDescriptor(width, height, RenderTextureFormat.ARGBInt); desc.enableRandomWrite = true; desc.msaaSamples = 1; desc.depthBufferBits = 0; rt = RenderTexture.GetTemporary(desc); if (!rt.IsCreated()) rt.Create(); } } private void EnsureComputeBuffer(ref ComputeBuffer cb, int count, int stride, ComputeBufferType cbt = ComputeBufferType.Append) { if (cb == null || cb.count != count || cb.stride != stride) { if (cb != null) cb.Release(); cb = new ComputeBuffer(count, stride, cbt); } } private void OnEnable() { if (Instance != null) { enabled = false; Debug.LogError(\u0026#34;An instance of ONITileRenderManager already exists.\u0026#34;); } else { Instance = this; } } private void OnDisable() { Instance = null; } private void OnValidate() { tileTypeCount = Mathf.Max(1, tileTypeCount); tileCount.x = Mathf.Max(1, tileCount.x); tileCount.y = Mathf.Max(1, tileCount.y); tileCountExt = new Vector2Int(tileCount.x * 2 + 1, tileCount.y * 2 + 1); textureSize = new Vector4(tileCount.x, tileCount.y, 1.0f / tileCount.x, 1.0f / tileCount.y); textureSizeExt = new Vector4(tileCountExt.x, tileCountExt.y, 1.0f / tileCountExt.x, 1.0f / tileCountExt.y); EnsureComputeBuffer(ref computeBuffer, tileTypeCount * tileCountExt.x * tileCountExt.y, System.Runtime.InteropServices.Marshal.SizeOf\u0026lt;PerTileProperty\u0026gt;()); EnsureComputeBuffer(ref argBuffer, tileTypeCount * 5, 4, ComputeBufferType.IndirectArguments); EnsureComputeBuffer(ref counterBuffer, tileTypeCount * 5, 4, ComputeBufferType.Raw); EnsureRenderTexture(ref tileRenderTexture, tileCount.x, tileCount.y); EnsureRenderTexture(ref tileRenderTextureExt, tileCountExt.x, tileCountExt.y); GenerateRandomTiles(); hasValidBuffer = false; ExpandTileTexture(); CopyToArgBuffer(); hasValidBuffer = true; } private void GenerateRandomTiles() { if (!oniTileComputeShader) return; int kernelID = oniTileComputeShader.FindKernel(ONITileShaderConstants.S_RANDOM_KERNEL_NAME); oniTileComputeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); oniTileComputeShader.SetVector(ONITileShaderConstants.I_RandomSeed, randomSeed); oniTileComputeShader.SetInt(ONITileShaderConstants.I_TileTypeCount, tileTypeCount); oniTileComputeShader.SetVector(ONITileShaderConstants.I_TextureSize, textureSize); oniTileComputeShader.SetTexture(kernelID, ONITileShaderConstants.I_RW_RandomTiles, tileRenderTexture); oniTileComputeShader.Dispatch(kernelID, Mathf.CeilToInt((float)tileCount.x / x), Mathf.CeilToInt((float)tileCount.y / y), 1); } private void ExpandTileTexture() { if (!oniTileComputeShader) return; int[] data = new int[counterBuffer.count]; for (int i = 0; i \u0026lt; data.Length; i++) { if (i % 5 == 0) { data[i] = 6; } else { data[i] = 0; } } counterBuffer.SetData(data); int kernelID = oniTileComputeShader.FindKernel(ONITileShaderConstants.S_EXPAND_KERNEL_NAME); oniTileComputeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); oniTileComputeShader.SetVector(ONITileShaderConstants.I_TextureSize, textureSize); oniTileComputeShader.SetVector(ONITileShaderConstants.I_TextureSizeExt, textureSizeExt); oniTileComputeShader.SetTexture(kernelID, ONITileShaderConstants.I_RandomTiles, tileRenderTexture); oniTileComputeShader.SetTexture(kernelID, ONITileShaderConstants.I_RW_RandomTilesExt, tileRenderTextureExt); oniTileComputeShader.SetBuffer(kernelID, ONITileShaderConstants.I_RW_TileData, computeBuffer); oniTileComputeShader.SetBuffer(kernelID, ONITileShaderConstants.I_RW_CounterBuffer, counterBuffer); oniTileComputeShader.Dispatch(kernelID, Mathf.CeilToInt((float)tileCountExt.x / x), Mathf.CeilToInt((float)tileCountExt.y / y), 1); } private void CopyToArgBuffer() { if (!oniTileComputeShader) return; int kernelID = oniTileComputeShader.FindKernel(ONITileShaderConstants.S_COPY_KERNEL_NAME); oniTileComputeShader.GetKernelThreadGroupSizes(kernelID, out uint x, out uint y, out uint z); oniTileComputeShader.SetInt(ONITileShaderConstants.I_TileTypeCount, tileTypeCount); oniTileComputeShader.SetBuffer(kernelID, ONITileShaderConstants.I_CounterBuffer, counterBuffer); oniTileComputeShader.SetBuffer(kernelID, ONITileShaderConstants.I_RW_ArgBuffer, ArgBuffer); oniTileComputeShader.Dispatch(kernelID, Mathf.CeilToInt((float)(5*tileTypeCount) / x), 1, 1); } public Bounds GetBounds() { Vector3 start = tileStartPos; Vector2 totalSize = tileSize * tileCountExt; Vector3 size3D = new Vector3(totalSize.x, 0.0f, totalSize.y) + Vector3.one; Vector3 center = start + 0.5f * size3D; return new Bounds(center, size3D); } private void OnDestroy() { if (computeBuffer != null) { computeBuffer.Release(); computeBuffer = null; } if (argBuffer != null) { argBuffer.Release(); argBuffer = null; } if (tileRenderTexture != null) { RenderTexture.ReleaseTemporary(tileRenderTexture); tileRenderTexture = null; } if (tileRenderTextureExt != null) { RenderTexture.ReleaseTemporary(tileRenderTextureExt); tileRenderTextureExt = null; } } public class ONITileShaderConstants { public static readonly string S_RANDOM_KERNEL_NAME = \u0026#34;RandomMain\u0026#34;; public static readonly string S_EXPAND_KERNEL_NAME = \u0026#34;ExpandMain\u0026#34;; public static readonly string S_COPY_KERNEL_NAME = \u0026#34;CopyMain\u0026#34;; public static readonly int I_TextureSize = Shader.PropertyToID(\u0026#34;_TextureSize\u0026#34;); public static readonly int I_TextureSizeExt = Shader.PropertyToID(\u0026#34;_TextureSizeExt\u0026#34;); public static readonly int I_TileTypeCount = Shader.PropertyToID(\u0026#34;_TileTypeCount\u0026#34;); public static readonly int I_RandomSeed = Shader.PropertyToID(\u0026#34;_RandomSeed\u0026#34;); public static readonly int I_RandomTiles = Shader.PropertyToID(\u0026#34;_RandomTiles\u0026#34;); public static readonly int I_RW_RandomTiles = Shader.PropertyToID(\u0026#34;_RW_RandomTiles\u0026#34;); public static readonly int I_RW_RandomTilesExt = Shader.PropertyToID(\u0026#34;_RW_RandomTilesExt\u0026#34;); public static readonly int I_RW_TileData = Shader.PropertyToID(\u0026#34;_RW_TileData\u0026#34;); public static readonly int I_RW_ArgBuffer = Shader.PropertyToID(\u0026#34;_RW_ArgBuffer\u0026#34;); public static readonly int I_CounterBuffer = Shader.PropertyToID(\u0026#34;_CounterBuffer\u0026#34;); public static readonly int I_RW_CounterBuffer = Shader.PropertyToID(\u0026#34;_RW_CounterBuffer\u0026#34;); public static readonly int I_TileTexture = Shader.PropertyToID(\u0026#34;_TileTexture\u0026#34;); public static readonly int I_TileData = Shader.PropertyToID(\u0026#34;_TileData\u0026#34;); public static readonly int I_TileSize = Shader.PropertyToID(\u0026#34;_TileSize\u0026#34;); public static readonly int I_TileStartPos = Shader.PropertyToID(\u0026#34;_TileStartPos\u0026#34;); public static readonly int I_TileType = Shader.PropertyToID(\u0026#34;_TileType\u0026#34;); public static readonly int I_MainTexture = Shader.PropertyToID(\u0026#34;_MainTexture\u0026#34;); public static readonly int I_MainTexture_ST = Shader.PropertyToID(\u0026#34;_MainTexture_ST\u0026#34;); } } ONITileRendererFeature.cs 没什么好说的，照本宣科罢了。\nnamespace UnityEngine.Rendering.Universal { public class ONITileRendererFeature : ScriptableRendererFeature { [System.Serializable] public class ONITileSettings { public RenderPassEvent renderPassEvent = RenderPassEvent.BeforeRenderingTransparents; public Shader drawShader; public Texture tileTexture; public bool IsValid() { return drawShader != null \u0026amp;\u0026amp; tileTexture != null; } } private ONITileRenderPass oniTileRenderPass; public ONITileSettings oniTileSettings = new ONITileSettings(); public override void Create() { oniTileRenderPass = new ONITileRenderPass(oniTileSettings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { if (ONITileRenderManager.Instance != null \u0026amp;\u0026amp; oniTileSettings.IsValid()) { oniTileRenderPass.Setup(ONITileRenderManager.Instance); renderer.EnqueuePass(oniTileRenderPass); } } } } ONITileRenderPass.cs DrawProceduralIndirect需要的argument一共五个uint的数据，分别为每个实例的顶点数，实例数，顶点起始位置，实例起始位置和一个预留给OpenGL的空位。如果我们需要画十三个Quad，我们只需要传入{6, 13, 0, 0, 0}即可。i * 5 * 4意味着对第i个瓦片类型，我们需要读取i*5+0到i*5+4这五个uint的数据作为IndirectDraw的argument，而uint的大小为4。\nnamespace UnityEngine.Rendering.Universal { public class ONITileRenderPass : ScriptableRenderPass { private const string profilerTag = \u0026#34;ONI Tile Render Pass\u0026#34;; private ProfilingSampler oniTileRenderSampler = new ProfilingSampler(profilerTag); private ONITileRendererFeature.ONITileSettings settings; private ONITileRenderManager oniTileRenderManager; private Material drawMaterial; public ONITileRenderPass(ONITileRendererFeature.ONITileSettings settings) { this.settings = settings; renderPassEvent = settings.renderPassEvent; if (settings.drawShader != null) { drawMaterial = new Material(settings.drawShader); } } public void Setup(ONITileRenderManager oniTileRenderManager) { this.oniTileRenderManager = oniTileRenderManager; } private void DoONITileRendering(CommandBuffer cmd, Material material) { if(oniTileRenderManager.HasValidBuffer) { MaterialPropertyBlock mpb = new MaterialPropertyBlock(); mpb.SetBuffer(ONITileRenderManager.ONITileShaderConstants.I_TileData, oniTileRenderManager.ComputeBuffer); mpb.SetVector(ONITileRenderManager.ONITileShaderConstants.I_TextureSizeExt, oniTileRenderManager.TextureSizeExt); mpb.SetVector(ONITileRenderManager.ONITileShaderConstants.I_TileStartPos, oniTileRenderManager.tileStartPos); mpb.SetVector(ONITileRenderManager.ONITileShaderConstants.I_TileSize, oniTileRenderManager.tileSize); mpb.SetTexture(ONITileRenderManager.ONITileShaderConstants.I_TileTexture, settings.tileTexture); mpb.SetVector(ONITileRenderManager.ONITileShaderConstants.I_MainTexture_ST, oniTileRenderManager.mainTextureST); int mainTextureLength = oniTileRenderManager.mainTextures.Length; for (int i = 0; i \u0026lt; oniTileRenderManager.tileTypeCount; i++) { if(mainTextureLength \u0026gt; 0) { int mainTextureIndex = i % mainTextureLength; mpb.SetTexture(ONITileRenderManager.ONITileShaderConstants.I_MainTexture, oniTileRenderManager.mainTextures[mainTextureIndex]); } mpb.SetInt(ONITileRenderManager.ONITileShaderConstants.I_TileType, i); cmd.DrawProceduralIndirect(Matrix4x4.identity, material, 0, MeshTopology.Triangles, oniTileRenderManager.ArgBuffer, i * 5 * 4, properties:mpb); } } } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(profilerTag); context.ExecuteCommandBuffer(cmd); cmd.Clear(); using (new ProfilingScope(cmd, oniTileRenderSampler)) { DoONITileRendering(cmd, drawMaterial); } context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } } } ONITileRender.hlsl 这里定义了一个结构体PerTileProperty，需要和CPU代码里结构体的数据布局保持一致。事实上，channel和rotation都只是0-3的int类型，占两个bit，可以合并在一起，这样一个PerTileProperty刚好是四个字节，这里就不这么做了。至于中间的函数，我真的不太擅长命名。SortAndReturnIndex在排序的同时，还返回了每个元素在排序后的序号，方便后面的处理。ProcessSortedArray用于处理排序，生成并列第二名第三名。RotateAccordingToMinimum是旋转处理好的排序，从而通过GetMode获取接缝类型。我尽可能地减少了GetMode的分支数量（其实是一种二进制+三进制+特例）。\n#ifndef ONI_TILE_RENDER_HLSL #define ONI_TILE_RENDER_HLSL #define ONI_TILE_TEXTURE_SIZE 8u struct PerTileProperty { uint2 coord; uint mode; uint rotation; uint channel; }; SamplerState sampler_LinearClamp; SamplerState sampler_PointClamp; uint _TileTypeCount; float4 _TextureSize; float4 _TextureSizeExt; float3 _TileStartPos; float2 _TileSize; // https://www.shadertoy.com/view/4djSRW float hash12(float2 p) { float3 p3 = frac(float3(p.xyx) * .1031); p3 += dot(p3, p3.yzx + 33.33); return frac((p3.x + p3.y) * p3.z); } // A sort4 function with 6 compares. // output: sorted array of input; // return: sorted index of elements from input; uint4 SortAndReturnIndex(float4 input, out uint output[4]) { uint3 ab_ac_ad = input.xxx \u0026lt;= input.yzw ? 0 : 1; uint3 bc_bd_cd = input.yyz \u0026lt;= input.zww ? 0 : 1; uint indexA = ab_ac_ad.x + ab_ac_ad.y + ab_ac_ad.z; uint indexB = 1 - ab_ac_ad.x + bc_bd_cd.x + bc_bd_cd.y; uint indexC = 2 - ab_ac_ad.y - bc_bd_cd.x + bc_bd_cd.z; uint indexD = 3 - ab_ac_ad.z - bc_bd_cd.y - bc_bd_cd.z; output[indexA] = 0; output[indexB] = 1; output[indexC] = 2; output[indexD] = 3; return uint4(indexA, indexB, indexC, indexD); } // Input numbers might have equal elements, adjust sorted index based on that. void ProcessSortedArray(float4 input, uint4 output, inout uint processedArray[4]) { if(input[output.x] == input[output.y]) { processedArray[output.y] -= 1; processedArray[output.z] -= 1; processedArray[output.w] -= 1; } if(input[output.y] == input[output.z]) { processedArray[output.z] -= 1; processedArray[output.w] -= 1; } if(input[output.z] == input[output.w]) { processedArray[output.w] -= 1; } } // Rotate processed array according to the index of minimum element. uint4 RotateAccordingToMinimum(uint processedArray[4], uint sortedArray[4]) { uint minIndex = sortedArray[0]; uint rotateX = processedArray[(0+minIndex)%4]; uint rotateY = processedArray[(1+minIndex)%4]; uint rotateZ = processedArray[(2+minIndex)%4]; uint rotateW = processedArray[(3+minIndex)%4]; return uint4(rotateX, rotateY, rotateZ, rotateW); } // Get mode based on \u0026#34;sorted-processed-rotated\u0026#34; result, mode is used to sample tile texture later. // 0 a // c b uint GetMode(uint a, uint b, uint c) { if(a==0) { if(b==0) return c==0 ? 0 : 1; if(b==1) return c+2; if(b==2) return 5; } if(a==1) { return b==3 ? 16 : c+b*3+6; } if(a==2) { if(b==0) return 17; if(b==1) return c+18; if(b==2) return 22; if(b==3) return 23; } if(a==3) { return b==1 ? 24 : 25; } return 0; } #endif ONITileComputeShader.compute 这里定义了三个Kernel，分别用于生成随机数，找到所有应渲染的瓦片及其属性，把ByteAddressBuffer的内容复制到IndirectArgumentBuffer。实际运用的话我们不会在GPU里生成瓦片，理论上也没必要做一次Buffer数据的复制。使用了ByteAddressBuffer.InterlockedAdd来获取当前类型的瓦片需要Instancing的数量（并根据这个数量储存对应的PerTileProperty）。\n#pragma kernel RandomMain #pragma kernel ExpandMain #pragma kernel CopyMain #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;ONITileRender.hlsl\u0026#34; float2 _RandomSeed; Texture2D\u0026lt;float4\u0026gt; _RandomTiles; RWTexture2D\u0026lt;float4\u0026gt; _RW_RandomTiles; RWTexture2D\u0026lt;float4\u0026gt; _RW_RandomTilesExt; ByteAddressBuffer _CounterBuffer; RWStructuredBuffer\u0026lt;PerTileProperty\u0026gt; _RW_TileData; RWStructuredBuffer\u0026lt;uint\u0026gt; _RW_ArgBuffer; RWByteAddressBuffer _RW_CounterBuffer; [numthreads(8,8,1)] void RandomMain (uint3 dispatchThreadID : SV_DispatchThreadID) { if(any((float2)dispatchThreadID.xy \u0026gt;= _TextureSize.xy)) return; float randomVal = hash12(dispatchThreadID.xy + _RandomSeed); float val = floor(randomVal * _TileTypeCount); float4 returnColor = float4(val, val, val, val); _RW_RandomTiles[dispatchThreadID.xy] = returnColor; } float LoadFromRandomTiles(Texture2D\u0026lt;float4\u0026gt; tex, int2 coord, float2 textureSize) { float2 tempCoord = clamp(float2(0.0f, 0.0f), textureSize-1.0f, coord); return tex.Load(uint3(tempCoord, 0)).r; } [numthreads(8, 8, 1)] void ExpandMain (uint3 dispatchThreadID : SV_DispatchThreadID) { if(any((float2)dispatchThreadID.xy \u0026gt;= _TextureSizeExt.xy)) return; float tr = LoadFromRandomTiles(_RandomTiles, dispatchThreadID.xy / 2, _TextureSize.xy); float tl = LoadFromRandomTiles(_RandomTiles, (int2)((dispatchThreadID.xy+uint2(1, 0)) / 2) - int2(1, 0), _TextureSize.xy); float br = LoadFromRandomTiles(_RandomTiles, (int2)((dispatchThreadID.xy+uint2(0, 1)) / 2) - int2(0, 1), _TextureSize.xy); float bl = LoadFromRandomTiles(_RandomTiles, (int2)((dispatchThreadID.xy+uint2(1, 1)) / 2) - int2(1, 1), _TextureSize.xy); // x y // w z float4 packedColor = float4(tl, tr, br, bl); uint output[4]; uint4 sortedIndex = SortAndReturnIndex(packedColor, output); uint4 tempOutput = int4(output[0], output[1], output[2], output[3]); uint processedArray[4] = {sortedIndex.x, sortedIndex.y, sortedIndex.z, sortedIndex.w}; ProcessSortedArray(packedColor, tempOutput, processedArray); uint4 rotatedIndex = RotateAccordingToMinimum(processedArray, output); uint mode = GetMode(rotatedIndex.y, rotatedIndex.z, rotatedIndex.w); for (float i = 0; i \u0026lt; (float)_TileTypeCount; i++) { bool shouldRender = false; int channel = 3; if(bl == i) { shouldRender = true; channel = processedArray[3]; } if(br == i) { shouldRender = true; channel = processedArray[2]; } if(tr == i) { shouldRender = true; channel = processedArray[1]; } if(tl == i) { shouldRender = true; channel = processedArray[0]; } if(shouldRender) { uint totalCount; _RW_CounterBuffer.InterlockedAdd(4 + i * 5 * 4, 1, totalCount); PerTileProperty prop = (PerTileProperty)0; prop.coord = dispatchThreadID.xy; prop.mode = mode; prop.rotation = tempOutput.x; prop.channel = channel; _RW_TileData[totalCount + i * _TextureSizeExt.x * _TextureSizeExt.y] = prop; } } _RW_RandomTilesExt[dispatchThreadID.xy] = float4(mode, tempOutput.x, 0.0f, 1.0f); } [numthreads(16, 1, 1)] void CopyMain (uint3 dispatchThreadID : SV_DispatchThreadID) { if(dispatchThreadID.x \u0026gt;= 5 * _TileTypeCount) return; uint status; _RW_ArgBuffer[dispatchThreadID.x] = _CounterBuffer.Load(4 * dispatchThreadID.x, status); } ONITileRenderShader.shader 由于我们画的TileTexture是从左上角开始的，采样的时候会用到一些用1减去uv的y值，旋转也不会乘上-1。前面给缝隙画上灰色的好处也在这里体现出来，我们可以用两个smoothstep来找到一定程度上抗锯齿的渲染区域和缝隙区域。此外在开启MSAA的时候可能会需要根据uv是否在01之间剔除掉当前像素，不然会有奇怪的边缘锯齿。\nShader \u0026#34;zznewclear13/ONITileRenderShader\u0026#34; { HLSLINCLUDE #pragma enable_d3d11_debug_symbols #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;ONITileRender.hlsl\u0026#34; StructuredBuffer\u0026lt;PerTileProperty\u0026gt; _TileData; sampler2D _TileTexture; sampler2D _MainTexture; float4 _MainTexture_ST; uint _TileType; struct Attributes { uint vertexID : SV_VERTEXID; uint instanceID : SV_INSTANCEID; }; struct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; float3 positionWS : TEXCOORD1; uint mode : TEXCOORD2; uint rotation : TEXCOORD3; uint channel : TEXCOORD4; }; Varyings vert(Attributes input) { Varyings output = (Varyings)0; PerTileProperty prop = _TileData[input.instanceID + _TileType * _TextureSizeExt.x * _TextureSizeExt.y]; uint vertexIndex = input.vertexID % 3; uint triangleID = input.vertexID / 3; uint uvX = ((vertexIndex\u0026amp;2)\u0026gt;\u0026gt;1)^triangleID; uint uvY = ((vertexIndex+vertexIndex\u0026gt;\u0026gt;1)\u0026amp;1)^triangleID; float2 uv = float2(uvX, uvY); float2 tileCoord = (prop.coord + 0.5f) * _TileSize; float3 center = float3(tileCoord.x, 0.0f, tileCoord.y) + _TileStartPos; float2 positionWSOffset = (uv - 0.5f) * _TileSize; float3 positionWS = center + float3(positionWSOffset.x, 0.0f, positionWSOffset.y); output.positionCS = mul(UNITY_MATRIX_VP, float4(positionWS, 1.0f)); output.uv = uv; output.positionWS = positionWS; output.mode = prop.mode; output.rotation = prop.rotation; output.channel = prop.channel; return output; } float4 frag(Varyings input) : SV_TARGET { float2 uv = input.uv; uv.y = 1.0f - uv.y; int2 startCoord = int2(input.mode % ONI_TILE_TEXTURE_SIZE, input.mode / ONI_TILE_TEXTURE_SIZE); float2 startUV = (float2)(startCoord) / ONI_TILE_TEXTURE_SIZE; float rotationVal = (float)(input.rotation) * PI * 0.5f; float sinVal, cosVal; sincos(rotationVal, sinVal, cosVal); float2x2 rotationMat = float2x2(cosVal, sinVal, -sinVal, cosVal); float2 rotatedCoord = saturate(mul(rotationMat, uv - 0.5f) + 0.5f); float2 sampleCoord = startUV + rotatedCoord / ONI_TILE_TEXTURE_SIZE; sampleCoord.y = 1.0f - sampleCoord.y; float4 tileTexture = tex2D(_TileTexture, sampleCoord); float visColor = tileTexture[input.channel]; float textureMask = smoothstep(0.25f, 1.0f, visColor); float gapMask = smoothstep(0.0f, 0.25f, visColor); float3 mainTex = tex2D(_MainTexture, input.positionWS.xz * _MainTexture_ST.xy + _MainTexture_ST.zw).rgb; mainTex = lerp(0.0f, mainTex, textureMask); float4 returnColor = float4(mainTex, gapMask); return returnColor; } ENDHLSL SubShader { pass { Tags {\u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34;} Blend SrcAlpha OneMinusSrcAlpha ZTest LEqual ZWrite Off HLSLPROGRAM #pragma vertex vert #pragma fragment frag ENDHLSL } } } 总结 总之就是很爽很快乐很有成就感。因为画图水平不行，有的地方的接缝差了一点点，也无所谓了。由于之前被HLSL里的Array坑了很多次，这里刻意地去限制了Array的数量，在加上之前玩了一会图灵完备，计算Procedural Quad的UV和顶点坐标简直不在话下，要是以前肯定就写一个长度为6的array进行采样了。对ByteAddressBuffer也有了新的理解，居然能用来计数，从而DrawIndirect或者DispatchIndirect，以前的话我只会CopyCounterValue。最后的最后也夸一下缺氧，画面确实很让人有深刻的印象，液体的渲染更是独树一帜，就是一个人玩太枯燥了。\n","permalink":"https://zznewclear13.github.io/posts/mimic-oxygen-not-includeds-tile-rendering/","summary":"缺氧的瓦片渲染的特点 很可惜我没有在RenderDoc里截到缺氧的帧，不过我还是能从渲染表现上来分析一下缺氧的瓦片渲染的特点。经过一段时间的游玩和从下面这张图中可以看到，缺氧的游戏逻辑是把整个2D的地图分成一个一个格子，每个格子记录了气体、液体、固体和建筑物的信息。气体只是一个扭曲的Shader，液体渲染和计算比较复杂，这里暂时不考虑，建筑物中的墙和管线虽然也有程序化生成再渲染的效果，但和场景中资源类型的固体格子是硬相接的关系，这里也不考虑。本文的研究重点放在资源类型的固体格子的渲染上（不包括这些格子的程序化生成）。\n资源类型的固体格子（这里就简称瓦片了）的特点如下：\n有多种类型的瓦片 仅在不同类型的瓦片相接时会有黑色的描边 瓦片之间会有排序，优先级高的瓦片会更多地扩张 瓦片之间黑色的描边呈现周期性规律 模仿这种渲染的思路 最简单的思路肯定就是在CPU中计算每一个瓦片应当有的形态，然后找到对应的贴图，把瓦片在GPU中绘制出来了。但是这样子做的话就失去了本文的意义，也太过无趣了。我想的是尽量多地用GPU来计算每个瓦片的形态，同时使用Instancing的方式，绘制每一个瓦片。\n第一个问题是，不规则的瓦片应当如何绘制。如果是正方形的瓦片，能够很轻易地使用一个Quad和纹理来绘制，但是不规则的瓦片，势必会使用透明度混合的方式来绘制，这时对应的模型就会超出瓦片的游戏逻辑上的位置。因此，我想的是绘制的Quad的数量是瓦片实际数量的两倍加一，如下图所示：\n在这张图中，ABC代表了不同类型的瓦片，左边是游戏游玩的时候逻辑上的瓦片分布，ABC是相接的，右边是在渲染的时候的瓦片的分布，在原有瓦片中间插入新的瓦片，专门用来渲染接缝。对于2号瓦片，其左上角右上角右下角左下角（顺时针的顺序）分别是ABCC，决定了这是一块三块相接的瓦片；对于1号瓦片，对应的编号是AACC（通过一些对2取模取余的运算可以排除掉B），决定了这是一块两块相接的瓦片；而对于3和4号瓦片，其编号为CCCC，决定了这两块是没有接缝的瓦片。这时我们又考虑到了瓦片之间优先级的关系，假设C\u0026gt;B\u0026gt;A，则AACC和AABB的接缝应当是相同的，ABCC和BCCA是旋转了九十度的关系。考虑到必定会有一个瓦片处于最低优先级，我们只需要将最低优先级的瓦片固定到左上角，讨论剩下三个瓦片的优先级与顺序即可。循着这个思路，我们可以把所有可能的接缝画在一张图上，这张图的RGBA通道记录了瓦片的优先级（R优先级最低，A优先级最高，接缝我使用了一个统一的灰色以便后续渲染），图片如下所示，为了比较容易观察，我对A通道做了反向，且对应的在下方标注了优先级顺序。同时我们还对应的写好一个函数用于根据优先级顺序找到对应的接缝类型从而在渲染时找到接缝在图上的位置（见ONITileRender.hlsl中的GetMode(uint a, uint b, uint c)）。\n由于会有优先级的比较，不可避免地会在GPU中进行排序，使用MergeSort的话，4个元素会有5次比较，由于我们还需要获得每个瓦片在四个瓦片中排序的序号，这里就硬写了手动比较，6次比较和MergeSort的5次也差不太多。我们绘制的图上仅有最低优先级瓦片在左上角的情况，因此我们还需要找到最低优先级瓦片初始的序号，从而在渲染时旋转我们的接缝图（这里就体现了我们使用顺时针编号的优势，方便了旋转的操作，如果是左上角右上角左下角右下角的顺序，就不太好旋转了）。\n知道了每一个接缝图的旋转，我们还需要为其每一个部分（通道）渲染不同的贴图。这里使用了DrawProceduralIndirect来进行Instancing的渲染，DrawCall数量会和瓦片类型的数量一样多。对于一种瓦片，需要渲染的总瓦片数相当于是这类瓦片的图形向外扩展一个瓦片的数量，我们可以通过判断左上右上右下左下的瓦片类型来轻易地判断当前瓦片是否应该和目标瓦片类型一起渲染。我们会使用一个数据数量为瓦片类型数量*(2*地图宽高+1)的StructuredBuffer来统计所有应当绘制的瓦片（实际使用的大小不会大于4*(2*地图宽+1)*(2*地图高+1)）。同时我们会使用一个数据数量为瓦片类型数量*5的ByteAddressBuffer来统计每种瓦片类型Instancing时需要的参数。\n本文中的岩石的2D无缝贴图来自OpenGameArt.org\n具体的代码和相关的解释 由于会用到CommandBuffer进行瓦片的绘制，我就把相关的代码放到Universal RP的Package里了。CPU代码，ONITileRenderManager.cs放在Packages/com.unity.render-pipelines.universal/Runtime/Overrides/下，ONITileRendererFeature.cs放在Packages/com.unity.render-pipelines.universal/Runtime/RendererFeature/下，ONITileRenderPass.cs放在Packages/com.unity.render-pipelines.universal/Runtime/Passes/下；GPU代码，ONITileRender.hlsl，ONITileComputeShader.compute和ONITileRenderShader.shader放在Packages/com.unity.render-pipelines.universal/Shaders/ONITile/下。\nONITileRenderManager用于地图的设置、计算和Buffer的获取。ONITileRendererFeature和ONITileRenderPass用于在Unity URP中渲染瓦片，ONITileComputeShader用于瓦片相关的计算，ONITileRenderShader用于瓦片的渲染。\nONITileRenderManager.cs 这里尤其需要注意每个Buffer的大小。在这个脚本里使用Compute Shader做了三件事：1. 对地图每一个点生成一个随机数作为瓦片类型；2. 从地图中计算每一种瓦片类型需要绘制的数量、位置、解封类型、旋转和应当采样的通道；3. 把ByteAddressBuffer中的数据复制到IndirectArgumentBuffer里。事实上我感觉ComputeShader.Dispatch应该做成一个异步的方法，不过这个调用频率不高，就这样好了。\nusing UnityEngine; [ExecuteInEditMode] public class ONITileRenderManager : MonoBehaviour { [HideInInspector] public static ONITileRenderManager Instance { get; private set; } public ComputeShader oniTileComputeShader; public int tileTypeCount = 4; public Vector2Int tileCount = new Vector2Int(16, 16); public Vector2 tileSize = Vector2.one; public Vector3 tileStartPos; public Vector2 randomSeed; public Texture[] mainTextures = new Texture[] {}; public Vector4 mainTextureST = new Vector4(1.","title":"模仿缺氧的瓦片渲染方法"},{"content":"环境光遮蔽 环境光遮蔽，在很久很久以前玩刺客信条的时候就看到过这个词语，但是并不懂什么意思，本着画质拉满的原则总是会勾选这个选项。后来才知道环境光遮蔽翻译自Ambient Occlusion（还真是直白的翻译），用来表现角落里阴暗的效果。\n环境光遮蔽作用在光线计算的间接光照的阶段，由于光栅化渲染的局限性，间接光照往往分为漫反射间接光照和高光间接光照，因此环境光遮蔽也分漫反射和高光两种，这里暂时只讨论作用于漫反射间接光照的漫反射环境光遮蔽。而又由于前向渲染的局限性，屏幕空间的环境光遮蔽不分差别地作用于直接光照和间接光照，因此其强度还需要特别地留意。\nGround Truth Ambient Occlusion是Jorge Jimenez在他的文章Practical Real-Time Strategies for Accurate Indirect Occlusion中介绍的一种在主机上能够符合事实环境光遮蔽效果的一种屏幕空间环境光遮蔽的算法。我认为这个算法相较于其他的环境光遮蔽的算法最大的优点是，暗部够暗，在很窄的缝隙中能够很黑很黑，这是别的算法做不到的。\n本文极大地参考了英特尔的XeGTAO开源代码。\n具体的操作 这篇文章着重要讲的是使用Compute Shader来加速计算的操作方式，因此不会具体涉及到GTAO算法本身，感兴趣的话可以去SIGGRAPH 2016 Course上阅读GTAO的ppt。\nGTAO的计算需要视空间法线和深度两个数据，如果是延迟管线的话能轻易得拿到所有数据，但对于前向渲染来说，需要从深度数据还原出视空间法线。正好我之前的文章介绍了一些从深度图计算视空间法线的方法。但在原有文章的基础上，我们还能使用Group Shared Memory对采样数进行一系列的优化。\n由于GTAO相对来说算是比较低频的信息，我们可以考虑使用下采样的方式只用半分辨率甚至是更低的分辨率来计算GTAO。这里使用的方法是对NxN大小的区域，每一帧只取一个采样点，最后通过TAA来进行混合。\nGTAO本身的采样数也能使用时空噪声来生成较少的采样点，最后通过TAA来进行混合。但是实际使用中发现，如果使用较多的时间混合，当场景中的物体发生移动之后，会露出一部分白色的画面，和较深的AO有比较明显的对比，因此考虑尽量多地使用空间的混合。\n得益于Group Shared Memory，可以在非常大的范围内进行空间的混合，这里使用高斯模糊的方式进行混合，能够尽量保持暗部较暗的颜色。如果直接对所有的采样进行平均的话，会导致暗部变得很亮，失去了GTAO最出众的优点。对水平和竖直方向做两次高斯模糊的话，由于本身还会根据深度和法线算出额外的几何上的权重，在下采样程度较大的时候会产生比较明显的瑕疵，可以用全分辨率的深度图和法线来解决，但这会带来额外的采样。\n在高斯模糊的阶段，由于模糊是作用在低分辨率的图像上的，在我们的上采样的操作中，还需要根据上采样的位置进行双线性插值（实际上只要一个方向线性插值就好了）。\nRender Texture的精度上，GTAO最后的值可以用8位通道来储存，如果不需要额外的视空间法线的话，可以把GTAO值和24位的深度一起存到RGBA32的RT中。这里就偷懒使用R16G16B16A16_SFloat来储存了。\n如此一来整个路线图就比较清晰了\n下采样深度图获取深度数据 使用深度图计算视空间的法线，或者从G Buffer直接获取法线数据 使用深度图和法线计算GTAO的值 横向上采样，计算水平高斯模糊后的GTAO的值 纵向上采样，计算垂直高斯模糊后的GTAO的值 相关代码和说明 GTAOComputeShader.compute 重中之重就是Compute Shader了。分了四个Kernel：第一个计算GTAO的值，同时还储存了深度图和法线（除了直接储存法线的两个分量，也可以Encode成八面体来储存）；第二个和第三个分别是水平和竖直方向的模糊；最后一个用来可视化，实际项目中可以不用这个。\n和XeGTAO不同的是，我增加了一个USE_AVERAGE_COS的宏，正常是在每一个Slice中选择最大的cos值，但是考虑到场景中有栅格这样的物体，在时空混合程度不是很大的时候，可以计算cos的平均值来降低栅格对GTAO的影响（也就是减弱了噪声），这个宏完全可以不用开启。\n本文为了尽量多的使用空间混合（亦即不使用时间混合），在XeGTAO的时空平均噪波中限制了时间的参数为13，这样GTAO就不会随着时间而变化了，实际上可以传入_FrameIndex充分利用时空噪波的优势。\n主要是用groupIndex来储存和读取Group Shared Memory，每个点至多采样两次。计算法线时会采样5x5的区域，因此NORMAL_FROM_DEPTH_PIXEL_RANGE的值是2；计算模糊时既有高斯模糊的采样，还有后续手动线性插值的采样，所以CACHED_AO_NORMAL_DEPTH_FOR_BLUR_SIZE会有两者之和。线性插值还需要注意subpixelBias对线性插值的权重产生的影响。\n本文使用了宽度为29的高斯核，可以在demofox的网站上轻松的计算很大的高斯核。\n可能会有报寄存器使用数量超过限制的问题，感觉是const array和循环导致的，不过reimport之后就不会报这个警告了。\n#pragma kernel GTAOMain #pragma kernel BlurHorizontalMain #pragma kernel BlurVerticalMain #pragma kernel VisualizeMain #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; Texture2D\u0026lt;float4\u0026gt; _ColorTexture; Texture2D\u0026lt;float\u0026gt; _DepthTexture; Texture2D\u0026lt;float4\u0026gt; _GTAOTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_NormalTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_GTAOTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_BlurTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_VisualizeTexture; SamplerState sampler_LinearClamp; SamplerState sampler_PointClamp; //region Parameters uint _FrameIndex; uint _DownsamplingFactor; float _Intensity; float _SampleRadius; float _DistributionPower; float _FalloffRange; float2 _HeightFogFalloff; float4 _TextureSize; float4 _TAAOffsets; //endregion //region Pre-defined Marcos #define SQRT2_2 0.70710678118 #define USE_AVERAGE_COS 0 #define SLICE_COUNT 4 #define STEPS_PER_SLICE 3 #define GTAO_THREAD_GROUP_SIZE_X 32 #define GTAO_THREAD_GROUP_SIZE_Y 32 #define BLUR_THREAD_GROUP_SIZE 64 // For normal calculation, can be deleted if we calculate normal in mrt. const static int NORMAL_FROM_DEPTH_PIXEL_RANGE = 2; const static int CACHED_DEPTH_FOR_NORMAL_OFFSET = (GTAO_THREAD_GROUP_SIZE_X + 2*NORMAL_FROM_DEPTH_PIXEL_RANGE); const static int CACHED_DEPTH_FOR_NORMAL_SIZE = CACHED_DEPTH_FOR_NORMAL_OFFSET * (GTAO_THREAD_GROUP_SIZE_Y + 2*NORMAL_FROM_DEPTH_PIXEL_RANGE); // For blur const static int BILINEAR_RADIUS = 1; const static int BLUR_RADIUS = 14; // [-14, +14] for 29 tap gaussian blur const static int CACHED_AO_NORMAL_DEPTH_FOR_BLUR_SIZE = BLUR_THREAD_GROUP_SIZE + 2*(BILINEAR_RADIUS+BLUR_RADIUS); const static int CACHED_AO_FOR_BILINEAR_SIZE = BLUR_THREAD_GROUP_SIZE+2*BILINEAR_RADIUS; //endregion //region Group Shared Memory Help Functions // For normal calculation groupshared float depthForNormal[CACHED_DEPTH_FOR_NORMAL_SIZE]; void SetDepthForNormal(float depth, int index) {depthForNormal[index]=depth;} float GetDepthForNormal(int2 threadPos) {return depthForNormal[threadPos.x+NORMAL_FROM_DEPTH_PIXEL_RANGE+(threadPos.y+NORMAL_FROM_DEPTH_PIXEL_RANGE)*CACHED_DEPTH_FOR_NORMAL_OFFSET];} void CacheDepthForNormal(int2 groupCacheStartPos, uint cacheIndex, int2 subpixelBias) { int2 threadPos = int2(cacheIndex % CACHED_DEPTH_FOR_NORMAL_OFFSET, cacheIndex / CACHED_DEPTH_FOR_NORMAL_OFFSET); int2 texturePos = (groupCacheStartPos + threadPos) * _DownsamplingFactor + subpixelBias; float depth = _DepthTexture.Load(uint3(texturePos, 0)); SetDepthForNormal(depth, cacheIndex); } groupshared float4 aoNormalDepthForBlur[CACHED_AO_NORMAL_DEPTH_FOR_BLUR_SIZE]; void SetAONormalDepthForBlur(float4 aoNormalDepth, int index) {aoNormalDepthForBlur[index]=aoNormalDepth;} float4 GetAONormalDepthForBlur(int threadPos) {return aoNormalDepthForBlur[threadPos+BLUR_RADIUS+BILINEAR_RADIUS];} void CacheAONormalDepthForBlur(int2 groupCacheStartPos, uint cacheIndex, uint vertical) { int2 threadPos = int2(0, 0); threadPos[vertical] = cacheIndex; int2 texturePos = groupCacheStartPos + threadPos; int2 threshold; if(vertical == 0) { threshold = _TextureSize.xy/_DownsamplingFactor - 1; } else { threshold = int2(_TextureSize.x, _TextureSize.y / _DownsamplingFactor) - 1; } texturePos = clamp(texturePos, 0, threshold); float4 aoNormalDepth = _GTAOTexture.Load(uint3(texturePos, 0)); SetAONormalDepthForBlur(aoNormalDepth, cacheIndex); } groupshared float aoForBilinear[CACHED_AO_FOR_BILINEAR_SIZE]; void SetAOForBilinear(float ao, int index) {aoForBilinear[index]=ao;} float GetAOForBilinear(int threadPos) {return aoForBilinear[threadPos.x+BILINEAR_RADIUS];} //endregion //region Space Transformation Help Functions float3 GetViewSpacePositionFromLinearDepth(float2 uv, float linearDepth) { #if UNITY_UV_STARTS_AT_TOP uv.y = 1.0 - uv.y; #endif float2 uvNDC = uv * 2.0 - 1.0; return float3(uvNDC * linearDepth * rcp(UNITY_MATRIX_P._m00_m11), -linearDepth); } float3 GetViewSpacePosition(float2 uv, float depth) { #if UNITY_MATRIX_I_P_SUPPORTED #if UNITY_UV_STARTS_AT_TOP uv.y = 1.0 - uv.y; #endif float3 positionNDC = float3(uv * 2.0 - 1.0, depth); float4 positionVS = mul(UNITY_MATRIX_I_P, float4(positionNDC, 1.0)); positionVS /= positionVS.w; return positionVS.xyz; #else float linearDepth = LinearEyeDepth(depth, _ZBufferParams); return GetViewSpacePositionFromLinearDepth(uv, linearDepth); #endif } float4 LinearEyeDepthFloat4(float4 depthTBLR, float4 zBufferParams) { return rcp(depthTBLR * zBufferParams.z + zBufferParams.w); } //endregion //region GTAO Help Functions //https://github.com/GameTechDev/XeGTAO/blob/master/Source/Rendering/Shaders/XeGTAO.h //-UNITY_MATRIX_P._m11 = rcp(tan(fovy / 2)) float GetScreenSpaceRadius(float linearDepth) { return _SampleRadius * _TextureSize.y * (-UNITY_MATRIX_P._m11) / (2 * linearDepth); } float GetLengthToPixelRatio(float linearDepth) { return _TextureSize.y * (-UNITY_MATRIX_P._m11) / (2 * linearDepth); } // From https://www.shadertoy.com/view/3tB3z3 - except we\u0026#39;re using R2 here #define XE_HILBERT_LEVEL 6U #define XE_HILBERT_WIDTH ( (1U \u0026lt;\u0026lt; XE_HILBERT_LEVEL) ) uint HilbertIndex( uint posX, uint posY ) { uint index = 0U; for( uint curLevel = XE_HILBERT_WIDTH/2U; curLevel \u0026gt; 0U; curLevel /= 2U ) { uint regionX = ( posX \u0026amp; curLevel ) \u0026gt; 0U; uint regionY = ( posY \u0026amp; curLevel ) \u0026gt; 0U; index += curLevel * curLevel * ( (3U * regionX) ^ regionY); if( regionY == 0U ) { if( regionX == 1U ) { posX = uint( (XE_HILBERT_WIDTH - 1U) ) - posX; posY = uint( (XE_HILBERT_WIDTH - 1U) ) - posY; } uint temp = posX; posX = posY; posY = temp; } } return index; } float2 SpatioTemporalNoise(uint2 pixCoord, uint temporalIndex) { uint index = HilbertIndex(pixCoord.x, pixCoord.y); index += 288*(temporalIndex%64); // why 288? tried out a few and that\u0026#39;s the best so far (with XE_HILBERT_LEVEL 6U) - but there\u0026#39;s probably better :) // R2 sequence - see http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/ return frac( 0.5 + index * float2(0.75487766624669276005, 0.5698402909980532659114)); } //endregion //region Blur Help Functions float GetWeight(float4 center, float4 samplePoint) { float4 centerAndSampleXY = float4(center.yz, samplePoint.yz) * 2.0f - 1.0f; float3 centerVS = float3(centerAndSampleXY.xy, sqrt(max(0.0f, 1.0f - dot(centerAndSampleXY.xy, centerAndSampleXY.xy)))); float3 sampleVS = float3(centerAndSampleXY.zw, sqrt(max(0.0f, 1.0f - dot(centerAndSampleXY.zw, centerAndSampleXY.zw)))); float normalWeight = saturate(dot(centerVS, sampleVS)); float depthWeight = 1.0f - saturate(abs(center.w - samplePoint.w) * 100.0f); return normalWeight * depthWeight; } void CacheGaussianBlur(uint cacheAOIndex, uint vertical) { int cacheAOThreadPos = cacheAOIndex - BILINEAR_RADIUS; float4 aoNormalDepthC = GetAONormalDepthForBlur(cacheAOThreadPos); float aoSum = 0.0f; float weightSum = 0.0f; float4 aoNormalDepth; float weight; // http://demofox.org/gauss.html const float weights[] = { 0.0002,\t0.0005,\t0.0011,\t0.0023,\t0.0044,\t0.0080,\t0.0136,\t0.0217,\t0.0325,\t0.0457,\t0.0605,\t0.0752,\t0.0879,\t0.0965,\t0.0995,\t0.0965,\t0.0879,\t0.0752,\t0.0605,\t0.0457,\t0.0325,\t0.0217,\t0.0136,\t0.0080,\t0.0044,\t0.0023,\t0.0011,\t0.0005,\t0.0002 }; [unroll(2*BLUR_RADIUS+1)] for (int i = -BLUR_RADIUS; i \u0026lt;= BLUR_RADIUS; ++i) { aoNormalDepth = GetAONormalDepthForBlur(cacheAOThreadPos + i); weight = GetWeight(aoNormalDepthC, aoNormalDepth) * weights[i + BLUR_RADIUS]; aoSum += aoNormalDepth.r * weight; weightSum += weight; } float avgAO = aoSum / weightSum; SetAOForBilinear(avgAO, cacheAOIndex); } //endregion [numthreads(GTAO_THREAD_GROUP_SIZE_X,GTAO_THREAD_GROUP_SIZE_Y,1)] void GTAOMain(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { uint sqrDownSamplingFactor = _DownsamplingFactor * _DownsamplingFactor; int subpixelIndex = _FrameIndex % sqrDownSamplingFactor; int2 subpixelBias = int2(subpixelIndex % _DownsamplingFactor, subpixelIndex / _DownsamplingFactor); int2 pixelCoord = dispatchThreadID.xy * _DownsamplingFactor + subpixelBias; float2 uv = (pixelCoord + 0.5) * _TextureSize.zw; int2 groupCacheStartPos = groupID.xy * int2(GTAO_THREAD_GROUP_SIZE_X, GTAO_THREAD_GROUP_SIZE_Y) - NORMAL_FROM_DEPTH_PIXEL_RANGE; //region Cache Normal int cacheIndex = groupIndex * 2; if(cacheIndex \u0026lt; CACHED_DEPTH_FOR_NORMAL_SIZE-1) { CacheDepthForNormal(groupCacheStartPos, cacheIndex, subpixelBias); CacheDepthForNormal(groupCacheStartPos, cacheIndex + 1, subpixelBias); } GroupMemoryBarrierWithGroupSync(); //endregion uint loadCacheIndex = groupIndex; int2 threadPos = int2(loadCacheIndex % GTAO_THREAD_GROUP_SIZE_X, loadCacheIndex / GTAO_THREAD_GROUP_SIZE_X); //region Calculate Normal From Depth float depthC = GetDepthForNormal(threadPos ); float depthT = GetDepthForNormal(threadPos + int2( 0, 1)); float depthB = GetDepthForNormal(threadPos + int2( 0, -1)); float depthL = GetDepthForNormal(threadPos + int2(-1, 0)); float depthR = GetDepthForNormal(threadPos + int2( 1, 0)); float depthT2 = GetDepthForNormal(threadPos + int2( 0, 2)); float depthB2 = GetDepthForNormal(threadPos + int2( 0, -2)); float depthL2 = GetDepthForNormal(threadPos + int2(-2, 0)); float depthR2 = GetDepthForNormal(threadPos + int2( 2, 0)); // fp16 depth should use this to prevent contour lines due to loss of depth precision. // linearDepth *= 0.99920; // This is for fp32 depth. // linearDepth *= 0.99999; float linearDepth = LinearEyeDepth(depthC, _ZBufferParams) * 0.99999; float4 linearDepths = LinearEyeDepthFloat4(float4(depthT, depthB, depthL, depthR), _ZBufferParams) * 0.99999; float2 center = pixelCoord + 0.5f; float3 viewPosC = GetViewSpacePositionFromLinearDepth(center * _TextureSize.zw, linearDepth); float3 viewPosT = GetViewSpacePositionFromLinearDepth((center + float2( 0, 1) * _DownsamplingFactor) * _TextureSize.zw, linearDepths.x); float3 viewPosB = GetViewSpacePositionFromLinearDepth((center + float2( 0, -1) * _DownsamplingFactor) * _TextureSize.zw, linearDepths.y); float3 viewPosL = GetViewSpacePositionFromLinearDepth((center + float2(-1, 0) * _DownsamplingFactor) * _TextureSize.zw, linearDepths.z); float3 viewPosR = GetViewSpacePositionFromLinearDepth((center + float2( 1, 0) * _DownsamplingFactor) * _TextureSize.zw, linearDepths.w); float3 t = normalize(viewPosT - viewPosC); float3 b = normalize(viewPosC - viewPosB); float3 l = normalize(viewPosC - viewPosL); float3 r = normalize(viewPosR - viewPosC); float4 H = float4(depthL, depthR, depthL2, depthR2); float4 V = float4(depthB, depthT, depthB2, depthT2); float2 he = abs((2 * H.xy - H.zw) - depthC); float2 ve = abs((2 * V.xy - V.zw) - depthC); float3 hDeriv = he.x \u0026lt; he.y ? l : r; float3 vDeriv = ve.x \u0026lt; ve.y ? b : t; float3 normalVS = normalize(cross(hDeriv, vDeriv)); //endregion //region Calculate GTAO From Depth and Normal float2 localNoise = SpatioTemporalNoise(dispatchThreadID.xy, 13); float3 pixCenterPos = viewPosC; float3 viewVec = normalize(-pixCenterPos); float effectRadius = _SampleRadius; float sampleDistributionPower = _DistributionPower; float falloffRange = effectRadius * _FalloffRange; float falloffFrom = effectRadius - falloffRange; float falloffMul = -rcp(falloffRange); float falloffAdd = 1.0 - falloffFrom * falloffMul; float visibility = 0.0; { float noiseSlice = localNoise.x; float noiseSample = localNoise.y; float pixelTooCloseThreshold = 1.3;//Some basic bias preventing sampling current pixel float lengthToPixelRatio = GetLengthToPixelRatio(linearDepth); float screenSpaceRadius = effectRadius * lengthToPixelRatio; //fade GTAO if screenSpaceRadius is too small visibility += saturate((10 - screenSpaceRadius) / 200); float minS = pixelTooCloseThreshold / screenSpaceRadius; //2 * SLICE_COUNT * STEPS_PER_SLICE samples //Almost exactly \u0026#34;Algorithm 1\u0026#34; in //https://www.activision.com/cdn/research/Practical_Real_Time_Strategies_for_Accurate_Indirect_Occlusion_NEW%20VERSION_COLOR.pdf [unroll(SLICE_COUNT)] for (int slice = 0; slice \u0026lt; SLICE_COUNT; slice++) { float phi = (slice + noiseSlice) * PI / SLICE_COUNT; float sinPhi, cosPhi; sincos(phi, sinPhi, cosPhi); float2 omega = float2(cosPhi, sinPhi); omega *= screenSpaceRadius; float3 directionVec = float3(cosPhi, sinPhi, 0.0); float3 orthoDirectionVec = directionVec - (dot(directionVec, viewVec) * viewVec); float3 axisVec = normalize(cross(orthoDirectionVec, viewVec)); float3 projectedNormalVec = normalVS - axisVec * dot(normalVS, axisVec); float signNormal = sign(dot(orthoDirectionVec, projectedNormalVec)); float projectedNormalVecLength = length(projectedNormalVec); float cosNorm = saturate(dot(projectedNormalVec, viewVec) / projectedNormalVecLength); float n = signNormal * acos(cosNorm); float lowHorizonCos0 = cos(n + HALF_PI); float lowHorizonCos1 = cos(n - HALF_PI); //Minor improvement float horizonCos0 = lowHorizonCos0; float horizonCos1 = lowHorizonCos1; #if USE_AVERAGE_COS float baseCos0 = 0.0; float baseCos1 = 0.0; #endif [unroll] for (float step = 0; step \u0026lt; STEPS_PER_SLICE; step++) { float stepBaseNoise = (slice + step * STEPS_PER_SLICE) * 0.6180339887498948482; float stepNoise = frac(noiseSample + stepBaseNoise); float s = (step + stepNoise) / STEPS_PER_SLICE; s = pow(s, sampleDistributionPower); s += minS; float2 sampleOffset = s * omega; //In pixel coord; float sampleOffsetLength = length(sampleOffset); sampleOffset = round(sampleOffset) * _TextureSize.zw; //To UV coord float2 sampleScreenPos0 = uv + sampleOffset; float sampleLinearDepth0 = LinearEyeDepth(_DepthTexture.SampleLevel(sampler_PointClamp, sampleScreenPos0, 0), _ZBufferParams); float3 samplePos0 = GetViewSpacePositionFromLinearDepth(sampleScreenPos0, sampleLinearDepth0); float2 sampleScreenPos1 = uv - sampleOffset; float sampleLinearDepth1 = LinearEyeDepth(_DepthTexture.SampleLevel(sampler_PointClamp, sampleScreenPos1, 0), _ZBufferParams); float3 samplePos1 = GetViewSpacePositionFromLinearDepth(sampleScreenPos1, sampleLinearDepth1); float3 sampleDelta0 = samplePos0 - pixCenterPos; float3 sampleDelta1 = samplePos1 - pixCenterPos; float sampleDist0 = length(sampleDelta0); float sampleDist1 = length(sampleDelta1); //Normalize float3 sampleHorizonVec0 = sampleDelta0 / sampleDist0; float3 sampleHorizonVec1 = sampleDelta1 / sampleDist1; float weight0 = saturate(sampleDist0 * falloffMul + falloffAdd); float weight1 = saturate(sampleDist1 * falloffMul + falloffAdd); //sample horizon cos float shc0 = dot(sampleHorizonVec0, viewVec); float shc1 = dot(sampleHorizonVec1, viewVec); shc0 = lerp(lowHorizonCos0, shc0, weight0); shc1 = lerp(lowHorizonCos1, shc1, weight1); #if USE_AVERAGE_COS baseCos0 += shc0; baseCos1 += shc1; #else horizonCos0 = max(horizonCos0, shc0); horizonCos1 = max(horizonCos1, shc1); #endif } #if USE_AVERAGE_COS baseCos0 /= STEPS_PER_SLICE; baseCos1 /= STEPS_PER_SLICE; horizonCos0 = max(baseCos0, horizonCos0); horizonCos1 = max(baseCos1, horizonCos1); #endif float h0 = acos(horizonCos0); float h1 = -acos(horizonCos1); h0 = n + clamp(h0 - n, -HALF_PI, HALF_PI); h1 = n + clamp(h1 - n, -HALF_PI, HALF_PI); float val0 = (cosNorm + 2 * h0 * sin(n) - cos(2 * h0 - n)) / 4; float val1 = (cosNorm + 2 * h1 * sin(n) - cos(2 * h1 - n)) / 4; visibility += projectedNormalVecLength * (val0 + val1); } visibility /= SLICE_COUNT; visibility = max(0.03, visibility); } float outputColor = visibility; #if USE_AVERAGE_COS outputColor /= HALF_PI; #endif //endregion _RW_GTAOTexture[dispatchThreadID.xy] = float4(outputColor, normalVS.xy * 0.5f + 0.5f, depthC); } [numthreads(BLUR_THREAD_GROUP_SIZE,1,1)] void BlurHorizontalMain(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { uint sqrDownSamplingFactor = _DownsamplingFactor * _DownsamplingFactor; int subpixelIndex = _FrameIndex % sqrDownSamplingFactor; int2 subpixelBias = int2(subpixelIndex % _DownsamplingFactor, subpixelIndex / _DownsamplingFactor); int2 pixelCoord = int2(dispatchThreadID.x / _DownsamplingFactor, dispatchThreadID.y); int2 thisSubpixelBias = int2(dispatchThreadID.x % _DownsamplingFactor, 0); //region Cache AO Normal Depth int2 groupCacheStartPos = groupID.xy * int2(BLUR_THREAD_GROUP_SIZE, 1) / int2(_DownsamplingFactor, 1) - int2(BLUR_RADIUS+BILINEAR_RADIUS, 0); int cacheIndex = groupIndex * 2; if(cacheIndex \u0026lt; int(CACHED_AO_NORMAL_DEPTH_FOR_BLUR_SIZE-1)) { CacheAONormalDepthForBlur(groupCacheStartPos, cacheIndex, 0); CacheAONormalDepthForBlur(groupCacheStartPos, cacheIndex + 1, 0); } GroupMemoryBarrierWithGroupSync(); //endregion float4 thisAONormalDepth = GetAONormalDepthForBlur(groupIndex / _DownsamplingFactor); //region Blur AO int cacheAOIndex = groupIndex * 2; if(cacheAOIndex \u0026lt; CACHED_AO_FOR_BILINEAR_SIZE-1) { CacheGaussianBlur(cacheAOIndex, 0); CacheGaussianBlur(cacheAOIndex + 1, 0); } GroupMemoryBarrierWithGroupSync(); //endregion //region Bilinear Sampling uint loadIndex = groupIndex; int threadPos = loadIndex / _DownsamplingFactor; float2 signVal = sign(thisSubpixelBias - subpixelBias); float thisAO = GetAOForBilinear(threadPos); float leftAO = GetAOForBilinear(threadPos + signVal.x); float2 lerpVal = abs(thisSubpixelBias - subpixelBias) / (float)_DownsamplingFactor; float finalAO = lerp(thisAO, leftAO, lerpVal.x); //endregion _RW_BlurTexture[dispatchThreadID.xy] = float4(finalAO, thisAONormalDepth.yzw); } [numthreads(1,BLUR_THREAD_GROUP_SIZE,1)] void BlurVerticalMain(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { uint sqrDownSamplingFactor = _DownsamplingFactor * _DownsamplingFactor; int subpixelIndex = _FrameIndex % sqrDownSamplingFactor; int2 subpixelBias = int2(subpixelIndex % _DownsamplingFactor, subpixelIndex / _DownsamplingFactor); int2 pixelCoord = dispatchThreadID.xy / _DownsamplingFactor; int2 thisSubpixelBias = dispatchThreadID.xy % _DownsamplingFactor; //region Cache AO Normal Depth int2 groupCacheStartPos = groupID.xy * int2(1, BLUR_THREAD_GROUP_SIZE) / int2(1, _DownsamplingFactor) - int2(0, BLUR_RADIUS+BILINEAR_RADIUS); int cacheIndex = groupIndex * 2; if(cacheIndex \u0026lt; int(CACHED_AO_NORMAL_DEPTH_FOR_BLUR_SIZE-1)) { CacheAONormalDepthForBlur(groupCacheStartPos, cacheIndex, 1); CacheAONormalDepthForBlur(groupCacheStartPos, cacheIndex + 1, 1); } GroupMemoryBarrierWithGroupSync(); //endregion //region Blur AO int cacheAOIndex = groupIndex * 2; if(cacheAOIndex \u0026lt; CACHED_AO_FOR_BILINEAR_SIZE-1) { CacheGaussianBlur(cacheAOIndex, 1); CacheGaussianBlur(cacheAOIndex + 1, 1); } GroupMemoryBarrierWithGroupSync(); //endregion //region Bilinear Sampling uint loadIndex = groupIndex; int threadPos = loadIndex / _DownsamplingFactor; float2 signVal = sign(thisSubpixelBias - subpixelBias); float thisAO = GetAOForBilinear(threadPos); float topAO = GetAOForBilinear(threadPos + signVal.y); float2 lerpVal = abs(thisSubpixelBias - subpixelBias) / (float)_DownsamplingFactor; float finalAO = lerp(thisAO, topAO, lerpVal.y); //endregion _RW_BlurTexture[dispatchThreadID.xy] = finalAO; } [numthreads(GTAO_THREAD_GROUP_SIZE_X,GTAO_THREAD_GROUP_SIZE_Y,1)] void VisualizeMain(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { float gtaoVal = _GTAOTexture.Load(uint3(dispatchThreadID.xy, 0)).r; float4 colorTexture = _ColorTexture.Load(uint3(dispatchThreadID.xy, 0)); float3 finalColor = colorTexture.rgb * lerp(1.0f, gtaoVal, _Intensity); _RW_VisualizeTexture[dispatchThreadID.xy] = float4(finalColor, 1.0f); } GroundTruthAmbientOcclusion.cs 用于控制GTAO的各种参数，没什么好说的。\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenu(\u0026#34;SSAO/GTAO\u0026#34;)] public class GroundTruthAmbientOcclusion : VolumeComponent, IPostProcessComponent { public ClampedIntParameter downsamplingFactor = new ClampedIntParameter(2, 1, 4); public ClampedFloatParameter intensity = new ClampedFloatParameter(0.0f, 0.0f, 1.0f); public ClampedFloatParameter radius = new ClampedFloatParameter(1.0f, 0.01f, 5.0f); public ClampedFloatParameter distributionPower = new ClampedFloatParameter(2.0f, 1.0f, 5.0f); public ClampedFloatParameter falloffRange = new ClampedFloatParameter(0.1f, 0.01f, 1.0f); public bool IsActive() { return active \u0026amp;\u0026amp; intensity.value \u0026gt; 0.0f; } public bool IsTileCompatible() { return false; } } } GTAORendererFeature.cs 也没啥好说的，很普通的RendererFeature。\nnamespace UnityEngine.Rendering.Universal { public class GTAORendererFeature : ScriptableRendererFeature { [System.Serializable] public class GTAOSettings { public bool isEnabled; public RenderPassEvent renderPassEvent = RenderPassEvent.AfterRenderingOpaques; public ComputeShader gtaoComputeShader; } public GTAOSettings settings = new GTAOSettings(); private GTAORenderPass gtaoRenderPass; public override void Create() { gtaoRenderPass = new GTAORenderPass(settings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { GroundTruthAmbientOcclusion gtao = VolumeManager.instance.stack.GetComponent\u0026lt;GroundTruthAmbientOcclusion\u0026gt;(); if (gtao != null \u0026amp;\u0026amp; gtao.IsActive()) { gtaoRenderPass.Setup(gtao); renderer.EnqueuePass(gtaoRenderPass); } } } } GTAORenderPass.cs 主要值得注意的是图像分辨率的大小，获取半分辨率的大小时，要记得使用Ceil来获取更大的图片。然后每一个阶段使用的Dispatch数目也不尽相同，主要是上采样的阶段花样比较多。\nnamespace UnityEngine.Rendering.Universal { public class GTAORenderPass : ScriptableRenderPass { private const string profilerTag = \u0026#34;Ground Truth Ambient Occlusion\u0026#34;; private const string gtaoKernelName = \u0026#34;GTAOMain\u0026#34;; private const string blurHorizontalKernelName = \u0026#34;BlurHorizontalMain\u0026#34;; private const string blurVerticalKernelName = \u0026#34;BlurVerticalMain\u0026#34;; private const string visualizeKernelName = \u0026#34;VisualizeMain\u0026#34;; private ProfilingSampler profilingSampler; private ProfilingSampler gtaoSampler = new ProfilingSampler(\u0026#34;GTAO Pass\u0026#34;); private ProfilingSampler blurSampler = new ProfilingSampler(\u0026#34;Blur Pass\u0026#34;); private ProfilingSampler visualizeSampler = new ProfilingSampler(\u0026#34;Visualize Pass\u0026#34;); private RenderTargetHandle cameraColor; private RenderTargetIdentifier cameraColorIden; private RenderTargetHandle cameraDepth; private RenderTargetIdentifier cameraDepthIden; private RenderTargetHandle cameraDepthAttachment; private RenderTargetIdentifier cameraDepthAttachmentIden; private static readonly string gtaoTextureName = \u0026#34;_GTAOBuffer\u0026#34;; private static readonly int gtaoTextureID = Shader.PropertyToID(gtaoTextureName); private RenderTargetHandle gtaoTextureHandle; private RenderTargetIdentifier gtaoTextureIden; private static readonly string horizontalBlurTextureName = \u0026#34;_HorizontalBlurBuffer\u0026#34;; private static readonly int horizontalBlurTextureID = Shader.PropertyToID(horizontalBlurTextureName); private RenderTargetHandle horizontalBlurTextureHandle; private RenderTargetIdentifier horizontalBlurTextureIden; private static readonly string vericalBlurTextureName = \u0026#34;_VerticalBlurBuffer\u0026#34;; private static readonly int vericalBlurTextureID = Shader.PropertyToID(vericalBlurTextureName); private RenderTargetHandle vericalBlurTextureHandle; private RenderTargetIdentifier vericalBlurTextureIden; private static readonly string visualizeTextureName = \u0026#34;_VisualizeBuffer\u0026#34;; private static readonly int visualizeTextureID = Shader.PropertyToID(visualizeTextureName); private RenderTargetHandle visualizeTextureHandle; private RenderTargetIdentifier visualizeTextureIden; private GroundTruthAmbientOcclusion groundTruthAmbientOcclusion; private ComputeShader gtaoComputeShader; private GTAORendererFeature.GTAOSettings settings; private int downsamplingFactor; private Vector2Int fullRes; private Vector2Int downsampleRes; private int frameIndex; static readonly int _GTAOFrameIndexID = Shader.PropertyToID(\u0026#34;_FrameIndex\u0026#34;); static readonly int _GTAODownsamplingFactorID = Shader.PropertyToID(\u0026#34;_DownsamplingFactor\u0026#34;); static readonly int _GTAOIntensityID = Shader.PropertyToID(\u0026#34;_Intensity\u0026#34;); static readonly int _GTAOSampleRadiusID = Shader.PropertyToID(\u0026#34;_SampleRadius\u0026#34;); static readonly int _GTAODistributionPowerID = Shader.PropertyToID(\u0026#34;_DistributionPower\u0026#34;); static readonly int _GTAOFalloffRangeID = Shader.PropertyToID(\u0026#34;_FalloffRange\u0026#34;); static readonly int _GTAOTextureSizeID = Shader.PropertyToID(\u0026#34;_TextureSize\u0026#34;); static readonly int _GTAOColorTextureID = Shader.PropertyToID(\u0026#34;_ColorTexture\u0026#34;); static readonly int _GTAODepthTextureID = Shader.PropertyToID(\u0026#34;_DepthTexture\u0026#34;); static readonly int _GTAOTextureID = Shader.PropertyToID(\u0026#34;_GTAOTexture\u0026#34;); static readonly int _GTAORWTextureID = Shader.PropertyToID(\u0026#34;_RW_GTAOTexture\u0026#34;); static readonly int _GTAORWBlurTextureID = Shader.PropertyToID(\u0026#34;_RW_BlurTexture\u0026#34;); static readonly int _GTAORWVisualizeTextureID = Shader.PropertyToID(\u0026#34;_RW_VisualizeTexture\u0026#34;); public GTAORenderPass(GTAORendererFeature.GTAOSettings settings) { this.settings = settings; profilingSampler = new ProfilingSampler(profilerTag); renderPassEvent = settings.renderPassEvent; gtaoComputeShader = settings.gtaoComputeShader; cameraColor.Init(\u0026#34;_CameraColorTexture\u0026#34;); cameraColorIden = cameraColor.Identifier(); cameraDepth.Init(\u0026#34;_CameraDepthTexture\u0026#34;); cameraDepthIden = cameraDepth.Identifier(); cameraDepthAttachment.Init(\u0026#34;_CameraDepthAttachment\u0026#34;); cameraDepthAttachmentIden = cameraDepthAttachment.Identifier(); gtaoTextureHandle.Init(gtaoTextureName); gtaoTextureIden = gtaoTextureHandle.Identifier(); horizontalBlurTextureHandle.Init(horizontalBlurTextureName); horizontalBlurTextureIden = horizontalBlurTextureHandle.Identifier(); vericalBlurTextureHandle.Init(vericalBlurTextureName); vericalBlurTextureIden = vericalBlurTextureHandle.Identifier(); visualizeTextureHandle.Init(visualizeTextureName); visualizeTextureIden = visualizeTextureHandle.Identifier(); frameIndex = 0; } public void Setup(GroundTruthAmbientOcclusion groundTruthAmbientOcclusion) { this.groundTruthAmbientOcclusion = groundTruthAmbientOcclusion; } public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor) { RenderTextureDescriptor desc = cameraTextureDescriptor; desc.enableRandomWrite = true; desc.depthBufferBits = 0; desc.msaaSamples = 1; desc.graphicsFormat = Experimental.Rendering.GraphicsFormat.R16G16B16A16_SFloat; downsamplingFactor = groundTruthAmbientOcclusion.downsamplingFactor.value; fullRes = new Vector2Int(desc.width, desc.height); downsampleRes = new Vector2Int(Mathf.CeilToInt((float)desc.width / downsamplingFactor), Mathf.CeilToInt((float)desc.height / downsamplingFactor)); cmd.GetTemporaryRT(visualizeTextureID, desc); cmd.GetTemporaryRT(vericalBlurTextureID, desc); desc.height = downsampleRes.y; cmd.GetTemporaryRT(horizontalBlurTextureID, desc); desc.width = downsampleRes.x; cmd.GetTemporaryRT(gtaoTextureID, desc); } private void DoGTAOCalculation(CommandBuffer cmd, RenderTargetIdentifier depthid, RenderTargetIdentifier gtaoid, ComputeShader computeShader) { if (!computeShader.HasKernel(gtaoKernelName)) return; int gtaoKernel = computeShader.FindKernel(gtaoKernelName); computeShader.GetKernelThreadGroupSizes(gtaoKernel, out uint x, out uint y, out uint z); cmd.SetComputeIntParam(computeShader, _GTAOFrameIndexID, frameIndex); cmd.SetComputeIntParam(computeShader, _GTAODownsamplingFactorID, downsamplingFactor); cmd.SetComputeVectorParam(computeShader, _GTAOTextureSizeID, new Vector4(fullRes.x, fullRes.y, 1.0f / fullRes.x, 1.0f / fullRes.y)); cmd.SetComputeFloatParam(computeShader, _GTAOSampleRadiusID, groundTruthAmbientOcclusion.radius.value); cmd.SetComputeFloatParam(computeShader, _GTAODistributionPowerID, groundTruthAmbientOcclusion.distributionPower.value); cmd.SetComputeFloatParam(computeShader, _GTAOFalloffRangeID, groundTruthAmbientOcclusion.falloffRange.value); cmd.SetComputeTextureParam(computeShader, gtaoKernel, _GTAODepthTextureID, depthid); cmd.SetComputeTextureParam(computeShader, gtaoKernel, _GTAORWTextureID, gtaoid); cmd.DispatchCompute(computeShader, gtaoKernel, Mathf.CeilToInt((float)downsampleRes.x / x), Mathf.CeilToInt((float)downsampleRes.y / y), 1); } private void DoBlur(CommandBuffer cmd, RenderTargetIdentifier gtaoid, RenderTargetIdentifier horizontalid, RenderTargetIdentifier verticalid, ComputeShader computeShader) { if (!computeShader.HasKernel(blurHorizontalKernelName) || !computeShader.HasKernel(blurVerticalKernelName)) return; int horizontalKernel = computeShader.FindKernel(blurHorizontalKernelName); int verticalKernel = computeShader.FindKernel(blurVerticalKernelName); uint x, y, z; computeShader.GetKernelThreadGroupSizes(horizontalKernel, out x, out y, out z); cmd.SetComputeTextureParam(computeShader, horizontalKernel, _GTAOTextureID, gtaoid); cmd.SetComputeTextureParam(computeShader, horizontalKernel, _GTAORWBlurTextureID, horizontalid); cmd.DispatchCompute(computeShader, horizontalKernel, Mathf.CeilToInt((float)fullRes.x / x), Mathf.CeilToInt((float)downsampleRes.y / y), 1); computeShader.GetKernelThreadGroupSizes(verticalKernel, out x, out y, out z); cmd.SetComputeTextureParam(computeShader, verticalKernel, _GTAOTextureID, horizontalid); cmd.SetComputeTextureParam(computeShader, verticalKernel, _GTAORWBlurTextureID, verticalid); cmd.DispatchCompute(computeShader, verticalKernel, Mathf.CeilToInt((float)fullRes.x / x), Mathf.CeilToInt((float)fullRes.y / y), 1); } private void DoVisualization(CommandBuffer cmd, RenderTargetIdentifier colorid, RenderTargetIdentifier verticalid, RenderTargetIdentifier visualizeid, ComputeShader computeShader) { if (!computeShader.HasKernel(visualizeKernelName)) return; int visualzieKernel = computeShader.FindKernel(visualizeKernelName); cmd.SetComputeFloatParam(computeShader, _GTAOIntensityID, groundTruthAmbientOcclusion.intensity.value); computeShader.GetKernelThreadGroupSizes(visualzieKernel, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, visualzieKernel, _GTAOColorTextureID, colorid); cmd.SetComputeTextureParam(computeShader, visualzieKernel, _GTAOTextureID, verticalid); cmd.SetComputeTextureParam(computeShader, visualzieKernel, _GTAORWVisualizeTextureID, visualizeid); cmd.DispatchCompute(computeShader, visualzieKernel, Mathf.CeilToInt((float)fullRes.x / x), Mathf.CeilToInt((float)fullRes.y / y), 1); cmd.Blit(visualizeid, colorid); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(profilerTag); context.ExecuteCommandBuffer(cmd); cmd.Clear(); using (new ProfilingScope(cmd, profilingSampler)) { using (new ProfilingScope(cmd, gtaoSampler)) { if(renderingData.cameraData.isSceneViewCamera) { DoGTAOCalculation(cmd, cameraDepthIden, gtaoTextureIden, gtaoComputeShader); } else { DoGTAOCalculation(cmd, cameraDepthAttachmentIden, gtaoTextureIden, gtaoComputeShader); } } using (new ProfilingScope(cmd, blurSampler)) { DoBlur(cmd, gtaoTextureIden, horizontalBlurTextureIden, vericalBlurTextureIden, gtaoComputeShader); } using (new ProfilingScope(cmd, visualizeSampler)) { DoVisualization(cmd, cameraColorIden, vericalBlurTextureIden, visualizeTextureIden, gtaoComputeShader); } } frameIndex=(++frameIndex)%60; context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } public override void FrameCleanup(CommandBuffer cmd) { cmd.ReleaseTemporaryRT(gtaoTextureID); cmd.ReleaseTemporaryRT(horizontalBlurTextureID); cmd.ReleaseTemporaryRT(vericalBlurTextureID); cmd.ReleaseTemporaryRT(visualizeTextureID); } } } 后记 又隔了很久，总算逼着自己把这篇文章写完了，也逼着自己强行用Group Shared Memory来做各种采样的优化。写出来的代码果然很吓人也应该没人能看得懂吧（当然更可能是没人会看）。\n","permalink":"https://zznewclear13.github.io/posts/unity-ground-truth-ambient-occlusion/","summary":"环境光遮蔽 环境光遮蔽，在很久很久以前玩刺客信条的时候就看到过这个词语，但是并不懂什么意思，本着画质拉满的原则总是会勾选这个选项。后来才知道环境光遮蔽翻译自Ambient Occlusion（还真是直白的翻译），用来表现角落里阴暗的效果。\n环境光遮蔽作用在光线计算的间接光照的阶段，由于光栅化渲染的局限性，间接光照往往分为漫反射间接光照和高光间接光照，因此环境光遮蔽也分漫反射和高光两种，这里暂时只讨论作用于漫反射间接光照的漫反射环境光遮蔽。而又由于前向渲染的局限性，屏幕空间的环境光遮蔽不分差别地作用于直接光照和间接光照，因此其强度还需要特别地留意。\nGround Truth Ambient Occlusion是Jorge Jimenez在他的文章Practical Real-Time Strategies for Accurate Indirect Occlusion中介绍的一种在主机上能够符合事实环境光遮蔽效果的一种屏幕空间环境光遮蔽的算法。我认为这个算法相较于其他的环境光遮蔽的算法最大的优点是，暗部够暗，在很窄的缝隙中能够很黑很黑，这是别的算法做不到的。\n本文极大地参考了英特尔的XeGTAO开源代码。\n具体的操作 这篇文章着重要讲的是使用Compute Shader来加速计算的操作方式，因此不会具体涉及到GTAO算法本身，感兴趣的话可以去SIGGRAPH 2016 Course上阅读GTAO的ppt。\nGTAO的计算需要视空间法线和深度两个数据，如果是延迟管线的话能轻易得拿到所有数据，但对于前向渲染来说，需要从深度数据还原出视空间法线。正好我之前的文章介绍了一些从深度图计算视空间法线的方法。但在原有文章的基础上，我们还能使用Group Shared Memory对采样数进行一系列的优化。\n由于GTAO相对来说算是比较低频的信息，我们可以考虑使用下采样的方式只用半分辨率甚至是更低的分辨率来计算GTAO。这里使用的方法是对NxN大小的区域，每一帧只取一个采样点，最后通过TAA来进行混合。\nGTAO本身的采样数也能使用时空噪声来生成较少的采样点，最后通过TAA来进行混合。但是实际使用中发现，如果使用较多的时间混合，当场景中的物体发生移动之后，会露出一部分白色的画面，和较深的AO有比较明显的对比，因此考虑尽量多地使用空间的混合。\n得益于Group Shared Memory，可以在非常大的范围内进行空间的混合，这里使用高斯模糊的方式进行混合，能够尽量保持暗部较暗的颜色。如果直接对所有的采样进行平均的话，会导致暗部变得很亮，失去了GTAO最出众的优点。对水平和竖直方向做两次高斯模糊的话，由于本身还会根据深度和法线算出额外的几何上的权重，在下采样程度较大的时候会产生比较明显的瑕疵，可以用全分辨率的深度图和法线来解决，但这会带来额外的采样。\n在高斯模糊的阶段，由于模糊是作用在低分辨率的图像上的，在我们的上采样的操作中，还需要根据上采样的位置进行双线性插值（实际上只要一个方向线性插值就好了）。\nRender Texture的精度上，GTAO最后的值可以用8位通道来储存，如果不需要额外的视空间法线的话，可以把GTAO值和24位的深度一起存到RGBA32的RT中。这里就偷懒使用R16G16B16A16_SFloat来储存了。\n如此一来整个路线图就比较清晰了\n下采样深度图获取深度数据 使用深度图计算视空间的法线，或者从G Buffer直接获取法线数据 使用深度图和法线计算GTAO的值 横向上采样，计算水平高斯模糊后的GTAO的值 纵向上采样，计算垂直高斯模糊后的GTAO的值 相关代码和说明 GTAOComputeShader.compute 重中之重就是Compute Shader了。分了四个Kernel：第一个计算GTAO的值，同时还储存了深度图和法线（除了直接储存法线的两个分量，也可以Encode成八面体来储存）；第二个和第三个分别是水平和竖直方向的模糊；最后一个用来可视化，实际项目中可以不用这个。\n和XeGTAO不同的是，我增加了一个USE_AVERAGE_COS的宏，正常是在每一个Slice中选择最大的cos值，但是考虑到场景中有栅格这样的物体，在时空混合程度不是很大的时候，可以计算cos的平均值来降低栅格对GTAO的影响（也就是减弱了噪声），这个宏完全可以不用开启。\n本文为了尽量多的使用空间混合（亦即不使用时间混合），在XeGTAO的时空平均噪波中限制了时间的参数为13，这样GTAO就不会随着时间而变化了，实际上可以传入_FrameIndex充分利用时空噪波的优势。\n主要是用groupIndex来储存和读取Group Shared Memory，每个点至多采样两次。计算法线时会采样5x5的区域，因此NORMAL_FROM_DEPTH_PIXEL_RANGE的值是2；计算模糊时既有高斯模糊的采样，还有后续手动线性插值的采样，所以CACHED_AO_NORMAL_DEPTH_FOR_BLUR_SIZE会有两者之和。线性插值还需要注意subpixelBias对线性插值的权重产生的影响。\n本文使用了宽度为29的高斯核，可以在demofox的网站上轻松的计算很大的高斯核。\n可能会有报寄存器使用数量超过限制的问题，感觉是const array和循环导致的，不过reimport之后就不会报这个警告了。\n#pragma kernel GTAOMain #pragma kernel BlurHorizontalMain #pragma kernel BlurVerticalMain #pragma kernel VisualizeMain #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; Texture2D\u0026lt;float4\u0026gt; _ColorTexture; Texture2D\u0026lt;float\u0026gt; _DepthTexture; Texture2D\u0026lt;float4\u0026gt; _GTAOTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_NormalTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_GTAOTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_BlurTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_VisualizeTexture; SamplerState sampler_LinearClamp; SamplerState sampler_PointClamp; //region Parameters uint _FrameIndex; uint _DownsamplingFactor; float _Intensity; float _SampleRadius; float _DistributionPower; float _FalloffRange; float2 _HeightFogFalloff; float4 _TextureSize; float4 _TAAOffsets; //endregion //region Pre-defined Marcos #define SQRT2_2 0.","title":"Unity使用ComputeShader计算GTAO"},{"content":"为什么要用Ray Marching 要不还是别用Ray Marching了（除非是SDF Ray Marching），采样次数又多又不好debug，不过写起来比较快（如果要写二分法的话就又复杂了）。如前文所说，使用Ray Marching的体积雾只能在后处理阶段使用了，在处理不写深度的透明物体的时候，会有一些瑕疵。\n体积雾相关的就参考前文就好了，这里只是作为一个方法的补充。\n相关代码和说明 为了和使用3D纹理的体积雾作区分，这边所有代码的名字前加上了RM(Ray Marching)。\nRMVolumetricFog.cs 这个脚本和3D纹理的体积雾的参数几乎完全一致，只是多了用于控制Ray Marching次数的step。\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenu(\u0026#34;Post-processing/RM Volumetric Fog\u0026#34;)] public class RMVolumetricFog : VolumeComponent, IPostProcessComponent { [Tooltip(\u0026#34;是否启用体积雾\u0026#34;)] public BoolParameter enabled = new BoolParameter(false); [Tooltip(\u0026#34;整体控制体积雾强度\u0026#34;)] public ClampedFloatParameter intensity = new ClampedFloatParameter(1.0f, 0f, 1.0f); [Tooltip(\u0026#34;体积雾最大的透明程度（用于和天空混合）\u0026#34;)] public ClampedFloatParameter maxTransmittance = new ClampedFloatParameter(1.0f, 0f, 1.0f); [Tooltip(\u0026#34;体积雾的颜色倾向，目前强度为0.03\u0026#34;)] public ColorParameter fogTint = new ColorParameter(Color.white); [Tooltip(\u0026#34;体积雾距离相机最近的距离\u0026#34;)] public ClampedFloatParameter fogNear = new ClampedFloatParameter(0.1f, 0.01f, 10f); [Tooltip(\u0026#34;体积雾距离相机最远的距离\u0026#34;)] public ClampedFloatParameter fogFar = new ClampedFloatParameter(100f, 1.0f, 1000.0f); [Tooltip(\u0026#34;体积雾的密度，越密效果越明显\u0026#34;)] public ClampedFloatParameter density = new ClampedFloatParameter(3.0f, 0f, 10.0f); [Tooltip(\u0026#34;体积雾受光的各向异性程度\u0026#34;)] public ClampedFloatParameter phase = new ClampedFloatParameter(0.0f, -0.9f, 0.9f); [Tooltip(\u0026#34;Ray Marching的次数\u0026#34;)] public ClampedFloatParameter step = new ClampedFloatParameter(20.0f, 10.0f, 200.0f); public bool IsActive() =\u0026gt; (enabled.value \u0026amp;\u0026amp; (density.value \u0026gt; 0.0f) \u0026amp;\u0026amp; (intensity.value \u0026gt; 0.0f)); public bool IsTileCompatible() =\u0026gt; false; } } RMVolumetricRendererFeature.cs 和3D纹理的体积雾除了命名之外完全一致。\nnamespace UnityEngine.Rendering.Universal { public class RMVolumetricFogRendererFeature : ScriptableRendererFeature { [System.Serializable] public class RMVolumetricFogSetting { public RenderPassEvent renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; public ComputeShader volumetricFogComputeShader; } public RMVolumetricFogRenderPass volumetricFogRenderPass; public RMVolumetricFogSetting settings = new RMVolumetricFogSetting(); public override void Create() { volumetricFogRenderPass = new RMVolumetricFogRenderPass(settings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { RMVolumetricFog volumetricFog = VolumeManager.instance.stack.GetComponent\u0026lt;RMVolumetricFog\u0026gt;(); if(volumetricFog \u0026amp;\u0026amp; volumetricFog.IsActive()) { if(renderingData.cameraData.cameraType == CameraType.Game) { volumetricFogRenderPass.Setup(volumetricFog); renderer.EnqueuePass(volumetricFogRenderPass); } } } } } RMVolumetricFogRenderPass.cs 相较于3D纹理的体积雾来说，Ray Marching的体积雾只需要计算体积雾，将体积雾应用到最后相机的Render Texture就好了。不需要获取很多临时的Render Texture，但是没办法在其中进行时空的混合了，只能寄希望于后续的TAA，或者是直接增加Ray Marching的次数了。\nnamespace UnityEngine.Rendering.Universal { public class RMVolumetricFogRenderPass : ScriptableRenderPass { private const string profilerTag = \u0026#34;Ray Marched Volumetric Fog Rendering Pass\u0026#34;; private RMVolumetricFogRendererFeature.RMVolumetricFogSetting settings; private ProfilingSampler profilingSampler; RenderTargetIdentifier cameraColorIden; RenderTargetHandle cameraColorHandle; RenderTargetIdentifier cameraDepthIden; RenderTargetHandle cameraDepthHandle; static string volumetricFogName = \u0026#34;_VolumetrixFogBuffer\u0026#34;; static int volumetricFogID = Shader.PropertyToID(volumetricFogName); RenderTargetIdentifier volumetricFogIden; RenderTargetHandle volumetricFogHandle; private RMVolumetricFog volumetricFog; private ComputeShader volumetricFogComputeShader; private Vector2 textureSize; public RMVolumetricFogRenderPass(RMVolumetricFogRendererFeature.RMVolumetricFogSetting settings) { this.settings = settings; profilingSampler = new ProfilingSampler(profilerTag); renderPassEvent = settings.renderPassEvent; volumetricFogComputeShader = settings.volumetricFogComputeShader; cameraColorHandle.Init(\u0026#34;_CameraColorTexture\u0026#34;); cameraColorIden = cameraColorHandle.Identifier(); cameraDepthHandle.Init(\u0026#34;_CameraDepthAttachment\u0026#34;); cameraDepthIden = cameraDepthHandle.Identifier(); volumetricFogHandle.Init(volumetricFogName); volumetricFogIden = volumetricFogHandle.Identifier(); } public void Setup(RMVolumetricFog volumetricFog) { this.volumetricFog = volumetricFog; } public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor) { RenderTextureDescriptor desc = cameraTextureDescriptor; desc.enableRandomWrite = true; desc.graphicsFormat = Experimental.Rendering.GraphicsFormat.R16G16B16A16_SFloat; textureSize = new Vector2(desc.width, desc.height); cmd.GetTemporaryRT(volumetricFogID, desc); } private void DoVolumetricFog(CommandBuffer cmd, Camera camera, RenderTargetIdentifier colorid, RenderTargetIdentifier depthid, RenderTargetIdentifier volid, ComputeShader computeShader) { int volumetricFogKernel = computeShader.FindKernel(\u0026#34;VolumetricFogMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(volumetricFogKernel, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, volumetricFogKernel, \u0026#34;_ColorTexture\u0026#34;, colorid); cmd.SetComputeTextureParam(computeShader, volumetricFogKernel, \u0026#34;_DepthTexture\u0026#34;, depthid); cmd.SetComputeTextureParam(computeShader, volumetricFogKernel, \u0026#34;_RW_VolTexture\u0026#34;, volid); cmd.SetComputeFloatParam(computeShader, \u0026#34;_StepCount\u0026#34;, volumetricFog.step.value); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TextureSize\u0026#34;, new Vector4(textureSize.x, textureSize.y, 1.0f / textureSize.x, 1.0f / textureSize.y)); Color fogTint = volumetricFog.fogTint.value; fogTint.a = 0.03f; volumetricFog.fogTint.Override(fogTint); cmd.SetComputeVectorParam(computeShader, \u0026#34;_FogTint\u0026#34;, volumetricFog.fogTint.value); cmd.SetComputeVectorParam(computeShader, \u0026#34;_NearFar\u0026#34;, new Vector4(camera.nearClipPlane, camera.farClipPlane, volumetricFog.fogNear.value, volumetricFog.fogFar.value)); cmd.SetComputeVectorParam(computeShader, \u0026#34;_VolumetricFogParams\u0026#34;, new Vector4(volumetricFog.phase.value, volumetricFog.density.value, volumetricFog.intensity.value, volumetricFog.maxTransmittance.value)); cmd.DispatchCompute(computeShader, volumetricFogKernel, Mathf.CeilToInt(textureSize.x / x), Mathf.CeilToInt(textureSize.y / y), 1); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(profilerTag); context.ExecuteCommandBuffer(cmd); cmd.Clear(); using (new ProfilingScope(cmd, profilingSampler)) { DoVolumetricFog(cmd, renderingData.cameraData.camera, cameraColorIden, cameraDepthIden, volumetricFogIden, volumetricFogComputeShader); cmd.Blit(volumetricFogIden, cameraColorIden); } context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } public override void FrameCleanup(CommandBuffer cmd) { cmd.ReleaseTemporaryRT(volumetricFogID); } } } RMVolumetricFogComputeShader.compute 只需要一个kernel就能完成体积雾的计算了，和之前使用3D纹理的体积雾进行比较，可以很明显的看到Ray Marching在一个kernel中完成了3D纹理三个kernel的工作。具体的流程甚至函数都是完全相同的。\n#pragma kernel VolumetricFogMain #define _MAIN_LIGHT_SHADOWS #define _MAIN_LIGHT_SHADOWS_CASCADE #define _SHADOWS_SOFT #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; Texture2D\u0026lt;float4\u0026gt; _ColorTexture; Texture2D\u0026lt;float\u0026gt; _DepthTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_VolTexture; float4 _TAAOffsets; float4 _TextureSize; float _StepCount; float4 _NearFar; // x: cam near, y: cam far, z: fog near, w: fog far float4 _FogTint; float4 _VolumetricFogParams; #define _Phase _VolumetricFogParams.x #define _Density _VolumetricFogParams.y #define _Intensity _VolumetricFogParams.z #define _MaxTransmittance _VolumetricFogParams.w float3 NDCToWorld(float3 ndc) { ndc.xy = 2.0f * ndc.xy - 1.0f; ndc.y = -ndc.y; float4x4 invJitteredVP = UNITY_MATRIX_I_VP;//mul(UNITY_MATRIX_I_V, _InvJitteredProj); float4 positionWS = mul(invJitteredVP, float4(ndc, 1.0f)); return positionWS.xyz / positionWS.w; } float Linear01DepthToRawDepth(float z, float4 zBufferParams) { return (rcp(z) - zBufferParams.y) / zBufferParams.x; } float LinearEyeToRawDepth(float depth, float4 zBufferParams) { return (1.0f / depth - zBufferParams.w) / zBufferParams.z; } float GetDepth(float2 camNearFar, float2 vfNearFar, float ratio) { float valLeft = log(vfNearFar.x); float valRight = log(vfNearFar.y); float val = lerp(valLeft, valRight, ratio); float depthVal = exp(val); return depthVal; } float HGPhaseFunction(float g, float cosTheta) { float g2 = g * g; float denominator = 1.0f + g2 - 2 * g * cosTheta; return 0.25 * (1.0f - g2) * rsqrt(denominator * denominator * denominator); } float3 GetFogColor(float3 color, float3 lightDir, float3 viewDir, float g) { float cosVal = dot(-lightDir, viewDir); return color * HGPhaseFunction(g, cosVal); } float Hash13(float3 p) { p = frac(p * 0.1031); p += dot(p, p.zyx + 31.32); return frac((p.x + p.y) * p.z); } [numthreads(8,8,1)] void VolumetricFogMain(uint3 id : SV_DispatchThreadID) { float4 colorTex = _ColorTexture.Load(int3(id.xy, 0), 0); float depthTex = _DepthTexture.Load(int3(id.xy, 0), 0); float2 texcoord = (id.xy + 0.5f) * _TextureSize.zw; texcoord = texcoord + 0.5f * _TAAOffsets.xy; float3 nearPlaneNDC = float3(texcoord, 1.0f); float3 nearPlaneWS = NDCToWorld(nearPlaneNDC); float3 targetNDC = float3(texcoord, depthTex); float3 targetWS = NDCToWorld(targetNDC); float4 targetShadowCoord = TransformWorldToShadowCoord(targetWS); Light targetLight = GetMainLight(targetShadowCoord); float3 toTarget = targetWS - nearPlaneWS; float3 rayDir = normalize(nearPlaneWS - _WorldSpaceCameraPos); float3 viewDir = -rayDir; float totalStep = _StepCount; float lastStepDepthVal = _NearFar.z; float jitter = Hash13(float3(texcoord, _Time.y)); float3 accumScatter = float3(0.0f, 0.0f, 0.0f); float accumTrans = 1.0f; for (int s = 0; s \u0026lt; totalStep; s++) { jitter = Hash13(float3(texcoord, jitter)); float ratio = (s + jitter) / totalStep; float depthVal = GetDepth(_NearFar.xy, _NearFar.zw, ratio); float rawDepth = LinearEyeToRawDepth(depthVal, _ZBufferParams); float stepSize = depthVal - lastStepDepthVal; lastStepDepthVal = depthVal; if (rawDepth \u0026lt; depthTex) break; float3 tempNDC = float3(texcoord, rawDepth); float3 tempPosWS = NDCToWorld(tempNDC); float4 shadowCoord = TransformWorldToShadowCoord(tempPosWS); Light mainLight = GetMainLight(shadowCoord); float3 lightColor = mainLight.color * mainLight.shadowAttenuation; float3 lightDir = mainLight.direction; float3 fogColor = GetFogColor(lightColor, lightDir, viewDir, _Phase); fogColor += _FogTint.rgb * _FogTint.a; float density = _Density; float transmittance = exp(-density * stepSize * 0.01f); float3 scatter = fogColor * (1.0f - transmittance); accumScatter += scatter * accumTrans; accumTrans *= transmittance; } accumTrans = max(1.0f - _MaxTransmittance, accumTrans); float3 finalColor = colorTex.rgb * accumTrans + accumScatter; finalColor = lerp(colorTex.rgb, finalColor, _Intensity); _RW_VolTexture[id.xy] = float4(finalColor, 1.0f); } RayMarchingDebug.cs 这个脚本是用来可视化Ray Marching的位置的，这样可以更好的理解GetDepth这个函数对Ray Marching步长的影响，也可以发现\nusing System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; using Unity.Mathematics; [ExecuteInEditMode] public class RayMarchingDebug : MonoBehaviour { public Camera cam; public RMVolumetricFog volumetricFog; public bool drawDebug = false; [Range(1.0f, 5.0f)] public float logBase = 2.0f; [Range(-1.0f, 1.0f)] public float xCoord; [Range(-1.0f, 1.0f)] public float yCoord; void OnEnable() { cam = Camera.main; volumetricFog = VolumeManager.instance.stack.GetComponent\u0026lt;RMVolumetricFog\u0026gt;(); } float log(float val) { return math.log(val) / math.log(logBase); } float exp(float val) { return math.exp(val * math.log(logBase)); } float GetDepth(Camera cam, RMVolumetricFog vf, float ratio) { float2 camNearFar = new float2(cam.nearClipPlane, cam.farClipPlane); float2 vfNearFar = new float2(vf.fogNear.value, vf.fogFar.value); float baseVal = log(camNearFar.y / camNearFar.x); float valLeft = log(vfNearFar.x / camNearFar.x); float valRight = log(vfNearFar.y / camNearFar.x); float val = math.lerp(valLeft, valRight, ratio); float depthVal = camNearFar.x * exp(val); return depthVal; } float LinearEyeToRawDepth(float depth, float4 zBufferParams) { return (1.0f / depth - zBufferParams.w) / zBufferParams.z; } private void OnDrawGizmos() { if (!drawDebug) return; if (!cam || !volumetricFog) return; Color originalColor = Gizmos.color; Gizmos.color = Color.red; float2 camNearFar = new float2(cam.nearClipPlane, cam.farClipPlane); float4 zBufferParams = new float4( 1.0f - camNearFar.y / camNearFar.x, 1.0f, (1.0f - camNearFar.y / camNearFar.x) / camNearFar.y, 1.0f / camNearFar.y ); float4x4 projMat = GL.GetGPUProjectionMatrix(cam.projectionMatrix, false); float4x4 invProj = math.inverse(projMat); float4x4 viewMat = cam.transform.worldToLocalMatrix; float4x4 invView = math.inverse(viewMat); float ratio = 0.0f; int slice = (int)volumetricFog.step.value; for (int i = 0; i \u0026lt; slice; i++) { ratio = (i + 0.5f) / slice; float depthVal = GetDepth(cam, volumetricFog, ratio); float rawDepth = LinearEyeToRawDepth(depthVal, zBufferParams); float4 ndc = new float4(xCoord, -yCoord, rawDepth, 1.0f); float4 positionVS = math.mul(invProj, ndc); float4 positionWS = math.mul(invView, positionVS); positionWS /= positionWS.w; Gizmos.DrawSphere(positionWS.xyz, 0.1f); } Gizmos.color = originalColor; } } 后记 诶，这就水了一篇新博客吗？\n","permalink":"https://zznewclear13.github.io/posts/create-volumetric-fog-using-ray-marching/","summary":"为什么要用Ray Marching 要不还是别用Ray Marching了（除非是SDF Ray Marching），采样次数又多又不好debug，不过写起来比较快（如果要写二分法的话就又复杂了）。如前文所说，使用Ray Marching的体积雾只能在后处理阶段使用了，在处理不写深度的透明物体的时候，会有一些瑕疵。\n体积雾相关的就参考前文就好了，这里只是作为一个方法的补充。\n相关代码和说明 为了和使用3D纹理的体积雾作区分，这边所有代码的名字前加上了RM(Ray Marching)。\nRMVolumetricFog.cs 这个脚本和3D纹理的体积雾的参数几乎完全一致，只是多了用于控制Ray Marching次数的step。\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenu(\u0026#34;Post-processing/RM Volumetric Fog\u0026#34;)] public class RMVolumetricFog : VolumeComponent, IPostProcessComponent { [Tooltip(\u0026#34;是否启用体积雾\u0026#34;)] public BoolParameter enabled = new BoolParameter(false); [Tooltip(\u0026#34;整体控制体积雾强度\u0026#34;)] public ClampedFloatParameter intensity = new ClampedFloatParameter(1.0f, 0f, 1.0f); [Tooltip(\u0026#34;体积雾最大的透明程度（用于和天空混合）\u0026#34;)] public ClampedFloatParameter maxTransmittance = new ClampedFloatParameter(1.0f, 0f, 1.0f); [Tooltip(\u0026#34;体积雾的颜色倾向，目前强度为0.03\u0026#34;)] public ColorParameter fogTint = new ColorParameter(Color.white); [Tooltip(\u0026#34;体积雾距离相机最近的距离\u0026#34;)] public ClampedFloatParameter fogNear = new ClampedFloatParameter(0.1f, 0.01f, 10f); [Tooltip(\u0026#34;体积雾距离相机最远的距离\u0026#34;)] public ClampedFloatParameter fogFar = new ClampedFloatParameter(100f, 1.","title":"使用Ray Marching来渲染体积雾"},{"content":"为什么要渲染体积雾 因为它就在那里。\n当然了，更重要的是因为体积雾能迅速的营造出场景的真实感与氛围感，谁不喜欢光源边上还有一小圈光晕呢，如果什么高亮的物体都能影响体积雾的话，是不是就不太需要bloom效果了呢。我实际地在生活中观察了一下，发现人眼所看到的光晕的效果，是光线进入眼睛之后产生的，也就是说bloom和体积雾确确实实是两种不同的效果。\n体积雾的渲染方法 体积雾一般有两种渲染方法，一种是单纯的从相机出发对场景进行Ray Marching，每次进行采样和混合。这种方法主要的缺点是Ray Marching的次数会比较高才能有较好的渲染效果。在我的测试中，开启TAA的时候，20次Ray Marching就能得到很好的体积雾效果了；但是不开启TAA的话，可能会需要60次甚至更高的Ray Marching才能得到和TAA类似的效果。同时，Ray Marching体积雾只能在后处理阶段使用，在处理不写深度的透明物体的时候，会有一些瑕疵。\n另一种方法就是使用一张3D纹理，将整个场景的体积雾储存在这张3D纹理中，当绘制物体的时候使用物体的世界空间坐标采样这张3D纹理，直接在片元着色器中计算雾效之后的颜色。这种方法使用的3D纹理会占用更多的内存，但是一定程度上能够正确的渲染所有物体，和60次Ray Marching相比，性能上也说不定会有一些优势。\n本文的体积雾实现，参考了EA的寒霜引擎在Siggraph 2015年时的演讲和diharaw的OpenGL的体积雾效果。值得一看的还有Bart Wronski在Siggraph 2014年的演讲，以及之后的荒野大镖客在Siggraph 2019年的课程。使用的是Unity2019.4.29的URP工程。\n具体的实现方法 将场景中的需要渲染的雾的信息和阴影信息储存到一张和相机的视锥体对齐的3D纹理中。按照寒霜引擎的做法，纹理大小为(分辨率宽/8)x(分辨率高/8)x64，这样就和屏幕大小的2D纹理占用的内存大小一致了，但我看Unity官方的体积雾工程中，3D纹理的深度为128，就也把自己的设置成128了，纹理深度越深，体积雾的细节就能越高。3D纹理的宽高和视锥体对齐，这很好理解，而这张贴图的纵向深度和实际的深度要怎么对齐呢？最简单的就是和视空间的深度线性对应，但是这会导致近处体积雾的分辨率不够；另一种是和裁剪空间的深度线性对应，经过一些分析可以知道这比之前的方法更糟糕；目前我看下来最好的应该是和视空间的深度指数型对应，这样离相机越近3D纹理的像素会越多，越远则越少。本文只使用了均一的雾，但是可以使用世界空间的坐标、噪波和一系列的运算，计算出某一点的体积雾的浓度。 使用上面的雾的信息和阴影信息计算出散射的值Lscat，从下面的图可以看到Lscat是对所有的光源（本文只有主光源）计算\\(f(v, l)Vis(x, l)Li(x, l)\\)的和，\\(Vis(x, l)\\)即为在x点l光的可见性，可以通过采样阴影贴图来获得，\\(Li(x, l)\\)即为在x点l光的光强，可以简单的计算获得，\\(f(v, l)\\)用来表述在v的方向观察雾时得到l的散射量，一般被叫做Phase Function，我们使用的是Henyey-Greenstein Phase Function，其中参数g是雾的各向异性的程度，越靠近1表示光线穿过雾时越保持之前的方向，越靠近0表示光线穿过雾时均匀的散射，越靠近-1表示光线穿过雾时越会进行反射（在实际的光照中，我们会去掉\\(\\pi\\)这一项，这样能和Unity的光照模型保持一致）。时空混合也在这一步可以完成。 $$ \\tag{Henyey-Greenstein} p(\\theta) = \\frac 1 {4\\pi} \\frac {1 - g^2} {(1 + g^2 - 2g \\cos \\theta)^{\\frac 3 2}} $$\n对3D纹理从相机近点到远点进行混合，这其实是一种Ray Marching，不过是在3D纹理的纹理空间进行Ray Marching，一次前进一个像素。当混合当前像素和上一个像素时，需要考虑符合物理的透光率(transmittance), \\(\\varepsilon\\)是一个用于归一化的常量，l是两点之间的距离，c是介质的吸收率（一定程度上可以用雾的密度来表示）。具体的混合的计算和说明可以看EA寒霜引擎的PPT第28、29页。 $$ \\tag{Beer-Lambert} transmittance = e^{-\\varepsilon l c} $$\n最终在绘制物体时，使用物体的世界空间的坐标，转换到3D纹理的坐标，采样3D纹理，使用透光率乘上物体本身的颜色，再加上雾的颜色，就得到了最终的体积雾的效果了。 相关代码和说明 VolumetricFog.cs 用于Global Volume中方便添加体积雾和控制各种参数。值得考虑的是maxTransmittance的值，因为相机远裁剪面会比较远，即使雾并不是很大，在最远处也总是能变成单一的颜色，这个值用来防止这种情况，人为地限制了最大不透光率（但是还是叫maxTransmittance）。fogNear这个参数实际是影响了3D纹理和相机之间的距离，最好还是设置成0，不然时空混合时会有一些瑕疵。\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenu(\u0026#34;Post-processing/Volumetric Fog\u0026#34;)] public class VolumetricFog : VolumeComponent, IPostProcessComponent { [Tooltip(\u0026#34;是否启用体积雾\u0026#34;)] public BoolParameter enabled = new BoolParameter(false); [Tooltip(\u0026#34;整体控制体积雾强度\u0026#34;)] public ClampedFloatParameter intensity = new ClampedFloatParameter(1.0f, 0f, 1.0f); [Tooltip(\u0026#34;体积雾最大的透明程度（用于和天空混合）\u0026#34;)] public ClampedFloatParameter maxTransmittance = new ClampedFloatParameter(1.0f, 0f, 1.0f); [Tooltip(\u0026#34;体积雾的颜色倾向，目前强度为0.03\u0026#34;)] public ColorParameter fogTint = new ColorParameter(Color.white); [Tooltip(\u0026#34;体积雾距离相机最近的距离\u0026#34;)] public ClampedFloatParameter fogNear = new ClampedFloatParameter(0.1f, 0.01f, 10f); [Tooltip(\u0026#34;体积雾距离相机最远的距离\u0026#34;)] public ClampedFloatParameter fogFar = new ClampedFloatParameter(100f, 1.0f, 1000.0f); [Tooltip(\u0026#34;体积雾的密度，越密效果越明显\u0026#34;)] public ClampedFloatParameter density = new ClampedFloatParameter(3.0f, 0f, 10.0f); [Tooltip(\u0026#34;体积雾受光的各向异性程度\u0026#34;)] public ClampedFloatParameter phase = new ClampedFloatParameter(0.0f, -0.9f, 0.9f); public bool IsActive() =\u0026gt; (enabled.value \u0026amp;\u0026amp; (density.value \u0026gt; 0.0f) \u0026amp;\u0026amp; (intensity.value \u0026gt; 0.0f)); public bool IsTileCompatible() =\u0026gt; false; } } VolumetricFogRendererFeature.cs 平平常常的RendererFeature，事实上RenderPassEvent应该在DepthPrePass之后，但我没改物体的shader，就放在后处理之前了。\nnamespace UnityEngine.Rendering.Universal { public class VolumetricFogRendererFeature : ScriptableRendererFeature { [System.Serializable] public class VolumetricFogSettings { public RenderPassEvent renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; public ComputeShader volumetricFogComputeShader; } private VolumetricFogRenderPass volumetricFogRenderPass; public VolumetricFogSettings volumetricFogSettings = new VolumetricFogSettings(); public override void Create() { volumetricFogRenderPass = new VolumetricFogRenderPass(volumetricFogSettings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { if (renderingData.cameraData.cameraType == CameraType.Game) { VolumetricFog volumetricFog = VolumeManager.instance.stack.GetComponent\u0026lt;VolumetricFog\u0026gt;(); if (volumetricFog \u0026amp;\u0026amp; volumetricFog.IsActive()) { volumetricFogRenderPass.Setup(volumetricFog); renderer.EnqueuePass(volumetricFogRenderPass); } } } } } VolumetricFogRenderPass.cs 平平常常的RenderPass，实际使用的时候，只会用到Froxel Generate Pass和Scatter Pass，Composite Pass完全可以用物件本身的渲染来代替。\nnamespace UnityEngine.Rendering.Universal { public class VolumetricFogRenderPass : ScriptableRenderPass { private const string profilerTag = \u0026#34;Volumetric Fog Pass\u0026#34;; private ProfilingSampler profilingSampler; private ProfilingSampler froxelSampler = new ProfilingSampler(\u0026#34;Froxel Generate Pass\u0026#34;); private ProfilingSampler scatterSampler = new ProfilingSampler(\u0026#34;Scatter Pass\u0026#34;); private ProfilingSampler compositeSampler = new ProfilingSampler(\u0026#34;Composite Pass\u0026#34;); private RenderTargetHandle cameraColor; private RenderTargetIdentifier cameraColorIden; private RenderTargetHandle cameraDepth; private RenderTargetIdentifier cameraDepthIden; private RenderTargetHandle cameraDepthAttachment; private RenderTargetIdentifier cameraDepthAttachmentIden; private VolumetricFog volumetricFog; private ComputeShader volumetricFogComputeShader; private VolumetricFogRendererFeature.VolumetricFogSettings settings; private RenderTexture[] froxelTextures; private RenderTextureDescriptor cubeDesc; private static readonly string froxelTextureOneName = \u0026#34;_FroxelBufferOne\u0026#34;; private static readonly int froxelTextureOneID = Shader.PropertyToID(froxelTextureOneName); private RenderTargetHandle froxelTextureOneHandle; private RenderTargetIdentifier froxelTextureOneIden; private static readonly string froxelTextureTwoName = \u0026#34;_FroxelBufferTwo\u0026#34;; private static readonly int froxelTextureTwoID = Shader.PropertyToID(froxelTextureTwoName); private RenderTargetHandle froxelTextureTwoHandle; private RenderTargetIdentifier froxelTextureTwoIden; private static readonly string scatterTextureName = \u0026#34;_ScatterBuffer\u0026#34;; private static readonly int scatterTextureID = Shader.PropertyToID(scatterTextureName); private RenderTargetHandle scatterTextureHandle; private RenderTargetIdentifier scatterTextureIden; private static readonly string compositeTextureName = \u0026#34;_CompositeBuffer\u0026#34;; private static readonly int compositeTextureID = Shader.PropertyToID(compositeTextureName); private RenderTargetHandle compositeTextureHandle; private RenderTargetIdentifier compositeTextureIden; private Vector2 colorTextureSize; private Vector2 invColorTextureSize; private Vector3 froxelTextureSize; private Vector3 invFroxelTextureSize; private Matrix4x4 lastViewProjMatrix; private int flipReadWrite = 0; public VolumetricFogRenderPass(VolumetricFogRendererFeature.VolumetricFogSettings settings) { this.settings = settings; profilingSampler = new ProfilingSampler(profilerTag); renderPassEvent = settings.renderPassEvent; volumetricFogComputeShader = settings.volumetricFogComputeShader; cameraColor.Init(\u0026#34;_CameraColorTexture\u0026#34;); cameraColorIden = cameraColor.Identifier(); cameraDepth.Init(\u0026#34;_CameraDepthTexture\u0026#34;); cameraDepthIden = cameraDepth.Identifier(); cameraDepthAttachment.Init(\u0026#34;_CameraDepthAttachment\u0026#34;); cameraDepthAttachmentIden = cameraDepthAttachment.Identifier(); froxelTextureOneHandle.Init(froxelTextureOneName); froxelTextureOneIden = froxelTextureOneHandle.Identifier(); froxelTextureTwoHandle.Init(froxelTextureTwoName); froxelTextureTwoIden = froxelTextureTwoHandle.Identifier(); scatterTextureHandle.Init(scatterTextureName); scatterTextureIden = scatterTextureHandle.Identifier(); compositeTextureHandle.Init(compositeTextureName); compositeTextureIden = compositeTextureHandle.Identifier(); lastViewProjMatrix = Matrix4x4.identity; } public void Setup(VolumetricFog volumetricFog) { this.volumetricFog = volumetricFog; } private static void EnsureArray\u0026lt;T\u0026gt;(ref T[] array, int size, T initialValue = default(T)) { if (array == null || array.Length != size) { array = new T[size]; for (int i = 0; i != size; i++) array[i] = initialValue; } } private static void EnsureRenderTexture(ref RenderTexture rt, RenderTextureDescriptor descriptor, string RTName) { if (rt != null \u0026amp;\u0026amp; (rt.width != descriptor.width || rt.height != descriptor.height)) { RenderTexture.ReleaseTemporary(rt); rt = null; } if (rt == null) { RenderTextureDescriptor desc = descriptor; desc.depthBufferBits = 0; desc.msaaSamples = 1; rt = RenderTexture.GetTemporary(desc); //rt = new RenderTexture(desc); rt.name = RTName; if (!rt.IsCreated()) rt.Create(); } } public static void EnsureRT(ref RenderTexture[] froxelTexs, RenderTextureDescriptor descriptor) { EnsureArray(ref froxelTexs, 2); EnsureRenderTexture(ref froxelTexs[0], descriptor, \u0026#34;Froxel Tex One\u0026#34;); EnsureRenderTexture(ref froxelTexs[1], descriptor, \u0026#34;Froxel Tex Two\u0026#34;); } public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor) { RenderTextureDescriptor desc = cameraTextureDescriptor; desc.enableRandomWrite = true; cmd.GetTemporaryRT(compositeTextureID, desc); colorTextureSize = new Vector2(desc.width, desc.height); invColorTextureSize = new Vector2(1.0f / desc.width, 1.0f / desc.height); int width = desc.width / 8; int height = desc.height / 8; int volmeDepth = 128; cubeDesc = new RenderTextureDescriptor( width, height, Experimental.Rendering.GraphicsFormat.R16G16B16A16_SFloat, 0); cubeDesc.volumeDepth = volmeDepth; cubeDesc.dimension = TextureDimension.Tex3D; cubeDesc.enableRandomWrite = true; froxelTextureSize = new Vector3(width, height, volmeDepth); invFroxelTextureSize = new Vector3(1.0f / (width-0), 1.0f / (height-0), 1.0f / (volmeDepth-0)); cmd.GetTemporaryRT(scatterTextureID, cubeDesc); } private void GenerateFroxel(CommandBuffer cmd, CameraData camData, RenderTargetIdentifier depthid, RenderTexture froxelReadid, RenderTexture froxelWriteid, ComputeShader computeShader) { int froxelKernel = computeShader.FindKernel(\u0026#34;FroxelMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(froxelKernel, out uint x, out uint y, out uint z); cmd.SetComputeVectorParam(computeShader, \u0026#34;_FroxelTextureSize\u0026#34;, froxelTextureSize); cmd.SetComputeVectorParam(computeShader, \u0026#34;_ColorTextureSize\u0026#34;, colorTextureSize); cmd.SetComputeMatrixParam(computeShader, \u0026#34;_LastViewProj\u0026#34;, lastViewProjMatrix); Matrix4x4 projMat = camData.GetGPUProjectionMatrix(); Matrix4x4 viewMat = camData.GetViewMatrix(); lastViewProjMatrix = projMat * viewMat; cmd.SetComputeTextureParam(computeShader, froxelKernel, \u0026#34;_DepthTexture\u0026#34;, depthid); cmd.SetComputeTextureParam(computeShader, froxelKernel, \u0026#34;_FroxelTexture\u0026#34;, froxelReadid); cmd.SetComputeTextureParam(computeShader, froxelKernel, \u0026#34;_RW_FroxelTexture\u0026#34;, froxelWriteid); Color fogTint = volumetricFog.fogTint.value; fogTint.a = 0.03f; volumetricFog.fogTint.Override(fogTint); cmd.SetComputeVectorParam(computeShader, \u0026#34;_FogTint\u0026#34;, volumetricFog.fogTint.value); cmd.SetComputeVectorParam(computeShader, \u0026#34;_NearFar\u0026#34;, new Vector4(camData.camera.nearClipPlane, camData.camera.farClipPlane, volumetricFog.fogNear.value, volumetricFog.fogFar.value)); cmd.SetComputeVectorParam(computeShader, \u0026#34;_VolumetricFogParams\u0026#34;, new Vector4(volumetricFog.phase.value, volumetricFog.density.value, volumetricFog.intensity.value, volumetricFog.maxTransmittance.value)); cmd.DispatchCompute(computeShader, froxelKernel, Mathf.CeilToInt(froxelTextureSize.x / x), Mathf.CeilToInt(froxelTextureSize.y / y), Mathf.CeilToInt(froxelTextureSize.z / z)); } private void ScatterFog(CommandBuffer cmd, RenderTexture froxelid, RenderTargetIdentifier scatterid, ComputeShader computeShader) { int scatterKernel = computeShader.FindKernel(\u0026#34;ScatterMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(scatterKernel, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, scatterKernel, \u0026#34;_FroxelTexture\u0026#34;, froxelid); cmd.SetComputeTextureParam(computeShader, scatterKernel, \u0026#34;_RW_ScatterTexture\u0026#34;, scatterid); cmd.DispatchCompute(computeShader, scatterKernel, Mathf.CeilToInt(froxelTextureSize.x / x), Mathf.CeilToInt(froxelTextureSize.y / y), 1); } private void CompositeVolumetricFog(CommandBuffer cmd, RenderTargetIdentifier colorid, RenderTargetIdentifier depthid, RenderTargetIdentifier scatterid, RenderTargetIdentifier compositeid, ComputeShader computeShader) { int compositeKernel = computeShader.FindKernel(\u0026#34;CompositeMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(compositeKernel, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, compositeKernel, \u0026#34;_ColorTexture\u0026#34;, colorid); cmd.SetComputeTextureParam(computeShader, compositeKernel, \u0026#34;_DepthTexture\u0026#34;, depthid); cmd.SetComputeTextureParam(computeShader, compositeKernel, \u0026#34;_ScatterTexture\u0026#34;, scatterid); cmd.SetComputeTextureParam(computeShader, compositeKernel, \u0026#34;_RW_CompositeTexture\u0026#34;, compositeid); cmd.DispatchCompute(computeShader, compositeKernel, Mathf.CeilToInt(colorTextureSize.x / x), Mathf.CeilToInt(colorTextureSize.y / y), 1); cmd.Blit(compositeid, colorid); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(profilerTag); context.ExecuteCommandBuffer(cmd); cmd.Clear(); EnsureRT(ref froxelTextures, cubeDesc); RenderTexture froxelReadTex = froxelTextures[flipReadWrite]; RenderTexture froxelWriteTex = froxelTextures[1 - flipReadWrite]; flipReadWrite = 1 - flipReadWrite; using (new ProfilingScope(cmd, froxelSampler)) { GenerateFroxel(cmd, renderingData.cameraData, cameraDepthAttachmentIden, froxelReadTex, froxelWriteTex, volumetricFogComputeShader); } using (new ProfilingScope(cmd, scatterSampler)) { ScatterFog(cmd, froxelWriteTex, scatterTextureIden, volumetricFogComputeShader); } using (new ProfilingScope(cmd, compositeSampler)) { CompositeVolumetricFog(cmd, cameraColorIden, cameraDepthAttachmentIden, scatterTextureIden, compositeTextureIden, volumetricFogComputeShader); } context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } public override void FrameCleanup(CommandBuffer cmd) { cmd.ReleaseTemporaryRT(scatterTextureID); cmd.ReleaseTemporaryRT(compositeTextureID); } } } VolumetricFogComputeShader.compute 重头戏来了，这个Compute Shader一共有3个kernel。第一个用来通过雾的信息和阴影的信息计算光照并储存到_RW_FroxelTexture中，同时也做了自身的和历史的混合，也对TAA做了适配。第二个用来做纹理空间的Ray Marching，计算散射的颜色和透光率。第三个其实是一个屏幕后处理的效果，将雾效画到屏幕上，当在物件shader中计算雾效时，就不需要这个kernel了。\nGetDepth是将纹理的z转换到视空间的线性深度(Linear Eye Depth)，GetRatio则是相反，把线性的视空间的深度转换到纹理的z坐标， NOT_SIMPLIFIED这个宏可以让人更好的理解指数型分布的计算过程。\n为了简化问题，这边只考虑了主光源使用联级阴影时的体积雾效果，也没有考虑集成SH来计算全局光照对体积雾的影响。\n#pragma kernel FroxelMain #pragma kernel ScatterMain #pragma kernel CompositeMain #define _MAIN_LIGHT_SHADOWS #define _MAIN_LIGHT_SHADOWS_CASCADE #define _SHADOWS_SOFT #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; Texture2D\u0026lt;float\u0026gt; _DepthTexture; Texture2D\u0026lt;float4\u0026gt; _ColorTexture; Texture3D\u0026lt;float4\u0026gt; _FroxelTexture; Texture3D\u0026lt;float4\u0026gt; _ScatterTexture; RWTexture3D\u0026lt;float4\u0026gt; _RW_FroxelTexture; RWTexture3D\u0026lt;float4\u0026gt; _RW_ScatterTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_CompositeTexture; SamplerState sampler_LinearClamp; SamplerState sampler_PointClamp; float4 _ColorTextureSize; //float4 _InvColorTextureSize; float4 _FroxelTextureSize; //float4 _InvFroxelTextureSize; float _StepCount; float4 _NearFar; float4 _FogTint; float4 _VolumetricFogParams; // x: cam near, y: cam far, z: fog near, w: fog far #define _Phase _VolumetricFogParams.x #define _Density _VolumetricFogParams.y #define _Intensity _VolumetricFogParams.z #define _MaxTransmittance _VolumetricFogParams.w float4 _TAAOffsets; float4x4 _LastViewProj; float3 NDCToWorld(float3 ndc) { ndc.xy = 2.0f * ndc.xy - 1.0f; ndc.y = -ndc.y; float4x4 invJitteredVP = UNITY_MATRIX_I_VP;//mul(UNITY_MATRIX_I_V, _InvJitteredProj); float4 positionWS = mul(invJitteredVP, float4(ndc, 1.0f)); return positionWS.xyz / positionWS.w; } float Linear01DepthToRawDepth(float z, float4 zBufferParams) { return (rcp(z) - zBufferParams.y) / zBufferParams.x; } float LinearEyeToRawDepth(float depth, float4 zBufferParams) { return (1.0f / depth - zBufferParams.w) / zBufferParams.z; } float GetDepth(float2 camNearFar, float2 vfNearFar, float ratio) { #if NOT_SIMPLIFIED float valLeft = log(vfNearFar.x / camNearFar.x); float valRight = log(vfNearFar.y / camNearFar.x); float val = lerp(valLeft, valRight, ratio); float depthVal = camNearFar.x * exp(val); return depthVal; #else float valLeft = log(vfNearFar.x); float valRight = log(vfNearFar.y); float val = lerp(valLeft, valRight, ratio); float depthVal = exp(val); return depthVal; #endif } float GetRatio(float2 camNearFar, float2 vfNearFar, float linearDepth) { #if NOT_SIMPLIFIED float valLeft = log(vfNearFar.x / camNearFar.x); float valRight = log(vfNearFar.y / camNearFar.x); float val = log(linearDepth / camNearFar.x); float ratio = (val - valLeft) / (valRight - valLeft); return ratio; #else float valLeft = log(vfNearFar.x); float valRight = log(vfNearFar.y); float val = log(linearDepth); float ratio = (val - valLeft) / (valRight - valLeft); return ratio; #endif } float HGPhaseFunction(float g, float cosTheta) { float g2 = g * g; float denominator = 1.0f + g2 - 2 * g * cosTheta; return 0.25 * (1.0f - g2) * rsqrt(denominator * denominator * denominator); } float3 GetFogColor(float3 color, float3 lightDir, float3 viewDir, float g) { float cosVal = dot(-lightDir, viewDir); return color * HGPhaseFunction(g, cosVal); } float Hash13(float3 p) { p = frac(p * 0.1031); p += dot(p, p.zyx + 31.32); return frac((p.x + p.y) * p.z); } [numthreads(8,8,8)] void FroxelMain (uint3 id : SV_DispatchThreadID) { float2 texcoord = (id.xy + 0.5f) / _FroxelTextureSize.xy; texcoord += 0.5f * _TAAOffsets.xy; float jitter = Hash13(float3(texcoord, _Time.y * id.z)); float ratio = (id.z + jitter) / _FroxelTextureSize.z; float depthVal = GetDepth(_NearFar.xy, _NearFar.zw, ratio); float rawDepth = LinearEyeToRawDepth(depthVal, _ZBufferParams); float3 positionNDC = float3(texcoord, rawDepth); float3 positionWS = NDCToWorld(positionNDC); float3 viewDir = normalize(GetCameraPositionWS() - positionWS); float4 shadowCoord = TransformWorldToShadowCoord(positionWS); Light mainLight = GetMainLight(shadowCoord); float3 lightColor = mainLight.color * mainLight.shadowAttenuation; float3 lightDir = mainLight.direction; float3 fogColor = GetFogColor(lightColor, lightDir, viewDir, _Phase); fogColor += _FogTint.rgb * _FogTint.a; float density = _Density; float4 finalFroxel = float4(fogColor, density); // Reprojection Temporal Filter float ujRatio = (id.z + 0.5) / _FroxelTextureSize.z; float ujDepthVal = GetDepth(_NearFar.xy, _NearFar.zw, ujRatio); float ujRawDepth = LinearEyeToRawDepth(ujDepthVal, _ZBufferParams); float3 ujPositionNDC = float3(texcoord, ujRawDepth); float3 ujPositionWS = NDCToWorld(ujPositionNDC); float4 lastPositionCS = mul(_LastViewProj, float4(ujPositionWS, 1.0f)); lastPositionCS /= lastPositionCS.w; lastPositionCS.y = -lastPositionCS.y; float3 lastNDC = float3(lastPositionCS.xy * 0.5 + 0.5, lastPositionCS.z); lastNDC.xy -= 0.5f * _TAAOffsets.zw; if(all(lastNDC \u0026gt; 0.0) \u0026amp;\u0026amp; all(lastNDC \u0026lt; 1.0f)) { float linearEyeDepth = LinearEyeDepth(lastNDC.z, _ZBufferParams); float reprojRatio = GetRatio(_NearFar.xy, _NearFar.zw, linearEyeDepth); float4 froxelTex = _FroxelTexture.SampleLevel(sampler_LinearClamp, float3(lastNDC.xy, reprojRatio), 0); finalFroxel = lerp(finalFroxel, froxelTex, 0.95); } _RW_FroxelTexture[id] = finalFroxel; } float SliceThickness(int z) { float ratioThis = z / _FroxelTextureSize.z; float depthThis = GetDepth(_NearFar.xy, _NearFar.zw, ratioThis); float ratioNext = (z+1.0f) / _FroxelTextureSize.z; float depthNext = GetDepth(_NearFar.xy, _NearFar.zw, ratioNext); return depthNext - depthThis; } float4 AccumScatter(int z, float4 accum, float4 slice) { slice.a = max(slice.a, 1e-5); float thickness = SliceThickness(z); float sliceTransmittance = exp(-slice.a * thickness * 0.01f); float3 sliceScattering = slice.rgb * (1.0f - sliceTransmittance); float3 accumScattering = accum.rgb + sliceScattering * accum.a; float accumTransmittance = accum.a * sliceTransmittance; return float4(accumScattering, accumTransmittance); } [numthreads(16,16,1)] void ScatterMain (uint3 id : SV_DispatchThreadID) { float4 accum = float4(0.0f, 0.0f, 0.0f, 1.0f); for (int z=0; z\u0026lt;_FroxelTextureSize.z; z++) { int3 coord = int3(id.xy, z); float4 slice = _FroxelTexture[coord]; accum = AccumScatter(z, accum, slice); //_RW_ScatterTexture[coord] = slice; _RW_ScatterTexture[coord] = accum; } } [numthreads(16,16,1)] void CompositeMain (uint3 id : SV_DispatchThreadID) { float2 texcoord = (id.xy + 0.5f) * rcp(_ColorTextureSize.xy); float3 colorTex = _ColorTexture.SampleLevel(sampler_PointClamp, texcoord, 0).rgb; float depthTex = _DepthTexture.SampleLevel(sampler_PointClamp, texcoord, 0); float linearEyeDepth = LinearEyeDepth(depthTex, _ZBufferParams); float ratio = GetRatio(_NearFar.xy, _NearFar.zw, linearEyeDepth); float4 froxelTex = _ScatterTexture.SampleLevel(sampler_LinearClamp, float3(texcoord, ratio), 0); float3 accumScatter = froxelTex.rgb; float accumTrans = max(1.0f - _MaxTransmittance, froxelTex.a); float3 finalColor = colorTex * accumTrans + froxelTex.rgb; finalColor = lerp(colorTex.rgb, finalColor, _Intensity); _RW_CompositeTexture[id.xy] = float4(finalColor, 1.0f); } 后记 好久没有写新的博客啦，之前一直在学c++，不怎么有时间做新的东西。体积雾还是一个蛮重要的效果，之前做Ray Marching的时候老是把握不住该步进多少，用了指数型的步进之后就感觉豁然开朗了。TAA其实也做了一版新的，还没来得及写，GTAO也学了一遍，就之后再说吧。\n","permalink":"https://zznewclear13.github.io/posts/create-volumetric-fog-using-view-aligned-3d-texture/","summary":"为什么要渲染体积雾 因为它就在那里。\n当然了，更重要的是因为体积雾能迅速的营造出场景的真实感与氛围感，谁不喜欢光源边上还有一小圈光晕呢，如果什么高亮的物体都能影响体积雾的话，是不是就不太需要bloom效果了呢。我实际地在生活中观察了一下，发现人眼所看到的光晕的效果，是光线进入眼睛之后产生的，也就是说bloom和体积雾确确实实是两种不同的效果。\n体积雾的渲染方法 体积雾一般有两种渲染方法，一种是单纯的从相机出发对场景进行Ray Marching，每次进行采样和混合。这种方法主要的缺点是Ray Marching的次数会比较高才能有较好的渲染效果。在我的测试中，开启TAA的时候，20次Ray Marching就能得到很好的体积雾效果了；但是不开启TAA的话，可能会需要60次甚至更高的Ray Marching才能得到和TAA类似的效果。同时，Ray Marching体积雾只能在后处理阶段使用，在处理不写深度的透明物体的时候，会有一些瑕疵。\n另一种方法就是使用一张3D纹理，将整个场景的体积雾储存在这张3D纹理中，当绘制物体的时候使用物体的世界空间坐标采样这张3D纹理，直接在片元着色器中计算雾效之后的颜色。这种方法使用的3D纹理会占用更多的内存，但是一定程度上能够正确的渲染所有物体，和60次Ray Marching相比，性能上也说不定会有一些优势。\n本文的体积雾实现，参考了EA的寒霜引擎在Siggraph 2015年时的演讲和diharaw的OpenGL的体积雾效果。值得一看的还有Bart Wronski在Siggraph 2014年的演讲，以及之后的荒野大镖客在Siggraph 2019年的课程。使用的是Unity2019.4.29的URP工程。\n具体的实现方法 将场景中的需要渲染的雾的信息和阴影信息储存到一张和相机的视锥体对齐的3D纹理中。按照寒霜引擎的做法，纹理大小为(分辨率宽/8)x(分辨率高/8)x64，这样就和屏幕大小的2D纹理占用的内存大小一致了，但我看Unity官方的体积雾工程中，3D纹理的深度为128，就也把自己的设置成128了，纹理深度越深，体积雾的细节就能越高。3D纹理的宽高和视锥体对齐，这很好理解，而这张贴图的纵向深度和实际的深度要怎么对齐呢？最简单的就是和视空间的深度线性对应，但是这会导致近处体积雾的分辨率不够；另一种是和裁剪空间的深度线性对应，经过一些分析可以知道这比之前的方法更糟糕；目前我看下来最好的应该是和视空间的深度指数型对应，这样离相机越近3D纹理的像素会越多，越远则越少。本文只使用了均一的雾，但是可以使用世界空间的坐标、噪波和一系列的运算，计算出某一点的体积雾的浓度。 使用上面的雾的信息和阴影信息计算出散射的值Lscat，从下面的图可以看到Lscat是对所有的光源（本文只有主光源）计算\\(f(v, l)Vis(x, l)Li(x, l)\\)的和，\\(Vis(x, l)\\)即为在x点l光的可见性，可以通过采样阴影贴图来获得，\\(Li(x, l)\\)即为在x点l光的光强，可以简单的计算获得，\\(f(v, l)\\)用来表述在v的方向观察雾时得到l的散射量，一般被叫做Phase Function，我们使用的是Henyey-Greenstein Phase Function，其中参数g是雾的各向异性的程度，越靠近1表示光线穿过雾时越保持之前的方向，越靠近0表示光线穿过雾时均匀的散射，越靠近-1表示光线穿过雾时越会进行反射（在实际的光照中，我们会去掉\\(\\pi\\)这一项，这样能和Unity的光照模型保持一致）。时空混合也在这一步可以完成。 $$ \\tag{Henyey-Greenstein} p(\\theta) = \\frac 1 {4\\pi} \\frac {1 - g^2} {(1 + g^2 - 2g \\cos \\theta)^{\\frac 3 2}} $$\n对3D纹理从相机近点到远点进行混合，这其实是一种Ray Marching，不过是在3D纹理的纹理空间进行Ray Marching，一次前进一个像素。当混合当前像素和上一个像素时，需要考虑符合物理的透光率(transmittance), \\(\\varepsilon\\)是一个用于归一化的常量，l是两点之间的距离，c是介质的吸收率（一定程度上可以用雾的密度来表示）。具体的混合的计算和说明可以看EA寒霜引擎的PPT第28、29页。 $$ \\tag{Beer-Lambert} transmittance = e^{-\\varepsilon l c} $$\n最终在绘制物体时，使用物体的世界空间的坐标，转换到3D纹理的坐标，采样3D纹理，使用透光率乘上物体本身的颜色，再加上雾的颜色，就得到了最终的体积雾的效果了。 相关代码和说明 VolumetricFog.cs 用于Global Volume中方便添加体积雾和控制各种参数。值得考虑的是maxTransmittance的值，因为相机远裁剪面会比较远，即使雾并不是很大，在最远处也总是能变成单一的颜色，这个值用来防止这种情况，人为地限制了最大不透光率（但是还是叫maxTransmittance）。fogNear这个参数实际是影响了3D纹理和相机之间的距离，最好还是设置成0，不然时空混合时会有一些瑕疵。\nusing System; namespace UnityEngine.","title":"使用和视锥体对齐的3D纹理来渲染体积雾"},{"content":"为什么要从深度图重建视空间法线 一个很大的应用情景是在后处理的阶段，或是计算一些屏幕空间的效果（如SSR、SSAO等），只能获取到一张深度贴图，而不是每一个几何体的顶点数据，很多的计算中却又需要用到世界空间的法线或者是视空间的法线，这时我们就需要通过深度图来重建视空间的法线。（诶这段话我是不是写过一遍了）\n重建视空间法线的方法 bgolus在他的WorldNormalFromDepthTexture.shader里面很全面的介绍了各种重建视空间法线的方法。其中比较值得注意的是来自Janos Turanszki的根据深度差判断当前像素属于哪个平面的方法，和来自吴彧文的横向和纵向多采样一个点来判断当前像素属于哪个平面的方法，其中吴彧文的方法能够在绝大部分情况下获取到最准确的法线（除了尖角的一个像素）。\n除了bgolus介绍的方法之外，我在GameTechDev/XeGTAO中还看到了一种方法。这种方法类似于Janos Turanszki的深度差的方法，不过从深度差中获取的是0-1的边缘值（edgesLRTB，edgesLRTB.x越接近0即代表该像素的左侧越是一条边缘），再使用边缘的两两乘积对四个法线进行插值，最终计算出视空间法线。我个人认为当在两个面相接的地方不需要特别准确的法线值时，这是最好的计算法线的方法。用这个方式计算的法线，在两个面相接的地方，法线会有一种从一个面插值到另一个面的效果（且一定程度上抗锯齿），在两个面远近排布的时候，也能获取到准确的法线。\n具体的实现方法 根据需要使用的方法，采样深度图。在采样比较集中的情况下，可以使用GatherRed方法来减少采样的次数。GatherRed可以得到双线性采样时的四个像素的R通道的值并封装到一个float4中，当屏幕左下角是(0, 0)时，这个float4的x分量对应采样点左上角的颜色的R通道的值，y对应右上角，z对应右下角，w对应左下角，可以在HLSL的文档中看到Gather的相关介绍。Compute Shader的话可以使用group shared memory进一步减少采样。 使用深度图和当前的uv值计算出像素的视空间的坐标，这一步尤其需要注意视空间坐标Z分量的正负性的问题。Unity的视空间变换矩阵UNITY_MATRIX_V是摄像机位于视空间(0, 0, 0)，看向视空间Z轴负方向的，右手系的矩阵。即视空间的坐标Z分量往往是一个负值，其法线的Z分量在往往下是正值（即画面看上去应该多为蓝色）。 从深度图中计算视空间坐标的时候，如果Unity版本比较旧，会没有UNITY_MATRIX_I_P这个矩阵，这时可以使用unity_CameraInvProjection来代替，但需要注意DirectX平台UV上下翻转的问题。 当屏幕左下角是(0, 0)时，使用右侧的视空间坐标减去左侧的视空间坐标，使用上侧的视空间坐标减去下侧的视空间坐标。五个采样点（包括位于中心的当前像素）可以获得四个向量，对于右手系的视空间坐标来说，将这四个向量按照水平向量叉乘竖直向量的顺序，就可以获得四个当前像素的法线了。 最后使用前面介绍的获取法线的方法，从这四个法线中获取最为正确的法线。这些方法往往都会使用深度值来进行判断，这里需要注意的是透视变换带来的深度的非线性的问题。对于屏幕上等距分布的三个点ABC，当他们在世界空间中处于同一条直线时，有 $$ 2 \\cdot rawDepthB = rawDepthA + rawDepthC \\newline \\frac 2 {linearDepthB} = \\frac 1 {linearDepthA} + \\frac 1 {linearDepthC} $$ 这里也稍微证明一下，如下图所示，ABC三点通过O点透视投影到了A\u0026rsquo;B\u0026rsquo;C\u0026rsquo;三点：\n由于ABC三点共线，A\u0026rsquo;B\u0026rsquo;C\u0026rsquo;三点共线，不妨假设\n$$ \\begin{gather} k * \\overrightharpoon{OA\u0026rsquo;} + (1 - k) * \\overrightharpoon{OC\u0026rsquo;} = \\overrightharpoon{OB\u0026rsquo;} \\\\ K * \\overrightharpoon{OA} + (1 - K) * \\overrightharpoon{OC} = \\overrightharpoon{OB} \\\\ \\overrightharpoon{OA} = a * \\overrightharpoon{OA\u0026rsquo;} \\\\ \\overrightharpoon{OB} = b * \\overrightharpoon{OB\u0026rsquo;} \\\\ \\overrightharpoon{OC} = c * \\overrightharpoon{OC\u0026rsquo;} \\\\ \\end{gather} $$\n将（3）（4）（5）带入（2）可以得到： $$ \\begin{gather} K * a * \\overrightharpoon{OA\u0026rsquo;} + (1 - K) * c * \\overrightharpoon{OC\u0026rsquo;} = b * \\overrightharpoon{OB\u0026rsquo;} \\\\ \\end{gather} $$\n考虑到向量的性质，结合（1）和（6）可以得到： $$ \\begin{gather} \\frac {K * a} b = k \\\\ \\frac {(1 - K) * c} b = 1 - k \\\\ \\end{gather} $$\n由于我们只关心屏幕空间的插值，（7）（8）两式消去K有： $$ \\begin{gather} \\frac {k * b} a = K \\\\ \\frac {(1 - k) * b} c = {1 - K} \\\\ k * \\frac 1 a + (1 - k) * \\frac 1 c = \\frac 1 b \\\\ \\end{gather} $$\n（11）式即表明，对屏幕空间共线的三点，我们可以对其中两点的线性深度的倒数进行插值得到第三点的线性深度的倒数，亦即，对其中两点的深度值线性插值可以得到第三点的深度值。当然了，在实际计算中可能会有浮点数精度不足的问题。\nReconstructNormalComputeShader.compute 使用GatherRed的方法，可以减少ReconstructNormalAccurate所需要的的采样，但是在屏幕的边缘会有一些瑕疵，把采样的sampler改成sampler_LinearRepeat在一定程度上能够解决这些瑕疵。这样的话ReconstructNormalFast需要两次采样，ReconstructNormalAccurate则需要五次采样。 要注意使用边缘信息对法线进行插值的方法，需要先对法线进行归一化，不然叉乘导致前后平面计算出的向量长度会远大于同一平面的向量长度，影响最终的法线。\n#pragma kernel ReconstructNormalFast #pragma kernel ReconstructNormalAccurate #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #define FEWER_SAMPLES 0 Texture2D\u0026lt;float\u0026gt; _DepthTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_NormalTexture; SamplerState sampler_LinearClamp; SamplerState sampler_LinearRepeat; float4 _TextureSize; float3 GetViewSpacePosition(float2 uv, float depth) { #if UNITY_UV_STARTS_AT_TOP uv.y = 1.0 - uv.y; #endif float3 positionNDC = float3(uv * 2.0 - 1.0, depth); float4 positionVS = mul(UNITY_MATRIX_I_P, float4(positionNDC, 1.0)); positionVS /= positionVS.w; return positionVS.xyz; } //-UNITY_MATRIX_P._m11 = rcp(tan(fovy / 2)) float3 GetViewSpacePositionFromLinearDepth(float2 uv, float linearDepth) { #if UNITY_UV_STARTS_AT_TOP uv.y = 1.0 - uv.y; #endif float2 uvNDC = uv * 2.0 - 1.0; return float3(uvNDC * linearDepth * UNITY_MATRIX_I_P._m00_m11, -linearDepth); } //Calculate 4 linear eye depths at one time float4 LinearEyeDepthFloat4(float4 depthTBLR, float4 zBufferParams) { return rcp(depthTBLR * zBufferParams.z + zBufferParams.w); } //Heavily based on normal reconstruction method in github repository GameTechDev/XeGTAO. //https://github.com/GameTechDev/XeGTAO/blob/0d177ce06bfa642f64d8af4de1197ad1bcb862d4/Source/Rendering/Shaders/XeGTAO.hlsli#L143-L160 [numthreads(8,8,1)] void ReconstructNormalFast (uint3 dispatchThreadID : SV_DispatchThreadID) { float4 depthGatherBL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw); float4 depthGatherTR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(1, 1)); float depthC = depthGatherBL.y; float depthT = depthGatherTR.x; float depthB = depthGatherBL.z; float depthL = depthGatherBL.x; float depthR = depthGatherTR.z; float linearDepth = LinearEyeDepth(depthC, _ZBufferParams); float4 linearDepths = LinearEyeDepthFloat4(float4(depthT, depthB, depthL, depthR), _ZBufferParams); float4 depthDifferenceTBLR = linearDepths - linearDepth; float slopeTB = (depthDifferenceTBLR.x - depthDifferenceTBLR.y) * 0.5; float slopeLR = (depthDifferenceTBLR.w - depthDifferenceTBLR.z) * 0.5; float4 depthDifferenceTBLRAverage = depthDifferenceTBLR + float4(-slopeTB, slopeTB, slopeLR, -slopeLR); depthDifferenceTBLR = min(abs(depthDifferenceTBLR), abs(depthDifferenceTBLRAverage)); //0: edge; 1: non-edge float4 edgesTBLR = saturate(1.25 - depthDifferenceTBLR / (linearDepth * 0.011)); //TL, TR, BR, BL float4 acceptedNormals = saturate(float4(edgesTBLR.x * edgesTBLR.z, edgesTBLR.w * edgesTBLR.x, edgesTBLR.y * edgesTBLR.w, edgesTBLR.z * edgesTBLR.y) + 0.001); float3 viewPosC = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 0.5)) * _TextureSize.zw, depthC); float3 viewPosT = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 1.5)) * _TextureSize.zw, depthT); float3 viewPosB = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, -0.5)) * _TextureSize.zw, depthB); float3 viewPosL = GetViewSpacePosition((dispatchThreadID.xy + float2(-0.5, 0.5)) * _TextureSize.zw, depthL); float3 viewPosR = GetViewSpacePosition((dispatchThreadID.xy + float2(1.5, 0.5)) * _TextureSize.zw, depthR); float3 t = normalize(viewPosT - viewPosC); float3 b = normalize(viewPosC - viewPosB); float3 l = normalize(viewPosC - viewPosL); float3 r = normalize(viewPosR - viewPosC); float3 normalVS = acceptedNormals.x * cross(l, t) + acceptedNormals.y * cross(r, t) + acceptedNormals.z * cross(r, b) + acceptedNormals.w * cross(l, b); normalVS = normalize(normalVS); _RW_NormalTexture[dispatchThreadID.xy] = float4(normalVS, 1.0); } //Heavily based on github gist bgolus/WorldNormalFromDepthTexture.shader //https://gist.github.com/bgolus/a07ed65602c009d5e2f753826e8078a0#file-worldnormalfromdepthtexture-shader-L153-L218 //https://atyuwen.github.io/posts/normal-reconstruction/ [numthreads(8,8,1)] void ReconstructNormalAccurate (uint3 dispatchThreadID : SV_DispatchThreadID) { #if FEWER_SAMPLES float4 depthGatherTL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(-1, 1)); float4 depthGatherTR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(1, 2)); float4 depthGatherBR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(2, 0)); float4 depthGatherBL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(0, -1)); float depthC = _DepthTexture.Load(int3(dispatchThreadID.xy, 0)); float depthT = depthGatherTR.w; float depthB = depthGatherBL.y; float depthL = depthGatherTL.z; float depthR = depthGatherBR.x; float depthT2 = depthGatherTR.x; float depthB2 = depthGatherBL.z; float depthL2 = depthGatherTL.w; float depthR2 = depthGatherBR.y; #else float4 depthGatherBL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw); float4 depthGatherTR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(1, 1)); float depthC = depthGatherBL.y; float depthT = depthGatherTR.x; float depthB = depthGatherBL.z; float depthL = depthGatherBL.x; float depthR = depthGatherTR.z; float depthT2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(0, 2), 0)); float depthB2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(0, -2), 0)); float depthL2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(-2, 0), 0)); float depthR2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(2, 0), 0)); #endif float3 viewPosC = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 0.5)) * _TextureSize.zw, depthC); float3 viewPosT = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 1.5)) * _TextureSize.zw, depthT); float3 viewPosB = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, -0.5)) * _TextureSize.zw, depthB); float3 viewPosL = GetViewSpacePosition((dispatchThreadID.xy + float2(-0.5, 0.5)) * _TextureSize.zw, depthL); float3 viewPosR = GetViewSpacePosition((dispatchThreadID.xy + float2(1.5, 0.5)) * _TextureSize.zw, depthR); float3 t = viewPosT - viewPosC; float3 b = viewPosC - viewPosB; float3 l = viewPosC - viewPosL; float3 r = viewPosR - viewPosC; float4 H = float4(depthL, depthR, depthL2, depthR2); float4 V = float4(depthB, depthT, depthB2, depthT2); float2 he = abs((2 * H.xy - H.zw) - depthC); float2 ve = abs((2 * V.xy - V.zw) - depthC); float3 hDeriv = he.x \u0026lt; he.y ? l : r; float3 vDeriv = ve.x \u0026lt; ve.y ? b : t; float3 normalVS = normalize(cross(hDeriv, vDeriv)); _RW_NormalTexture[dispatchThreadID.xy] = float4(normalVS, 1.0); } 最后的思考 本来还想使用3x3的采样，使用类似于吴彧文的方法，延伸第三个点到当前像素来计算准确的法线的，但是实际操作了一下发现，只要四个点构成了平行四边形就会认为是接近于当前采样，于是就会导致计算出错误的法线了。XeGTAO里面使用的计算法线的方式确实很巧妙，应该多用用，之后可能会再写一篇计算GTAO的文章吧。\n","permalink":"https://zznewclear13.github.io/posts/get-view-space-normal-from-depth-texture/","summary":"为什么要从深度图重建视空间法线 一个很大的应用情景是在后处理的阶段，或是计算一些屏幕空间的效果（如SSR、SSAO等），只能获取到一张深度贴图，而不是每一个几何体的顶点数据，很多的计算中却又需要用到世界空间的法线或者是视空间的法线，这时我们就需要通过深度图来重建视空间的法线。（诶这段话我是不是写过一遍了）\n重建视空间法线的方法 bgolus在他的WorldNormalFromDepthTexture.shader里面很全面的介绍了各种重建视空间法线的方法。其中比较值得注意的是来自Janos Turanszki的根据深度差判断当前像素属于哪个平面的方法，和来自吴彧文的横向和纵向多采样一个点来判断当前像素属于哪个平面的方法，其中吴彧文的方法能够在绝大部分情况下获取到最准确的法线（除了尖角的一个像素）。\n除了bgolus介绍的方法之外，我在GameTechDev/XeGTAO中还看到了一种方法。这种方法类似于Janos Turanszki的深度差的方法，不过从深度差中获取的是0-1的边缘值（edgesLRTB，edgesLRTB.x越接近0即代表该像素的左侧越是一条边缘），再使用边缘的两两乘积对四个法线进行插值，最终计算出视空间法线。我个人认为当在两个面相接的地方不需要特别准确的法线值时，这是最好的计算法线的方法。用这个方式计算的法线，在两个面相接的地方，法线会有一种从一个面插值到另一个面的效果（且一定程度上抗锯齿），在两个面远近排布的时候，也能获取到准确的法线。\n具体的实现方法 根据需要使用的方法，采样深度图。在采样比较集中的情况下，可以使用GatherRed方法来减少采样的次数。GatherRed可以得到双线性采样时的四个像素的R通道的值并封装到一个float4中，当屏幕左下角是(0, 0)时，这个float4的x分量对应采样点左上角的颜色的R通道的值，y对应右上角，z对应右下角，w对应左下角，可以在HLSL的文档中看到Gather的相关介绍。Compute Shader的话可以使用group shared memory进一步减少采样。 使用深度图和当前的uv值计算出像素的视空间的坐标，这一步尤其需要注意视空间坐标Z分量的正负性的问题。Unity的视空间变换矩阵UNITY_MATRIX_V是摄像机位于视空间(0, 0, 0)，看向视空间Z轴负方向的，右手系的矩阵。即视空间的坐标Z分量往往是一个负值，其法线的Z分量在往往下是正值（即画面看上去应该多为蓝色）。 从深度图中计算视空间坐标的时候，如果Unity版本比较旧，会没有UNITY_MATRIX_I_P这个矩阵，这时可以使用unity_CameraInvProjection来代替，但需要注意DirectX平台UV上下翻转的问题。 当屏幕左下角是(0, 0)时，使用右侧的视空间坐标减去左侧的视空间坐标，使用上侧的视空间坐标减去下侧的视空间坐标。五个采样点（包括位于中心的当前像素）可以获得四个向量，对于右手系的视空间坐标来说，将这四个向量按照水平向量叉乘竖直向量的顺序，就可以获得四个当前像素的法线了。 最后使用前面介绍的获取法线的方法，从这四个法线中获取最为正确的法线。这些方法往往都会使用深度值来进行判断，这里需要注意的是透视变换带来的深度的非线性的问题。对于屏幕上等距分布的三个点ABC，当他们在世界空间中处于同一条直线时，有 $$ 2 \\cdot rawDepthB = rawDepthA + rawDepthC \\newline \\frac 2 {linearDepthB} = \\frac 1 {linearDepthA} + \\frac 1 {linearDepthC} $$ 这里也稍微证明一下，如下图所示，ABC三点通过O点透视投影到了A\u0026rsquo;B\u0026rsquo;C\u0026rsquo;三点：\n由于ABC三点共线，A\u0026rsquo;B\u0026rsquo;C\u0026rsquo;三点共线，不妨假设\n$$ \\begin{gather} k * \\overrightharpoon{OA\u0026rsquo;} + (1 - k) * \\overrightharpoon{OC\u0026rsquo;} = \\overrightharpoon{OB\u0026rsquo;} \\\\ K * \\overrightharpoon{OA} + (1 - K) * \\overrightharpoon{OC} = \\overrightharpoon{OB} \\\\ \\overrightharpoon{OA} = a * \\overrightharpoon{OA\u0026rsquo;} \\\\ \\overrightharpoon{OB} = b * \\overrightharpoon{OB\u0026rsquo;} \\\\ \\overrightharpoon{OC} = c * \\overrightharpoon{OC\u0026rsquo;} \\\\ \\end{gather} $$","title":"从深度图中获取视空间的法线"},{"content":"动机和想要实现的效果 最直接的动机是看了顽皮狗在Siggraph 2016上的PPT，里面介绍了顽皮狗在神秘海域中是如何让植被随风飘荡的。他们介绍了一种将植被的每一部分的pivot的物体空间坐标写到顶点色里，然后在shader中使用这个坐标进行风的效果的计算的方法。较为震撼在风吹过草原时，植被进行弯曲后，草表面的高光会有一种时空上的起伏感（也就是说神秘海域的植被的法线也会被风影响）。所以我也想要借助写pivot的方法来制作植被受到风吹的效果，通过这个方法计算出正确的风吹之后的植被的法线（同时由于法线贴图的存在，还要计算正确的切线）。\n稍微翻了一下网上的资料（也没仔细地去搜索），大部分的就是一个普通的顶点动画，有的是用的sin，有的就直接平移。这就产生了第二个需求，植被在顶点动画中应该保持差不多的长度，不然会发现很明显的拉伸的效果。\n当然最好还能投射出正常的影子了，这一步只需要把顶点着色器复制一份到投射影子的pass里就可以了。\n这里使用的植被模型是MegaScans上的CORDYLINE模型中的var12这个小模型。\n难点和相对应的应对方法 Unity的顶点色限制 稍微测试一下就能发现，Unity的顶点色是UNorm8的格式，也就是说无论你在Maya或是3ds Max里导出的模型的顶点色信息是什么样的，导入到Unity中就会变成只有256精度的UNorm8。顽皮狗使用的是自己的引擎，所以它们能够使用全精度的顶点色，但是由于Unity的引擎限制，我们可以考虑到导出pivot的顶点坐标到模型的UV中。\n但是很不幸的是，fbx导入到Unity时，即使UV是float4的类型（也就是16bytes)，在Unity中只会识别UV的前两位。所以只能无奈的将pivot的顶点坐标（float3的数据）储存到两个UV的三个通道里，同时将pivot的层级存到剩下的一个通道里。我不知道顽皮狗具体是怎么计算pivot的层级关系的，他在PPT中写的是无需计算，但我在实际操作中只能一层一层的算（而且只能算两层），也希望知道具体怎么操作的人告知一下方法。\n所以接下来要做的是在Maya中把pivot的物体空间坐标和pivot的层级写到对应顶点的某两套UV中，本文是写到第二套和第三套UV中（也就是TEXCOORD1和TEXCOORD2）。于是我恶补了一下maya的python脚本的写法，不过在写数值到UV中时，又遇到了一个小问题。Maya的cmds.polyEditUV这个方法，明明能传入uvSetName这个参数，用于操作对应的UV，但我实际使用时只能写数值到当前的UV中，导致最后写的脚本只能僵硬的操作当前UV，每次切换UV时需要重新修改脚本再运行一次。\n最终的脚本是这样的：\nVertexPivotWriteTool.py import maya.cmds as cmds targetVertexStr = \u0026#34;Select any vertex to start.\u0026#34; vertexColorStr = \u0026#34;Select any vertex to start.\u0026#34; pivotPosition = [0.0, 0.0, 0.0] def ui(): if cmds.window(\u0026#34;VertexPivotWriteTool\u0026#34;, exists = True): cmds.deleteUI(\u0026#34;VertexPivotWriteTool\u0026#34;) global targetVertexStr global targetVertexField global vertexColorStr global vertexColorField global pivotLayer vertexPivotWindow = cmds.window(\u0026#34;VertexPivotWriteTool\u0026#34;, widthHeight = [500, 400]) form = cmds.formLayout(numberOfDivisions = 100) pivotLayerLable = cmds.text(\u0026#34;Pivot Layer (0 for root pivot)\u0026#34;) pivotLayer = cmds.intField() cmds.intField(pivotLayer, e = True, minValue = 0, maxValue = 5, step = 1, value = 0) targetVertexButton = cmds.button(\u0026#34;Target Vertex\u0026#34;, command = \u0026#39;GetTargetVertex()\u0026#39;) targetVertexField = cmds.textField(text=targetVertexStr, width = 300) #writeVertexButton = cmds.button(\u0026#34;Write to Vertex Color\u0026#34;, command = \u0026#39;WriteToVertexColor()\u0026#39;) writeVertexButton = cmds.button(\u0026#34;Write to Vertex Texcoord\u0026#34;, command = \u0026#39;WriteToVertexTexcoord()\u0026#39;) targetVertexColorButton = cmds.button(\u0026#34;Show Vertex Color\u0026#34;, command = \u0026#39;GetTargetVertexColor()\u0026#39;) vertexColorField = cmds.textField(text=vertexColorStr, width = 300) cmds.showWindow(vertexPivotWindow) cmds.formLayout(form, e=True, attachForm = ( [pivotLayerLable, \u0026#39;left\u0026#39;, 25], [pivotLayerLable, \u0026#39;top\u0026#39;, 20], [pivotLayer, \u0026#39;right\u0026#39;, 25], [pivotLayer, \u0026#39;top\u0026#39;, 20], [targetVertexButton, \u0026#39;left\u0026#39;, 25], [targetVertexButton, \u0026#39;top\u0026#39;, 60], [targetVertexField, \u0026#39;right\u0026#39;, 25], [targetVertexField, \u0026#39;top\u0026#39;, 60], [writeVertexButton, \u0026#39;left\u0026#39;, 25], [writeVertexButton, \u0026#39;top\u0026#39;, 100], [targetVertexColorButton, \u0026#39;left\u0026#39;, 25], [targetVertexColorButton, \u0026#39;bottom\u0026#39;, 20], [vertexColorField, \u0026#39;right\u0026#39;, 25], [vertexColorField, \u0026#39;bottom\u0026#39;, 20], )) def GetPivotLayer(): value = cmds.intField(pivotLayer, q=True, value=True) print(\u0026#34;pivotLayerValue is: \u0026#34; + str(value)) return value def GetTargetVertex(): print(\u0026#34;Get Target Vertex...\u0026#34;) selVertices = cmds.ls(selection = True) global targetVertexStr global pivotPosition if len(selVertices) == 0: targetVertexStr = \u0026#34;No vetex selected!\u0026#34; elif len(selVertices) \u0026gt;= 2: targetVertexStr = \u0026#34;Too many vertices selected! Expected 1, got \u0026#34; + str(len(selVertices)) else: pivotPosition = cmds.pointPosition(selVertices[0]) tempStr = \u0026#34;(\u0026#34; for axis in range(len(pivotPosition)): if axis \u0026gt;= 1: tempStr += \u0026#34;, \u0026#34; tempStr += \u0026#34;{:.2f}\u0026#34;.format(pivotPosition[axis]) tempStr += \u0026#34;)\u0026#34; targetVertexStr = tempStr cmds.textField(targetVertexField, e= True, text = targetVertexStr) def GetTargetVertexColor(): print(\u0026#34;Get Target Vertex...\u0026#34;) selVertices = cmds.ls(selection = True) global vertexColorStr if len(selVertices) == 0: vertexColorStr = \u0026#34;No vetex selected!\u0026#34; elif len(selVertices) \u0026gt;= 2: vertexColorStr = \u0026#34;Too many vertices selected! Expected 1, got \u0026#34; + str(len(selVertices)) else: vertexColor = cmds.polyColorPerVertex(query=True, rgb=True) tempStr = \u0026#34;(\u0026#34; for axis in range(len(vertexColor)): if axis \u0026gt;= 1: tempStr += \u0026#34;, \u0026#34; tempStr += \u0026#34;{:.2f}\u0026#34;.format(vertexColor[axis]) tempStr += \u0026#34;)\u0026#34; vertexColorStr = tempStr cmds.textField(vertexColorField, e= True, text = vertexColorStr) def WriteToVertexColor(): print(\u0026#34;Write To Vertex Color...\u0026#34;) selVertices = cmds.ls(selection = True) for vertex in selVertices: cmds.polyColorPerVertex(vertex, rgb=(pivotPosition[0], pivotPosition[1], pivotPosition[2])) def WriteToVertexTexcoord(): print(\u0026#34;Write To Vertex Coord...\u0026#34;) pivotLayerValue = GetPivotLayer() allUVSets = cmds.polyUVSet( query=True, allUVSets=True ) uvSetCount = len(allUVSets) cmds.polyEditUV(relative = False, uValue = pivotPosition[0], vValue = pivotPosition[1]) #cmds.polyEditUV(relative = False, uValue = pivotPosition[2], vValue = pivotLayerValue) ui() 因为种种限制，使用时较为复杂，如果有更好的脚本的话，也很感谢分享出来告诉我。首先是要在UV集编辑器中，为模型新增两套UV，由于使用的MegaScans模型本身有两套不同的UV，操作是把原来的第二套UV移动到第四套UV中，然后把第一套UV复制到第二第三套UV中，然后在UV编辑器中定位当前UV到第二套UV。在脚本编辑器中打开或者复制上面的VertexPivotWriteTool.py，通过Crtl + Enter可以生成该脚本的一个窗口。然后执行下述操作：首先是将每个pivot的前两个坐标写到第二套UV中，对茎来说，其pivot是最底下的顶点，对叶片来说，其pivot是最接近茎的顶点，选中这个顶点然后点击Target Vertex，在窗口中可以看到这个顶点的物体空间的坐标；然后在UV编辑器选中该茎或者叶片的UV壳，点击Write to Vertex Texcoord，在UV编辑器中可以看到UV坍缩成了一个点（往往找不到）；对第二套UV中的所有的UV壳执行上述操作；然后将当前UV切换到第三套UV，同时注释掉脚本的第117行，取消注释脚本的第118行，然后输入Crtl + Enter重新生成一遍工具；这时我们将要把每个pivot的最后一个坐标和pivot的层级写到第三套UV中；对植被的每一片叶子和枝干，判断其pivot的层级（以现在使用的MegaScans模型为例，茎的层级是0，其他叶片的层级是1），在Pivot Layer中输入层级；然后重复判断层级，选择顶点，写入UV；最后最后，不要忘记把脚本还原成最开始的样子。这样就把每个顶点对应的pivot坐标写入到第二和第三套UV了！导出到Unity就可以了。\n如何计算拉伸较小的风的效果，并且计算对应的法线 首先来看这样一张图：\n这张图表现了在Bend Space中把红色的线段弯曲到绿色线段的算法，X轴是风的方向，可以看到风的强度越高，Radius的大小就越小。同时为了计算出正确的法线和切线，需要同样的计算出AxisX和AxisZ在Bend Space中的向量。使用这个算法，当模型处在Bend Space的Z轴上时，不会受到扭曲，当其X轴大于0时，会受到压缩，当X轴小于0时，会受到拉伸。同样的，这种算法可以推广到三维空间中，同时扭曲Y轴和Z轴，我特地写了一个C#脚本来对变换的结果进行可视化。\nWindDebugger.cs using UnityEngine; using Unity.Mathematics; public class WindDebugger : MonoBehaviour { public bool draw = true; public float debugRadius = 0.01f; public float debugLength = 0.2f; public Color pivotColor; public Color sphereColor; public float radius; public float3 originalPosition; private void DrawAxes(Color color, Vector3 pos, Vector3 tangent, Vector3 bitangent, Vector3 normal) { Gizmos.color = color; Gizmos.DrawSphere(pos, debugRadius); Gizmos.color = Color.red; Gizmos.DrawLine(pos, pos + tangent * debugLength); Gizmos.color = Color.green; Gizmos.DrawLine(pos, pos + bitangent * debugLength); Gizmos.color = Color.blue; Gizmos.DrawLine(pos, pos + normal * debugLength); } private float3 CircleTransform(float3 positionBS, float radius, out float3 axisX, out float3 axisY, out float3 axisZ) { float radVal = math.length(positionBS.xy) / radius; float sinVal = math.sin(radVal); float cosVal = math.cos(radVal); float2 normalizeDir = math.normalize(positionBS.xy); float3 targetPosBS = new float3((radius * sinVal) * normalizeDir, radius - radius * cosVal); float3 tempAxisX = new float3(-sinVal * normalizeDir, cosVal); float3 tempAxisY = new float3(normalizeDir.y, -normalizeDir.x, 0.0f); float3 tempAxisZ = new float3(cosVal * normalizeDir, sinVal); axisX = tempAxisX; axisY = normalizeDir.y * tempAxisY + normalizeDir.x * tempAxisZ; axisZ = -normalizeDir.x * tempAxisY + normalizeDir.y * tempAxisZ; float3 newPositionBS = targetPosBS + axisX * positionBS.z; return newPositionBS; } private void OnDrawGizmos() { if (!draw) { return; } Color originalColor = Gizmos.color; Gizmos.color = sphereColor; Gizmos.DrawSphere(new float3(0.0f, 0.0f, radius), radius); float3 axisX, axisY, axisZ; float3 newPosition = CircleTransform(originalPosition, radius, out axisX, out axisY, out axisZ); DrawAxes(pivotColor, newPosition, axisY, axisZ, axisX); DrawAxes(pivotColor, originalPosition, new float3(1.0f, 0.0f, 0.0f), new float3(0.0f, 1.0f, 0.0f), new float3(0.0f, 0.0f, 1.0f)); Gizmos.color = Color.black; Gizmos.DrawLine(float3.zero, originalPosition); Gizmos.color = Color.white; Gizmos.DrawLine(float3.zero, newPosition); Gizmos.color = originalColor; } } 其他的一些问题 由于整个计算过程中用到了很多的坐标变换，需要特别的注意每一次变换是从什么空间变换到什么空间。首先是物体空间到风的弯曲空间，由于我们CircleTransform方法是认为风是吹向X轴正方向的，所以需要先对所有的坐标、向量进行一个变换，由于只涉及到旋转，所以可以用一个float3x3的矩阵来表示从物体空间到弯曲空间的变换矩阵。\n然后分两种情况：一种是Pivot Layer为0的顶点，也就是所使用的模型的茎上的顶点。这种相对简单，将顶点在物体Bend Space中进行CircleTransform后，就能获得新的Bend Space的坐标和新的三个轴的向量（新的三个轴可以组合出顶点的Bend Space到物体的Bend Space的变换矩阵），可以计算出顶点、法线和切线在物体Bend Space的坐标和向量。最后再从物体Bend Space转换到物体空间就可以了。\n第二种是Pivot Layer为1的顶点，要先计算出Pivot的新的物体Bend Space坐标，然后在其基础上计算出每一个顶点相对于Pivot Bend Space的新的坐标，然后一层套一层的算回顶点及其法线切线在物体Bend Space的坐标。最后再从物体Bend Space转换到物体空间就可以了。值得一提的是，我在计算pivot的Bend Space时，所使用的空间和之前图上不太一样，是AxisZ, AxisY和-AxisX对应新的Bend Space的XYZ轴，这样能让垂直于枝干的叶片有更好的风吹的效果。\n为了让随风摆动的效果看上去更自然，除了按照圆形来变换顶点之外，参考顽皮狗的演讲，还要给枝干的摇晃添加一个和距离相关的延迟，这样不会显得生硬。至于随风飘动的频率，就随便找一个sin函数的组合就可以了。\n具体代码和相关的思考 顶点着色器就按照之前介绍的来做就可以了。由于较好看的植被都是双面渲染的，在Cull的参数里面选择Off。这样同样的会遇到一个问题，就是模型背面的法线和正面的法线是相同的，这里需要使用HLSL片元着色器的VFACE语义，来判断当前面是正面还是背面，如果是背面的话需要反转一下法线。这里写的Shader也同时写了阴影、深度图和烘焙所需要的pass。\nVertexAnimationPlantShader.shader Shader \u0026#34;zznewclear13/VertexAnimationPlantShader\u0026#34; { Properties { _BaseColor (\u0026#34;Base Color\u0026#34;, color) = (1, 1, 1, 1) _BaseMap(\u0026#34;Base Map\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BumpMap (\u0026#34;Bump Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _BumpIntensity (\u0026#34;Bump Intensity\u0026#34;, range(0, 1)) = 1 _RoughnessMap(\u0026#34;Roughness Map\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _RoughnessIntensity (\u0026#34;Roughness Intensity\u0026#34;, range(0, 1)) = 1 _MetallicMap (\u0026#34;Metallic Map\u0026#34;, 2D) = \u0026#34;black\u0026#34; {} _MetallicIntensity (\u0026#34;Metallic Intensity\u0026#34;, range(0, 1)) = 1 _WindDirection (\u0026#34;Wind Direction\u0026#34;, vector) = (1.0, 0.0, 0.0, 0.0) _WindIntensity (\u0026#34;Wind Intensity\u0026#34;, float) = 1 _WindVariety (\u0026#34;Wind Variety\u0026#34;, range(0, 10)) = 0.5 _BranchDelay (\u0026#34;Branch Delay\u0026#34;, range(0, 10)) = 2 _WindVarietyLeaves (\u0026#34;Wind Variety Leaves\u0026#34;, range(0, 10)) = 1 _BranchDelayLeaves (\u0026#34;Branch Delay Leaves\u0026#34;, range(0, 10)) = 3 _WindVaryFrequency (\u0026#34;Wind Vary Frequency\u0026#34;, float) = 5 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; sampler2D _BaseMap; sampler2D _BumpMap; sampler2D _RoughnessMap; sampler2D _MetallicMap; CBUFFER_START(UnityPerMaterial) float4 _BaseColor; float4 _BaseMap_ST; float _BumpIntensity; float _RoughnessIntensity; float _MetallicIntensity; float4 _WindDirection; float _WindIntensity; float _WindVariety; float _BranchDelay; float _WindVarietyLeaves; float _BranchDelayLeaves; float _WindVaryFrequency; CBUFFER_END //Apply wind variety float GetVariety(float timeFunction) { return sin(timeFunction) + 0.25 * sin(timeFunction * 1.5) + 0.1 * sin(timeFunction * 0.33); } //CircleTransform transforms a current bend space point to a new position in bend space, //and output three axes of next bend space. //New position is in current bend space and ready for use. //Normal and tangent in current bend space can be calculated by axes. float3 CircleTransform(float3 positionBS, float windIntensity, out float3 axisX, out float3 axisY, out float3 axisZ) { float intensity = windIntensity; if(intensity == 0.0 || length(positionBS.yz) == 0.0) { axisX = float3(1.0, 0.0, 0.0); axisY = float3(0.0, 1.0, 0.0); axisZ = float3(0.0, 0.0, 1.0); return positionBS; } float radius = rcp(intensity); float radVal = length(positionBS.yz) * intensity; float sinVal = sin(radVal); float cosVal = cos(radVal); float2 normalizeDir = normalize(positionBS.yz); float3 targetPosBS = float3(radius - radius * cosVal, (radius * sinVal) * normalizeDir); float3 tempAxisX = float3(cosVal, -sinVal * normalizeDir); float3 tempAxisY = float3(0.0, normalizeDir.y, -normalizeDir.x); float3 tempAxisZ = float3(sinVal, cosVal * normalizeDir); axisX = tempAxisX; axisY = normalizeDir.y * tempAxisY + normalizeDir.x * tempAxisZ; axisZ = -normalizeDir.x * tempAxisY + normalizeDir.y * tempAxisZ; float3 newPositionBS = targetPosBS + axisX * positionBS.x; return newPositionBS; } //windDirection: object space, upVec: world upVec in object space void InitBendSpace(float3 windDirection, float3 upVec, out float3x3 objectToBend, out float3x3 bendToObject) { float3 u = windDirection; float3 v = normalize(cross(upVec, u)); float3 w = cross(u, v); //Object space to bend space objectToBend = float3x3(u, v, w); //Bend space to object space bendToObject = float3x3(u.x, v.x, w.x, u.y, v.y, w.y, u.z, v.z, w.z); } ENDHLSL SubShader { Tags{ \u0026#34;RenderType\u0026#34; = \u0026#34;Transparent\u0026#34; \u0026#34;Queue\u0026#34; = \u0026#34;Transparent\u0026#34;} Pass { Name \u0026#34;ForwardLit\u0026#34; Tags{\u0026#34;LightMode\u0026#34; = \u0026#34;UniversalForward\u0026#34;} Cull Off Blend SrcAlpha OneMinusSrcAlpha ZWrite On HLSLPROGRAM #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; #pragma shader_feature_local _NORMALMAP #pragma multi_compile _ _MAIN_LIGHT_SHADOWS _MAIN_LIGHT_SHADOWS_CASCADE _MAIN_LIGHT_SHADOWS_SCREEN #pragma multi_compile _ LIGHTMAP_SHADOW_MIXING #pragma multi_compile _ SHADOWS_SHADOWMASK #pragma multi_compile_fragment _ _SHADOWS_SOFT #pragma multi_compile _ LIGHTMAP_ON #pragma vertex LitPassVert #pragma fragment LitPassFrag struct Attributes { float4 positionOS : POSITION; float3 normalOS : NORMAL; float4 tangentOS : TANGENT; float2 texcoord0 : TEXCOORD0; float2 texcoord1 : TEXCOORD1; float2 texcoord2 : TEXCOORD2; float2 staticLightmapUV : TEXCOORD3; UNITY_VERTEX_INPUT_INSTANCE_ID }; struct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; float3 positionWS : TEXCOORD1; float3 normalWS : TEXCOORD2; float4 tangentWS : TEXCOORD3; float4 shadowCoord : TEXCOORD4; DECLARE_LIGHTMAP_OR_SH(staticLightmapUV, vertexSH, 5); UNITY_VERTEX_INPUT_INSTANCE_ID UNITY_VERTEX_OUTPUT_STEREO }; ////////////////////////////////// //GGX BRDF and related functions// ////////////////////////////////// float D(float ndoth, float roughness) { float a = ndoth * roughness; float k = roughness / (1.0 - ndoth * ndoth + a * a); return k * k; } float G(float ndotl, float ndotv, float roughness) { float a2 = roughness * roughness; float gv = ndotv * sqrt((1.0 - a2) * ndotl * ndotl + a2); float gl = ndotl * sqrt((1.0 - a2) * ndotv * ndotv + a2); return 0.5 * rcp(gv + gl); } float3 F(float3 specular, float hdotl) { return specular + (1 - specular) * pow(1 - hdotl, 5); } float3 GGXBRDF(float3 wi, float3 wo, float3 normal, float3 specular, float roughness) { float3 h = normalize(wi + wo); float ndotv = max(dot(normal, wo), 1e-5); float ndoth = max(dot(normal, h), 0.0); float ndotl = max(dot(normal, wi), 0.0); float hdotl = max(dot(h, wi), 0.0); float d = D(ndoth, roughness); float g = G(ndotl, ndotv, roughness); float3 f = F(specular, hdotl); return d * g * f; } Varyings LitPassVert(Attributes input) { Varyings output = (Varyings)0; UNITY_SETUP_INSTANCE_ID(input); UNITY_TRANSFER_INSTANCE_ID(input, output); UNITY_INITIALIZE_VERTEX_OUTPUT_STEREO(output); //Pivot positions are stored in TEXCOORD1.xy and TEXCOORD2.x. float3 pivotPosition = float3(input.texcoord1.xy, input.texcoord2.x) * 0.01; float3 pointOffset = input.positionOS.xyz - pivotPosition; //Initialize Bend Space. float3 windDirectionOS = mul((float3x3)UNITY_MATRIX_I_M, _WindDirection.xyz); float3 upVec = mul((float3x3)UNITY_MATRIX_I_M, float3(0.0, 1.0, 0.0)); windDirectionOS = normalize(windDirectionOS); upVec = normalize(upVec); float3x3 objectToBend, bendToObject; InitBendSpace(windDirectionOS, upVec, objectToBend, bendToObject); //Initialize vertex data, transform from object space to bend space. float3 pivotPositionBS = mul(objectToBend, pivotPosition); float3 pointOffsetBS = mul(objectToBend, pointOffset); float3 originalTangentBS = mul(objectToBend, input.tangentOS.xyz); float3 originalNormalBS = mul(objectToBend, input.normalOS); float3 windPointBS; float3 windTangentBS; float3 windNormalBS; //TEXCOORD2.y is used to check pivot layers. 0 is root layer. if(input.texcoord2.y \u0026gt; 0.5) { ////////////////////////////////// //Calculate pivot root transform// ////////////////////////////////// //Get pivot wind intensity. float intensity = _WindIntensity; float magnitude = length(pivotPositionBS); intensity += _WindVariety * GetVariety(_Time.y * _WindVaryFrequency - magnitude * _BranchDelay); //Calculate new position bent by wind in bend space, //and save the transform matrix. float3 axisX, axisY, axisZ; float3 windPivotPositionBS = CircleTransform(pivotPositionBS, intensity, axisX, axisY, axisZ); float4x4 pivotBSToObjBS = float4x4(float4(axisX.x, axisY.x, axisZ.x, windPivotPositionBS.x), float4(axisX.y, axisY.y, axisZ.y, windPivotPositionBS.y), float4(axisX.z, axisY.z, axisZ.z, windPivotPositionBS.z), float4(0.0, 0.0, 0.0, 1.0)); ///////////////////////////// //Calculate point transform// ///////////////////////////// //Switch axes, transform to next bend space (point bend space). pointOffsetBS = float3(pointOffsetBS.z, pointOffsetBS.y, -pointOffsetBS.x); //Get point wind intensity. intensity = abs(axisZ.x) * _WindIntensity;//_WindIntensityLeaves; magnitude = length(pointOffsetBS); intensity += _WindVarietyLeaves * GetVariety(_Time.y * _WindVaryFrequency - magnitude * _BranchDelayLeaves); //Calculate new position bent by wind in bend space, //and save the transform matrix from point bend space to pivot bend space. //This transform matrix can be used to calculate normal and tangent. float3 windPointPositionBS = CircleTransform(pointOffsetBS, intensity, axisX, axisY, axisZ); float4x4 pointBSToPivotBS = float4x4(float4(axisX.x, axisY.x, axisZ.x, windPointPositionBS.x), float4(axisX.y, axisY.y, axisZ.y, windPointPositionBS.y), float4(axisX.z, axisY.z, axisZ.z, windPointPositionBS.z), float4(0.0, 0.0, 0.0, 1.0)); //Switch axes, transform to pivot bend space. windPointPositionBS = float3(-windPointPositionBS.z, windPointPositionBS.y, windPointPositionBS.x); //Calculate position, normal and tangent in pivot bend space. float3 windPointPS = windPointPositionBS; float3 windTangentPS = mul((float3x3)pointBSToPivotBS, originalTangentBS); float3 windNormalPS = mul((float3x3)pointBSToPivotBS, originalNormalBS); //Calculate position, normal and tangent in object bend space. windPointBS = mul(pivotBSToObjBS, float4(windPointPS, 1.0)).xyz; windTangentBS = mul((float3x3)pivotBSToObjBS, windTangentPS); windNormalBS = mul((float3x3)pivotBSToObjBS, windNormalPS); } else { ////////////////////////////////// //Calculate point transform only// ////////////////////////////////// //Get point wind intensity. float intensity = _WindIntensity; float magnitude = length(pointOffsetBS); intensity += _WindVariety * GetVariety(_Time.y * _WindVaryFrequency - magnitude * _BranchDelay); float3 axisX, axisY, axisZ; float3 windPointOffsetBS = CircleTransform(pointOffsetBS, intensity, axisX, axisY, axisZ); float4x4 pointBSToObjBS = float4x4(float4(axisX.x, axisY.x, axisZ.x, windPointOffsetBS.x), float4(axisX.y, axisY.y, axisZ.y, windPointOffsetBS.y), float4(axisX.z, axisY.z, axisZ.z, windPointOffsetBS.z), float4(0.0, 0.0, 0.0, 1.0)); windPointBS = windPointOffsetBS + pivotPositionBS; windTangentBS = mul((float3x3)pointBSToObjBS, originalTangentBS); windNormalBS = mul((float3x3)pointBSToObjBS, originalNormalBS); } //Transform from bend space to object space float3 pivotPositionOS = mul(bendToObject, windPointBS); float3 pivotTangentOS = mul(bendToObject, windTangentBS); float3 pivotNormalOS = mul(bendToObject, windNormalBS); VertexPositionInputs vertexInput = GetVertexPositionInputs(pivotPositionOS); VertexNormalInputs normalInput = GetVertexNormalInputs(pivotNormalOS, float4(pivotTangentOS, input.tangentOS.w)); output.positionCS = vertexInput.positionCS; output.uv = TRANSFORM_TEX(input.texcoord0, _BaseMap); output.positionWS = vertexInput.positionWS; output.normalWS = normalInput.normalWS; output.tangentWS = float4(normalInput.tangentWS, input.tangentOS.w); output.shadowCoord = TransformWorldToShadowCoord(vertexInput.positionWS); OUTPUT_LIGHTMAP_UV(input.staticLightmapUV, unity_LightmapST, output.staticLightmapUV); OUTPUT_SH(normalInput.normalWS.xyz, output.vertexSH); output.normalWS = normalInput.normalWS; return output; } float4 LitPassFrag(Varyings input, float vFace : VFACE) : SV_TARGET { UNITY_SETUP_INSTANCE_ID(input); UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); //wo float3 positionWS = input.positionWS; float3 viewDirWS = GetWorldSpaceNormalizeViewDir(positionWS); //wi float4 shadowCoord = TransformWorldToShadowCoord(positionWS); float4 shadowMask = SAMPLE_SHADOWMASK(input.staticLightmapUV); Light mainLight = GetMainLight(shadowCoord, positionWS, shadowMask); //normal float3 normalMap = UnpackNormal(tex2D(_BumpMap, input.uv)); normalMap.xy *= _BumpIntensity; float3 bitangentWS = cross(input.normalWS, input.tangentWS.xyz) * input.tangentWS.w; float3x3 tbn = float3x3(input.tangentWS.xyz, bitangentWS, input.normalWS); float3 normalWS = mul(normalMap, tbn); normalWS = normalize(input.normalWS); //If we are looking and back faces, revert the normal. normalWS = vFace \u0026gt; 0.5 ? normalWS: -normalWS; //material properties float4 baseMap = tex2D(_BaseMap, input.uv) * _BaseColor; clip(baseMap.a - 0.5); float roughnessMap = tex2D(_RoughnessMap, input.uv).r; float roughness = max(roughnessMap * _RoughnessIntensity, 1e-2); float metallicMap = tex2D(_MetallicMap, input.uv).r; float metallic = metallicMap * _MetallicIntensity; float oneMinusReflectivity = kDieletricSpec.a * (1 - metallic); float reflectivity = 1.0 - oneMinusReflectivity; float3 diffuse = baseMap.rgb * oneMinusReflectivity; float3 specular = lerp(kDieletricSpec.rgb, baseMap.rgb, metallic); //gi float3 bakedGI = SAMPLE_GI(input.staticLightmapUV, input.vertexSH, normalWS); MixRealtimeAndBakedGI(mainLight, normalWS, bakedGI); float3 giDiffuse = bakedGI; float3 reflectVector = reflect(-viewDirWS, normalWS); float3 giSpecular = GlossyEnvironmentReflection(reflectVector, positionWS, roughness, 1.0); //directional lights float3 directDiffuse = diffuse; float3 directSpecular = GGXBRDF(mainLight.direction, viewDirWS, normalWS, specular, roughness); float ndotl = saturate(dot(mainLight.direction, normalWS)); float atten = mainLight.shadowAttenuation; //indirectional lights float3 indirectDiffse = giDiffuse * diffuse; float surfaceReduction = rcp(roughness * roughness + 1.0); float grazingTerm = saturate(1.0 - roughness + reflectivity); float ndotv = saturate(dot(normalWS, viewDirWS)); float fresnelTerm = pow(1.0 - ndotv, 5.0); float3 indirectSpecular = giSpecular * surfaceReduction * lerp(specular, grazingTerm, fresnelTerm); //final compose float3 directBRDF = (directDiffuse + directSpecular) * mainLight.color * atten * ndotl; float3 indirectBRDF = indirectDiffse + indirectSpecular; float3 finalColor = directBRDF + indirectBRDF; return float4(finalColor, baseMap.a); } ENDHLSL } Pass { Name \u0026#34;ShadowCaster\u0026#34; Tags{\u0026#34;LightMode\u0026#34; = \u0026#34;ShadowCaster\u0026#34;} HLSLPROGRAM #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/CommonMaterial.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Shadows.hlsl\u0026#34; #pragma vertex ShadowPassVertex #pragma fragment ShadowPassFragment float3 _LightDirection; struct Attributes { float4 positionOS : POSITION; float3 normalOS : NORMAL; float4 tangentOS : TANGENT; float2 texcoord0 : TEXCOORD0; float2 texcoord1 : TEXCOORD1; float2 texcoord2 : TEXCOORD2; float4 color : COLOR; UNITY_VERTEX_INPUT_INSTANCE_ID }; struct Varyings { float2 uv : TEXCOORD0; float4 positionCS : SV_POSITION; }; Varyings ShadowPassVertex(Attributes input) { Varyings output = (Varyings)0; UNITY_SETUP_INSTANCE_ID(input); UNITY_TRANSFER_INSTANCE_ID(input, output); UNITY_INITIALIZE_VERTEX_OUTPUT_STEREO(output); float3 pivotPosition = float3(input.texcoord1.xy, input.texcoord2.x) * 0.01; float3 pointOffset = input.positionOS.xyz - pivotPosition; float3 windDirectionOS = mul((float3x3)UNITY_MATRIX_I_M, _WindDirection.xyz); float3 upVec = mul((float3x3)UNITY_MATRIX_I_M, float3(0.0, 1.0, 0.0)); windDirectionOS = normalize(windDirectionOS); upVec = normalize(upVec); float3x3 objectToBend, bendToObject; InitBendSpace(windDirectionOS, upVec, objectToBend, bendToObject); float3 pivotPositionBS = mul(objectToBend, pivotPosition); float3 pointOffsetBS = mul(objectToBend, pointOffset); float3 originalTangentBS = mul(objectToBend, input.tangentOS.xyz); float3 originalNormalBS = mul(objectToBend, input.normalOS); float3 windPointBS; float3 windTangentBS; float3 windNormalBS; if(input.texcoord2.y \u0026gt; 0.5) { float intensity = _WindIntensity; float magnitude = length(pivotPositionBS); intensity += _WindVariety * GetVariety(_Time.y * _WindVaryFrequency - magnitude * _BranchDelay); float3 axisX, axisY, axisZ; float3 windPivotPositionBS = CircleTransform(pivotPositionBS, intensity, axisX, axisY, axisZ); float4x4 pivotBSToObjBS = float4x4(float4(axisX.x, axisY.x, axisZ.x, windPivotPositionBS.x), float4(axisX.y, axisY.y, axisZ.y, windPivotPositionBS.y), float4(axisX.z, axisY.z, axisZ.z, windPivotPositionBS.z), float4(0.0, 0.0, 0.0, 1.0)); pointOffsetBS = float3(pointOffsetBS.z, pointOffsetBS.y, -pointOffsetBS.x); intensity = abs(axisZ.x) * _WindIntensity;//_WindIntensityLeaves; magnitude = length(pointOffsetBS); intensity += _WindVarietyLeaves * GetVariety(_Time.y * _WindVaryFrequency - magnitude * _BranchDelayLeaves); float3 windPointPositionBS = CircleTransform(pointOffsetBS, intensity, axisX, axisY, axisZ); float4x4 pointBSToPivotBS = float4x4(float4(axisX.x, axisY.x, axisZ.x, windPointPositionBS.x), float4(axisX.y, axisY.y, axisZ.y, windPointPositionBS.y), float4(axisX.z, axisY.z, axisZ.z, windPointPositionBS.z), float4(0.0, 0.0, 0.0, 1.0)); windPointPositionBS = float3(-windPointPositionBS.z, windPointPositionBS.y, windPointPositionBS.x); float3 windPointPS = windPointPositionBS; float3 windTangentPS = mul((float3x3)pointBSToPivotBS, originalTangentBS); float3 windNormalPS = mul((float3x3)pointBSToPivotBS, originalNormalBS); windPointBS = mul(pivotBSToObjBS, float4(windPointPS, 1.0)).xyz; windTangentBS = mul((float3x3)pivotBSToObjBS, windTangentPS); windNormalBS = mul((float3x3)pivotBSToObjBS, windNormalPS); } else { float intensity = _WindIntensity; float magnitude = length(pointOffsetBS); intensity += _WindVariety * GetVariety(_Time.y * _WindVaryFrequency - magnitude * _BranchDelay); float3 axisX, axisY, axisZ; float3 windPointOffsetBS = CircleTransform(pointOffsetBS, intensity, axisX, axisY, axisZ); float4x4 pointBSToObjBS = float4x4(float4(axisX.x, axisY.x, axisZ.x, windPointOffsetBS.x), float4(axisX.y, axisY.y, axisZ.y, windPointOffsetBS.y), float4(axisX.z, axisY.z, axisZ.z, windPointOffsetBS.z), float4(0.0, 0.0, 0.0, 1.0)); windPointBS = windPointOffsetBS + pivotPositionBS; windTangentBS = mul((float3x3)pointBSToObjBS, originalTangentBS); windNormalBS = mul((float3x3)pointBSToObjBS, originalNormalBS); } float3 pivotPositionOS = mul(bendToObject, windPointBS); float3 pivotTangentOS = mul(bendToObject, windTangentBS); float3 pivotNormalOS = mul(bendToObject, windNormalBS); VertexPositionInputs vertexInput = GetVertexPositionInputs(pivotPositionOS); VertexNormalInputs normalInput = GetVertexNormalInputs(pivotNormalOS, float4(pivotTangentOS, input.tangentOS.w)); output.uv = TRANSFORM_TEX(input.texcoord0, _BaseMap); output.positionCS = TransformWorldToHClip(ApplyShadowBias(vertexInput.positionWS, normalInput.normalWS, _LightDirection)); return output; } half4 ShadowPassFragment(Varyings input) : SV_TARGET { return 0.0; } ENDHLSL } Pass { Name \u0026#34;DepthOnly\u0026#34; Tags{\u0026#34;LightMode\u0026#34; = \u0026#34;DepthOnly\u0026#34;} HLSLPROGRAM #pragma vertex DepthOnlyVertex #pragma fragment DepthOnlyFragment struct Attributes { float4 positionOS : POSITION; float2 texcoord : TEXCOORD0; float4 color : COLOR; UNITY_VERTEX_INPUT_INSTANCE_ID }; struct Varyings { float2 uv : TEXCOORD0; float4 positionCS : SV_POSITION; UNITY_VERTEX_INPUT_INSTANCE_ID UNITY_VERTEX_OUTPUT_STEREO }; Varyings DepthOnlyVertex(Attributes input) { Varyings output = (Varyings)0; UNITY_SETUP_INSTANCE_ID(input); UNITY_INITIALIZE_VERTEX_OUTPUT_STEREO(output); VertexPositionInputs vertexInput = GetVertexPositionInputs(input.positionOS); output.uv = input.texcoord; output.positionCS = vertexInput.positionCS; return output; } half4 DepthOnlyFragment(Varyings input) : SV_TARGET { return 0.0; } ENDHLSL } Pass { Name \u0026#34;Meta\u0026#34; Tags{\u0026#34;LightMode\u0026#34; = \u0026#34;Meta\u0026#34;} HLSLPROGRAM #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/MetaInput.hlsl\u0026#34; #pragma vertex MetaVertex #pragma fragment MetaFragment struct Attributes { float4 positionOS : POSITION; float3 normalOS : NORMAL; float2 uv0 : TEXCOORD0; float2 uv1 : TEXCOORD1; float2 uv2 : TEXCOORD2; }; struct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; }; Varyings MetaVertex(Attributes input) { Varyings output; output.positionCS = MetaVertexPosition(input.positionOS, input.uv1, input.uv2, unity_LightmapST, unity_DynamicLightmapST); output.uv = TRANSFORM_TEX(input.uv0, _BaseMap); return output; } half4 MetaFragment(Varyings input) : SV_Target { //material properties float4 baseMap = tex2D(_BaseMap, input.uv); float roughnessMap = tex2D(_RoughnessMap, input.uv).r; float roughness = max(roughnessMap * _RoughnessIntensity, 1e-2); float metallicMap = tex2D(_MetallicMap, input.uv).r; float metallic = metallicMap * _MetallicIntensity; float oneMinusReflectivity = kDieletricSpec.a * (1 - metallic); float reflectivity = 1.0 - oneMinusReflectivity; float3 diffuse = baseMap.rgb * oneMinusReflectivity; float3 specular = lerp(kDieletricSpec.rgb, baseMap.rgb, metallic); MetaInput metaInput; metaInput.Albedo = diffuse; metaInput.SpecularColor = specular; metaInput.Emission = 0; return MetaFragment(metaInput); } ENDHLSL } } } 后续的思考 首先先讲好的方面，风吹动的效果确实十分自然，同样的计算出的正确的法线在PBR的渲染中也十分重要（顶点动画中正确的法线尤其不易！）。但是不足之处是矩阵运算过多了，不过矩阵运算全都在顶点着色器中，消耗也不是特别大。太多的矩阵运算也导致了这种算法的扩展性不是很好，如果想要有Pivot Layer为3的顶点，在目前的算法里面是没办法计算的，也不知道顽皮狗是怎么做的了。在模型的形状不是特别好的时候，比如一个quad来渲染草，或者是模型叶子歪歪扭扭的，会有比较大的变形。不过仔细设置每一个顶点的Pivot的话，应该还是能够解决一部分问题的，当然拙劣的Maya脚本又成了一个痛点。\n总的来说我还是比较满意的，也算是解决了一个比较复杂的问题，当然了，我对hugo能够支持gif图片更加满意！\n","permalink":"https://zznewclear13.github.io/posts/create-plant-swaying-in-wind-using-vertex-animation/","summary":"动机和想要实现的效果 最直接的动机是看了顽皮狗在Siggraph 2016上的PPT，里面介绍了顽皮狗在神秘海域中是如何让植被随风飘荡的。他们介绍了一种将植被的每一部分的pivot的物体空间坐标写到顶点色里，然后在shader中使用这个坐标进行风的效果的计算的方法。较为震撼在风吹过草原时，植被进行弯曲后，草表面的高光会有一种时空上的起伏感（也就是说神秘海域的植被的法线也会被风影响）。所以我也想要借助写pivot的方法来制作植被受到风吹的效果，通过这个方法计算出正确的风吹之后的植被的法线（同时由于法线贴图的存在，还要计算正确的切线）。\n稍微翻了一下网上的资料（也没仔细地去搜索），大部分的就是一个普通的顶点动画，有的是用的sin，有的就直接平移。这就产生了第二个需求，植被在顶点动画中应该保持差不多的长度，不然会发现很明显的拉伸的效果。\n当然最好还能投射出正常的影子了，这一步只需要把顶点着色器复制一份到投射影子的pass里就可以了。\n这里使用的植被模型是MegaScans上的CORDYLINE模型中的var12这个小模型。\n难点和相对应的应对方法 Unity的顶点色限制 稍微测试一下就能发现，Unity的顶点色是UNorm8的格式，也就是说无论你在Maya或是3ds Max里导出的模型的顶点色信息是什么样的，导入到Unity中就会变成只有256精度的UNorm8。顽皮狗使用的是自己的引擎，所以它们能够使用全精度的顶点色，但是由于Unity的引擎限制，我们可以考虑到导出pivot的顶点坐标到模型的UV中。\n但是很不幸的是，fbx导入到Unity时，即使UV是float4的类型（也就是16bytes)，在Unity中只会识别UV的前两位。所以只能无奈的将pivot的顶点坐标（float3的数据）储存到两个UV的三个通道里，同时将pivot的层级存到剩下的一个通道里。我不知道顽皮狗具体是怎么计算pivot的层级关系的，他在PPT中写的是无需计算，但我在实际操作中只能一层一层的算（而且只能算两层），也希望知道具体怎么操作的人告知一下方法。\n所以接下来要做的是在Maya中把pivot的物体空间坐标和pivot的层级写到对应顶点的某两套UV中，本文是写到第二套和第三套UV中（也就是TEXCOORD1和TEXCOORD2）。于是我恶补了一下maya的python脚本的写法，不过在写数值到UV中时，又遇到了一个小问题。Maya的cmds.polyEditUV这个方法，明明能传入uvSetName这个参数，用于操作对应的UV，但我实际使用时只能写数值到当前的UV中，导致最后写的脚本只能僵硬的操作当前UV，每次切换UV时需要重新修改脚本再运行一次。\n最终的脚本是这样的：\nVertexPivotWriteTool.py import maya.cmds as cmds targetVertexStr = \u0026#34;Select any vertex to start.\u0026#34; vertexColorStr = \u0026#34;Select any vertex to start.\u0026#34; pivotPosition = [0.0, 0.0, 0.0] def ui(): if cmds.window(\u0026#34;VertexPivotWriteTool\u0026#34;, exists = True): cmds.deleteUI(\u0026#34;VertexPivotWriteTool\u0026#34;) global targetVertexStr global targetVertexField global vertexColorStr global vertexColorField global pivotLayer vertexPivotWindow = cmds.window(\u0026#34;VertexPivotWriteTool\u0026#34;, widthHeight = [500, 400]) form = cmds.formLayout(numberOfDivisions = 100) pivotLayerLable = cmds.text(\u0026#34;Pivot Layer (0 for root pivot)\u0026#34;) pivotLayer = cmds.","title":"使用顶点动画制作随风飘动的植物"},{"content":"动机和贝塞尔曲线相关的背景知识 动机当然是要在UI上绘制一个贝塞尔曲线的形状了，想做的效果大概就和虚幻引擎蓝图连接节点的线差不多了。这里要绘制的是一种较为特殊的贝塞尔曲线，它的两个端点的切线是水平的，且拥有旋转对称的特性。\n我们想要绘制的S形曲线是三阶的贝塞尔曲线。三阶贝塞尔曲线有四个控制点\\(P_0, P_1, P_2, P_3\\)，对于一个从0到1的变量\\(t\\)，贝塞尔曲线的做法是对这四个点按照顺序以\\(t\\)做插值生成三个新的点，然后对这三个点按照顺序以\\(t\\)做插值生成新的两个点，再对这两个点以t做插值生成最后的点，当t在0到1中变化时，这个点的轨迹就构成了贝塞尔曲线。贝塞尔曲线上的点可以用四个控制点和\\(t\\)来表示： $$ P_{bezier} = P_0 \\cdot (1 - t)^3 + 3 P_1 \\cdot (1 - t)^2 \\cdot t + 3 P_2 \\cdot (1 - t) \\cdot t^2 + P_3 \\cdot t^3 $$\n如果初始四个点分别是(0.0, 1.0), (d, 1.0), (1.0 - d, 0.0)，和(1.0, 0.0)的话，也就是我们所要绘制的特殊的贝塞尔曲线，可以算出贝塞尔曲线的坐标为 $$ P_{bezier} = ((3 t - 9t ^ 2 + 6t^3) \\cdot d + 3t^2 - 2t^3, 1 - 3t^2 + 2t^3) $$ 但是即使得到了贝塞尔曲线的参数方程，想要将其表达成\\(f(x)\\)的形式仍然是相当困难的。Alan Wolfe在他的博客中提到了一种一维贝塞尔曲线，也是一种贝塞尔曲线的特殊情况，四个控制点在水平方向上等距排开，这样子贝塞尔曲线的参数方程的水平分量就刚好是\\(t\\)，它的竖直分量也就是我们需要的\\(f(x)\\)。唯一美中不足的是，能轻易得到\\(f(x)\\)的一维贝赛尔曲线，往往是一个“躺倒”的贝赛尔曲线，感官上看上去是横着的，Shadertoy上有相关的演示。但这种一维贝塞尔曲线又有一种特殊情况，也就是前两个控制点的竖直高度相等，后两个控制点的竖直高度也相等，这时这种特殊的一维贝塞尔曲线就是我们耳熟能详的smoothstep曲线了（数学真奇妙啊）。可惜smoothstep不能满足我们随意控制曲线形状的需求，只能另求他法。\n事实上我们想要确保绘制出的贝塞尔曲线是等宽的，也不能只使用曲线的\\(f(x)\\)，这样只能确保其在竖直方向是等宽的。需要整体等宽，等价于需要知道平面上每一个点到贝塞尔曲线的最近距离（也就是我们之前提到过的距离场Distance Field了。要求任意贝塞尔曲线的距离场，不是一个特别简单的事情，需要牛顿迭代法等数学方法，但是好在Shadertoy上的用户NinjaKoala已经帮我们把这个问题解决了，而且无私的把源代码分享给了我们。本博客中使用的距离场是NinjaKoala给出的一种大致的距离场，在距离贝塞尔曲线较远时会有一些偏差，但较近的距离基本上没什么问题。\n一些其他的需求 已经基本知道该怎么绘制贝塞尔曲线了，但是我们还需要考虑到在使用Shader时候的一些需求。我们要考虑的不仅仅是调整颜色、宽度、阴影等参数，而是这个Shader将要如何放到屏幕上。\n正常来说策划会指定两个顶点，说在这两个顶点之间画一条贝赛尔曲线，这时程序会给我们两个屏幕空间坐标（在Unity中就是RectTransform的坐标了），根据这两个坐标使用Image这个组件，在屏幕上用我们写的Shader的材质渲染一个Quad。\n但是事情并没有这么简单，Quad的顶点是贝塞尔曲线的两个端点的话，一个等宽的贝赛尔曲线就会超出Quad的范围，更不要说还有一段偏移和模糊的阴影了。因此我们要做是在绘制给定四个顶点的Quad时，在顶点着色器中将其顶点向外偏移一定的数量，让整条贝塞尔曲线和其阴影都能落在扩大后的Quad的范围中。\n值得一提的是，本来消耗很高的软阴影（需要很多次采样），在获得了距离场之后，可以通过普通的lerp或者Smoothstep直接生成，也是可喜可贺的一件事。\n具体的实施步骤 在Canvas中创建一个Image组件，挂上我们的材质球和DrawBezierCurve脚本。 在DrawBezierCurve脚本中获取Image组件的像素单位的宽和高（其实只要高就行了），将这个值传给Shader。 顶点着色器中，首先需要将Quad上方的两个顶点往上移半个贝塞尔曲线的宽度个像素heightA = _Width * 0.5，然后要将Quad下方的两个顶点往下移阴影偏移加半个阴影宽度个像素heightC = _ShadowOffset + _ShadowWidth * 0.5。在uv的时候要确保偏移之后的uv在原Quad范围内仍是0-1之间的。这一步对最终的效果至关重要！ 片元着色器中，使用ddx(uv.x)和ddy(uv.y)，可以计算出贝塞尔曲线的以像素为单位的距离场（也能用C#脚本传入的Quad的高和宽来算，不过应该是ddx比较方便）。使用smoothstep就能绘制出抗锯齿的贝塞尔曲线，或是模糊的阴影了。 DrawBezierCurve.cs Update的时候要记得拿到对应的材质的引用。\nusing UnityEngine; using UnityEngine.UI; [ExecuteInEditMode] [RequireComponent(typeof(Image))] [RequireComponent(typeof(RectTransform))] public class DrawBezierCurve : MonoBehaviour { public Image image; public RectTransform rectTransform; private Material material; private void OnEnable() { image = GetComponent\u0026lt;Image\u0026gt;(); material = new Material(image.material); image.material = material; rectTransform = GetComponent\u0026lt;RectTransform\u0026gt;(); } private void Update() { Vector2 bottomLeftCorner = rectTransform.anchoredPosition + rectTransform.rect.min; Vector2 topRightCorner = rectTransform.anchoredPosition + rectTransform.rect.max; Vector4 anchorPos = new Vector4(bottomLeftCorner.x, bottomLeftCorner.y, topRightCorner.x, topRightCorner.y); material.SetVector(\u0026#34;_AnchorPos\u0026#34;, anchorPos); } } BezierShader.shader 贝塞尔的距离场来自Cubic bezier approx distance 2。\n我还增加了选项，在显示一条横线时关闭BEZIERCURVE就能使用效率更高的横线的距离场了（要注意绘制横线时，RectTransform中Height的值不能设置成0，最好设置成一个如0.01的小数），同时也添加了水平翻转的选项。\n贝塞尔曲线和阴影混合之后再参加透明度混合，会给贝塞尔曲线带上一条浅浅的阴影颜色的描边，效果还算不错，这里就暂时不修改了。但是如果不想要这个描边的话。需要预先计算出混合后的颜色值和透明度值。如果用\\((c_1, a_1)\\)、\\((c_2, a_2)\\)、\\((c_0)\\)来分别表示曲线和阴影的颜色值、透明度值和透明度混合时目标的颜色的话（从前往后的顺序应该是曲线\\((c_1, a_1)\\)，阴影\\((c_2, a_2)\\)和透明度混合目标\\((c_0)\\)），正确的透明度混合的颜色应该是\\(c_1 a_1 + c_2 a_2 (1 - a_1) + c_0 (1 - a_1) (1 - a_2)\\)，所以可以设置透明度混合模式为Blend One SrcAlpha，然后将输出的颜色设置成float4(c1 * a1 + c2 * a2 * (1 - a1), (1 - a1) * (1 - a2))，这样混合后的颜色就是完美的正常的透明度混合了。\n//Based on https://www.shadertoy.com/view/3lsSzS Shader \u0026#34;zznewclear13/BezierShader\u0026#34; { Properties { [HideInInspector] _MainTex (\u0026#34;Main Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} [Header(Marcos)] [Toggle(BEZIERCURVE)] _BezierCurve(\u0026#34;Bezier Curve\u0026#34;, float) = 1 [Toggle(FLIP)] _Flip (\u0026#34;Flip\u0026#34;, float) = 0 [Header(Curve Settings)] _Color (\u0026#34;Color\u0026#34;, Color) = (1.0, 1.0, 1.0, 1.0) _Curve (\u0026#34;Curve\u0026#34;, range(0, 1)) = 0.2 _Width (\u0026#34;Pixel Width\u0026#34;, float) = 5 [Header(Shadow Settings)] _ShadowColor (\u0026#34;Shadow Color\u0026#34;, color) = (0.0, 0.0, 0.0, 0.5) _ShadowOffset (\u0026#34;Shadow Offset\u0026#34;, float) = 30 _ShadowWidth (\u0026#34;Shadow Width\u0026#34;, float) = 20 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #pragma multi_compile_local _ FLIP #pragma multi_compile_local _ BEZIERCURVE sampler2D _MainTex; CBUFFER_START(UnityPerMaterial) float4 _Color; float _Curve; float _Width; float4 _ShadowColor; float _ShadowOffset; float _ShadowWidth; CBUFFER_END float4 _AnchorPos; static float2 VertexOffsets[] = { float2(0, -1), float2(1, 0), float2(1, 0), float2(0, -1), }; struct Attributes { uint vertexID : SV_VERTEXID; float4 positionOS : POSITION; float2 texcoord : TEXCOORD0; }; struct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; }; Varyings Vert(Attributes input) { Varyings output = (Varyings)0; float4 positionOS = input.positionOS; float heightA = _Width * 0.5; float heightB = _AnchorPos.w - _AnchorPos.y; float heightC = _ShadowOffset + _ShadowWidth * 0.5; positionOS.y += VertexOffsets[input.vertexID].x * heightA + VertexOffsets[input.vertexID].y * heightC; VertexPositionInputs vertexInput = GetVertexPositionInputs(positionOS); output.positionCS = vertexInput.positionCS; //0-1 -\u0026gt; -(heightC) / heightB, (heightA + heightB + heightC) / heightB float newCoordY = (heightA + heightB + heightC) / heightB * input.texcoord.y - (heightC) / heightB ; output.uv = float2(input.texcoord.x, newCoordY); return output; } float cubic_bezier_normal_iteration(float t, float2 a0, float2 a1, float2 a2, float2 a3) { //factor should be positive //it decreases the step size when lowered. //Lowering the factor and increasing iterations increases the area in which //the iteration converges, but this is quite costly const float factor=1.; //horner\u0026#39;s method float2 a_2=a2+t*a3; float2 a_1=a1+t*a_2; float2 b_2=a_2+t*a3; float2 uv_to_p=a0+t*a_1; float2 tang=a_1+t*b_2; float l_tang=dot(tang,tang); return t-factor*dot(tang,uv_to_p)/l_tang; } float cubic_bezier_dis_approx_sq(float2 uv, float2 p0, float2 p1, float2 p2, float2 p3) { float2 a3 = (-p0 + 3. * p1 - 3. * p2 + p3); float2 a2 = (3. * p0 - 6. * p1 + 3. * p2); float2 a1 = (-3. * p0 + 3. * p1); float2 a0 = p0 - uv; float d0 = 1e38; float t0=0.; float t; const int num_iterations=3; const int num_start_params=3; for(int i=0;i\u0026lt;num_start_params;i++) { t=t0; for(int j=0;j\u0026lt;num_iterations;j++) { t=cubic_bezier_normal_iteration(t,a0,a1,a2,a3); } t=clamp(t,0.,1.); float2 uv_to_p=((a3*t+a2)*t+a1)*t+a0; d0=min(d0,dot(uv_to_p,uv_to_p)); t0+=1./float(num_start_params-1); } return d0; } float cubic_bezier_dis_approx(float2 uv, float2 p0, float2 p1, float2 p2, float2 p3) { return sqrt(cubic_bezier_dis_approx_sq(uv,p0,p1,p2,p3)); } float4 Frag(Varyings input) : SV_TARGET { float ddxX = ddx(input.uv.x); float ddyY = ddy(input.uv.y); float2 scale = float2(rcp(ddxX), rcp(ddyY)); float2 coord = input.uv; #if FLIP coord.x = 1.0 - coord.x; #endif #if BEZIERCURVE float2 p0=float2(0.0, 1.0) * scale; float2 p1=float2(_Curve, 1.0) * scale; float2 p2=float2(1.0 - _Curve, 0.0) * scale; float2 p3=float2(1.0, 0.0) * scale; float dist = cubic_bezier_dis_approx(coord * scale,p0,p1,p2,p3); float widthVal = _Width * 0.5; float alpha = smoothstep(widthVal + 1.0, widthVal, dist); float shadowDist = cubic_bezier_dis_approx(coord * scale + float2(0.0, _ShadowOffset),p0,p1,p2,p3); float shadowWidthVal = _ShadowWidth * 0.5; float shadowAlpha = smoothstep(shadowWidthVal + 1.0, 0.0, shadowDist); #else float dist = abs((1.0 - coord.y) * scale.y); float widthVal = _Width * 0.5; float alpha = smoothstep(widthVal + 1.0, widthVal, dist); float shadowDist = abs((1.0 - coord.y) * scale.y - _ShadowOffset); float shadowWidthVal = _ShadowWidth * 0.5; float shadowAlpha = smoothstep(shadowWidthVal + 1.0, 0.0, shadowDist); #endif float3 color = lerp(_ShadowColor.rgb, _Color.rgb, alpha * _Color.a); float finalAlpha = max(alpha * _Color.a, shadowAlpha * _ShadowColor.a); float4 returnColor = float4(color, finalAlpha); //returnColor = float4(input.uv, 0.0, 1.0); return returnColor; } ENDHLSL SubShader { Tags{ \u0026#34;Queue\u0026#34; = \u0026#34;Transparent\u0026#34; \u0026#34;RenderType\u0026#34; = \u0026#34;Transparent\u0026#34; } pass { Cull Back ZTest LEqual ZWrite Off Blend SrcAlpha OneMinusSrcAlpha HLSLPROGRAM #pragma vertex Vert #pragma fragment Frag ENDHLSL } } } 最后的思考 数学真奇妙啊！也很感谢Shadertoy上用户们的无私的奉献！但是微积分对我来说还是太难了，是不是应该找个机会补一补呢。。。最后就是我感觉对fwidth抗锯齿的理解又加深了！\n","permalink":"https://zznewclear13.github.io/posts/draw-equal-width-bezier-curve-in-unity/","summary":"动机和贝塞尔曲线相关的背景知识 动机当然是要在UI上绘制一个贝塞尔曲线的形状了，想做的效果大概就和虚幻引擎蓝图连接节点的线差不多了。这里要绘制的是一种较为特殊的贝塞尔曲线，它的两个端点的切线是水平的，且拥有旋转对称的特性。\n我们想要绘制的S形曲线是三阶的贝塞尔曲线。三阶贝塞尔曲线有四个控制点\\(P_0, P_1, P_2, P_3\\)，对于一个从0到1的变量\\(t\\)，贝塞尔曲线的做法是对这四个点按照顺序以\\(t\\)做插值生成三个新的点，然后对这三个点按照顺序以\\(t\\)做插值生成新的两个点，再对这两个点以t做插值生成最后的点，当t在0到1中变化时，这个点的轨迹就构成了贝塞尔曲线。贝塞尔曲线上的点可以用四个控制点和\\(t\\)来表示： $$ P_{bezier} = P_0 \\cdot (1 - t)^3 + 3 P_1 \\cdot (1 - t)^2 \\cdot t + 3 P_2 \\cdot (1 - t) \\cdot t^2 + P_3 \\cdot t^3 $$\n如果初始四个点分别是(0.0, 1.0), (d, 1.0), (1.0 - d, 0.0)，和(1.0, 0.0)的话，也就是我们所要绘制的特殊的贝塞尔曲线，可以算出贝塞尔曲线的坐标为 $$ P_{bezier} = ((3 t - 9t ^ 2 + 6t^3) \\cdot d + 3t^2 - 2t^3, 1 - 3t^2 + 2t^3) $$ 但是即使得到了贝塞尔曲线的参数方程，想要将其表达成\\(f(x)\\)的形式仍然是相当困难的。Alan Wolfe在他的博客中提到了一种一维贝塞尔曲线，也是一种贝塞尔曲线的特殊情况，四个控制点在水平方向上等距排开，这样子贝塞尔曲线的参数方程的水平分量就刚好是\\(t\\)，它的竖直分量也就是我们需要的\\(f(x)\\)。唯一美中不足的是，能轻易得到\\(f(x)\\)的一维贝赛尔曲线，往往是一个“躺倒”的贝赛尔曲线，感官上看上去是横着的，Shadertoy上有相关的演示。但这种一维贝塞尔曲线又有一种特殊情况，也就是前两个控制点的竖直高度相等，后两个控制点的竖直高度也相等，这时这种特殊的一维贝塞尔曲线就是我们耳熟能详的smoothstep曲线了（数学真奇妙啊）。可惜smoothstep不能满足我们随意控制曲线形状的需求，只能另求他法。","title":"在Unity的UI中绘制等宽的贝赛尔曲线"},{"content":"对于描边的思考 描边可以说是一个特别关键的效果，不仅仅是二次元卡通渲染需要用到描边，在用户交互的方面，描边也是一个增强用户交互的关键效果。\n一般的描边的做法是绘制一个沿物体空间顶点法线（或是记录在顶点色中的描边方向）外扩的模型背面，这种做法在绝大部分情况都看上去不错，但是描边的深度测试会有一些小瑕疵，同时在物体距离摄像机较近的时候，描边会显得较粗，此外这种描边没有抗锯齿的效果，绘制模型的背面也让造成了性能的浪费。另外一种方法是使用Multiple Render Targets，渲染出一个模型的剪影，然后使用类似高斯模糊的办法，对采样进行偏移，这样可以渲染出一个较好的可以有抗锯齿效果的描边，但是仅限于模型向外的描边，缺少模型内部的描边效果。\n最好的描边应该是能够支持模型外描边、内描边、材质描边的描边效果，pencil +实现了这些效果，但是效率不是很高，这里有相关的演示（我也是看了这个之后才决定用安吉拉的模型的）。我看到的较好的方案应该还是L-灵刃的使用退化四边形生成描边的办法，github上也分享了源码。\n这篇博客中介绍的描边，是基于我上一篇博客中讲的世界空间中绘制等宽线条的方法，使用DrawProcedural绘制的等宽的描边。我认为只有等宽的描边，才是最能表现二次元画面特征的描边。这里的“等宽”，并不是说线条的宽度处处相等，线条当然可以控制每一部分的粗细，但是这个控制的粗细是基于一个固定值的相对粗细（也就是存在顶点色中的描边粗细值），当粗细值相同时，不管是画面的哪个部分的描边的粗细（不管是内描边还是外描边），都应该是相同的。\n实现描边时需要注意的点 首先参考退化四边形的案例，需要先对模型文件进行预处理。这里我做了简化，只去寻找两个三角面共用的边，忽略了只属于一个三角面的边的情况（事实上我觉得这样看上去的视觉效果也蛮不错的）。一条共用边对应了这条边的两个顶点，两侧的两个三角形和这两个三角形对应的额外的两个顶点。这里都用序号来表示，需要6个int值（事实上可以忽略两个三角形的编号，就能存在一个int4里了）。判断一条边共同属于两个三角形，就相当于判断两个三角形中的某两个顶点的序号是相同的（实际上顺序是相反的）。但是实际操作中，即使是相同的顶点，在两个三角形中顶点的序号也不一定是相同的，因此需要先把两个相同的顶点（使用距离来判断）合并成一个顶点，这也就是我使用vertRemapping这个数组的目的。剩下的就是循环所有三角形，获取共用边的算法部分了，尽可能的优化一下，不然三角面一多运算的时间要很久。\n有了共用边的数据，通过SkinnedMeshRenderer.BakeMesh()可以获取到当前帧每个顶点的物体空间的坐标，就能进行描边的计算了。使用DrawProcedural时顶点的数量可以是公用边数量的两倍，这样需要在Geometry Shader中把顶点数目从2扩充到6，或者是在绘制时将顶点数量设置成共用边数量的六倍，可能后者效率会高一点，不过思考的时候会有点乱，这里就使用Geometry Shader的方法了。\n如果是像之前的博客介绍的，以共用边两个顶点为中心，同时向左右两侧外扩的话，会因为深度测试的原因，导致描边部分被模型遮挡，这个问题比较严重，他直接导致了外描边和内描边的粗细不一样，也导致了在一条描边中会露出一部分模型的问题。这里采用的方法是仅向外侧描边，在计算是不是轮廓边的时候同时计算需要描边的方向，使用这个方向向外扩展描边，最后效果还蛮不错的。\n要实现风格化描边的话，除了使用顶点色来控制描边的粗细之外，还能使用一张贴图作为描边的笔刷，在绘制描边的时候采样这张贴图，本篇博客就暂不使用这种方法了。\n具体的实现描边的操作 对当前模型获取到所有的共用边对应的四个顶点序号，严格保持顺序！ 每一帧使用SkinnedMeshRenderer.BakeMesh，获取所有顶点当前的物体空间的坐标。 传入顶点坐标，顶点重映射数组，共用边信息，如果需要的话还要传顶点色到描边的Shader中。 使用DrawProcedural绘制描边，顶点数量为共用边的数量的两倍。 在顶点着色器中，计算共用边四个顶点的裁剪空间（实际上用的是屏幕空间）的坐标，判断这条边是不是轮廓边，同时记录描边外扩的方向，相当于对于每一条边（每两个点）存两个bool变量。 在几何体着色器中，计算共用边两个顶点的屏幕空间的坐标，计算出两个点之间的向量，计算与之相垂直的外扩的方向，根据两个向量，计算出描边的四个顶点的裁剪空间的坐标，并赋予uv的值。 在片元着色器中，根据uv计算出描边的颜色，可以采样贴图，也可以直接返回计算的颜色。 由于整个描边的操作较为复杂，我尽可能多的写了注释。\nOutlineObject.cs 定义共用边的结构体，也定义用于保存共用边数据的ScriptableObject。\nusing UnityEngine; namespace ZZNEWCLEAR13.Outline { [System.Serializable] public class OutlineObject : ScriptableObject { [System.Serializable] public class MeshOutlineInfo { public string meshName; //尽量不要都显示出来，不然很卡。。 //顶点、法线、切线和顶点色 [HideInInspector] public Vector3[] vertices; [HideInInspector] public Vector3[] normals; [HideInInspector] public Vector4[] tangents; [HideInInspector] public Color[] colors; //vertRemapping把相同位置的顶点编号映射到第一个该位置顶点的编号 [HideInInspector] public int[] vertRemapping; //三角形对应的顶点编号 [HideInInspector] public Vector3Int[] triangles; public Line[] commonLines; } public MeshOutlineInfo outlineInfo; } [System.Serializable] public struct Line { //Line public int v0; public int v1; //Triangle One: v0, v1, v2 public int t0; public int v2; //Triangle Two: v0, v3, v1 public int t1; public int v3; public Line(int _v0, int _v1) { v0 = _v0; v1 = _v1; t0 = -1; t1 = -1; v2 = -1; v3 = -1; } //重载了Equals方法 //两个三角形使用同一条边时，边的节点的顺序是相反的 //GetHashCode()不会写，也没必要写 :) public override bool Equals(object obj) { if (!(obj is Line line)) { return false; } return v0 == line.v1 \u0026amp;\u0026amp; v1 == line.v0; } } } ModelPreProcess.cs 用于模型预处理，生成并保存模型的共用边的信息。\nusing System.Collections.Generic; using UnityEngine; using UnityEditor; namespace ZZNEWCLEAR13.Outline { public class ModelPreProcess : EditorWindow { //当两个顶点之间距离小于EPSILON时，认为是同一个顶点 const float EPSILON = 0.00001f; public GameObject fbxObj; public string saveName = \u0026#34;OutlineInfo\u0026#34;; private Rect topToolBarRect { get { return new Rect(20, 10, position.width - 40, 120); } } [MenuItem(\u0026#34;zznewclear13/Model Pre-process\u0026#34;)] public static ModelPreProcess GetWindow() { ModelPreProcess window = GetWindow\u0026lt;ModelPreProcess\u0026gt;(); window.titleContent = new GUIContent(\u0026#34;Model Pre-process\u0026#34;); window.Focus(); window.Repaint(); return window; } private void OnGUI() { TopToolBar(topToolBarRect); } private void TopToolBar(Rect rect) { using (new GUILayout.AreaScope(rect)) { fbxObj = (GameObject)EditorGUILayout.ObjectField(\u0026#34;FBX Object\u0026#34;, fbxObj, typeof(GameObject), false); saveName = EditorGUILayout.TextField(\u0026#34;Save Name\u0026#34;, saveName); using (new EditorGUI.DisabledGroupScope(!fbxObj)) { if (GUILayout.Button(\u0026#34;Process!\u0026#34;, new GUILayoutOption[] { GUILayout.Height(30f) })) { ProcessAll(); } } } } private void ProcessAll() { MeshFilter[] meshFilters = fbxObj.GetComponentsInChildren\u0026lt;MeshFilter\u0026gt;(); SkinnedMeshRenderer[] skinnedMeshRenderers = fbxObj.GetComponentsInChildren\u0026lt;SkinnedMeshRenderer\u0026gt;(); int meshFilterLength = meshFilters.Length; OutlineObject.MeshOutlineInfo[] mois = new OutlineObject.MeshOutlineInfo[meshFilterLength + skinnedMeshRenderers.Length]; for (int i = 0; i \u0026lt; meshFilters.Length; i++) { mois[i] = ProcessMesh(meshFilters[i].sharedMesh); } for (int i = 0; i \u0026lt; skinnedMeshRenderers.Length; i++) { mois[i + meshFilterLength] = ProcessMesh(skinnedMeshRenderers[i].sharedMesh); } SaveAsset(mois); } //找出其中的共同边，并储存三角形序号和另外两个顶点的序号 //注意顶点的顺序 //v0, v1, v2是一个正面的三角形 //v0, v3, v1是一个正面的三角形 private void CheckLine(int triangleIndex, int vertexIndex, Line line, ref List\u0026lt;Line\u0026gt; lineList, ref List\u0026lt;Line\u0026gt; commonLines) { bool hasLine = false; int lineListIndex = -1; for (int i = 0; i \u0026lt; lineList.Count; i++) { if (line.Equals(lineList[i])) { hasLine = true; lineListIndex = i; break; } } if (hasLine) { Line tempLine = lineList[lineListIndex]; lineList.RemoveAt(lineListIndex); tempLine.t1 = triangleIndex; tempLine.v3 = vertexIndex; commonLines.Add(tempLine); } else { line.t0 = triangleIndex; line.v2 = vertexIndex; lineList.Add(line); } } //当两个顶点距离很近时，视做同一个顶点 //使用vertRemapping储存相同顶点的第一个顶点的序号 private int[] MergeIndexes(Vector3[] vertices) { int[] vertRemapping = new int[vertices.Length]; vertRemapping[0] = 0; for (int i = 1; i \u0026lt; vertices.Length; i++) { bool hasVert = false; for (int j = 0; j \u0026lt; i; j++) { if (vertRemapping[j] \u0026lt; j) { continue; } else { if ((vertices[i] - vertices[vertRemapping[j]]).magnitude \u0026lt; EPSILON) { vertRemapping[i] = vertRemapping[j]; hasVert = true; } } } if (!hasVert) { vertRemapping[i] = i; } } return vertRemapping; } private OutlineObject.MeshOutlineInfo ProcessMesh(Mesh sharedMesh) { OutlineObject.MeshOutlineInfo moi = new OutlineObject.MeshOutlineInfo(); moi.meshName = sharedMesh.name; Vector3[] vertices = sharedMesh.vertices; Vector3[] normals = sharedMesh.normals; Vector4[] tangents = sharedMesh.tangents; Color[] colors = sharedMesh.colors; moi.vertices = vertices; moi.normals = normals; moi.tangents = tangents; moi.colors = colors; int[] vertRemapping = MergeIndexes(vertices); moi.vertRemapping = vertRemapping; int[] triangles = sharedMesh.triangles; List\u0026lt;Line\u0026gt; lineList = new List\u0026lt;Line\u0026gt;(); List\u0026lt;Line\u0026gt; commonLines = new List\u0026lt;Line\u0026gt;(); System.Diagnostics.Debug.Assert(triangles.Length % 3 == 0); int triangleCount = triangles.Length / 3; Vector3Int[] packedTriangles = new Vector3Int[triangleCount]; //遍历所有的三角形，注意边的两个顶点的顺序 for (int i = 0; i \u0026lt; triangleCount; i++) { int triangleIndex = 3 * i; int vID0 = vertRemapping[triangles[triangleIndex]]; int vID1 = vertRemapping[triangles[triangleIndex + 1]]; int vID2 = vertRemapping[triangles[triangleIndex + 2]]; packedTriangles[i] = new Vector3Int(vID0, vID1, vID2); Line line0 = new Line(vID0, vID1); Line line1 = new Line(vID1, vID2); Line line2 = new Line(vID2, vID0); CheckLine(i, vID2, line0, ref lineList, ref commonLines); CheckLine(i, vID0, line1, ref lineList, ref commonLines); CheckLine(i, vID1, line2, ref lineList, ref commonLines); } moi.triangles = packedTriangles; moi.commonLines = commonLines.ToArray(); return moi; } private void SaveAsset(OutlineObject.MeshOutlineInfo[] outlineInfos) { string path = AssetDatabase.GetAssetPath(fbxObj); string assetPath = path.Substring(0, path.LastIndexOf(\u0026#39;/\u0026#39;)) + \u0026#34;/\u0026#34; + saveName; if (!System.IO.Directory.Exists(assetPath)) { System.IO.Directory.CreateDirectory(assetPath); } for (int i = 0; i \u0026lt; outlineInfos.Length; i++) { OutlineObject asset = ScriptableObject.CreateInstance\u0026lt;OutlineObject\u0026gt;(); asset.outlineInfo = outlineInfos[i]; string tempPath = assetPath + \u0026#34;/\u0026#34; + outlineInfos[i].meshName + \u0026#34;.asset\u0026#34;; AssetDatabase.CreateAsset(asset, tempPath); } AssetDatabase.SaveAssets(); } } } DrawOutline.cs 这里以SkinnedMeshRenderer为例，因为相对于普通的Mesh来说较为复杂，不能直接使用保存在共用边信息里的物体空间的顶点坐标。\nusing System.Collections.Generic; using UnityEngine; namespace ZZNEWCLEAR13.Outline { //仅支持SkinnedMeshRenderer //普通的Mesh只要稍改一下代码就好了 [ExecuteInEditMode] [RequireComponent(typeof(SkinnedMeshRenderer))] public class DrawOutline : MonoBehaviour { public Material outlineMaterial; public GameObject targetGO; public SkinnedMeshRenderer skinnedMeshRenderer; public OutlineObject outlineObject; private Mesh bakedMesh; private List\u0026lt;Vector3\u0026gt; meshVertices; private Vector3[] vertices; private Color[] colors; private int[] vertRemapping; private Line[] lines; private ComputeBuffer verticesBuffer; private ComputeBuffer colorBuffer; private ComputeBuffer vertRemappingBuffer; private ComputeBuffer lineBuffer; private void EnsureBuffer(ref ComputeBuffer buffer, int count, int stride) { if (buffer != null) { buffer.Release(); } buffer = new ComputeBuffer(count, stride, ComputeBufferType.Structured); } private void OnEnable() { meshVertices = new List\u0026lt;Vector3\u0026gt;(); vertices = outlineObject.outlineInfo.vertices; colors = outlineObject.outlineInfo.colors; vertRemapping = outlineObject.outlineInfo.vertRemapping; lines = outlineObject.outlineInfo.commonLines; EnsureBuffer(ref verticesBuffer, vertices.Length, 3 * 4); EnsureBuffer(ref colorBuffer, vertices.Length, 4 * 4); EnsureBuffer(ref vertRemappingBuffer, vertRemapping.Length, 4); EnsureBuffer(ref lineBuffer, lines.Length, 6 * 4); bakedMesh = new Mesh(); } private void Update() { DrawOutlineProcedural(); } private void DrawOutlineProcedural() { skinnedMeshRenderer.BakeMesh(bakedMesh); bakedMesh.GetVertices(meshVertices); verticesBuffer.SetData(meshVertices); colorBuffer.SetData(colors); vertRemappingBuffer.SetData(vertRemapping); lineBuffer.SetData(lines); MaterialPropertyBlock mpb = new MaterialPropertyBlock(); mpb.SetBuffer(\u0026#34;_VerticesBuffer\u0026#34;, verticesBuffer); mpb.SetBuffer(\u0026#34;_ColorBuffer\u0026#34;, colorBuffer); mpb.SetBuffer(\u0026#34;_VertRemappingBuffer\u0026#34;, vertRemappingBuffer); mpb.SetBuffer(\u0026#34;_LineBuffer\u0026#34;, lineBuffer); //一般需要传入SkinnedMeshRenderer的父物体的物体空间到世界空间的变换矩阵 mpb.SetMatrix(\u0026#34;_ObjToWorldMat\u0026#34;, targetGO.transform.localToWorldMatrix); Bounds bounds = skinnedMeshRenderer.bounds; Graphics.DrawProcedural(outlineMaterial, bounds, MeshTopology.Lines, lines.Length * 2, properties: mpb); } private void OnDestroy() { verticesBuffer.Dispose(); vertRemappingBuffer.Dispose(); lineBuffer.Dispose(); } } } OutlineShader.shader 感觉已经事无巨细的写在注释里了，最关键的就是时刻提醒自己绘制三角形时的顶点顺序。感觉我对使用数组来设计并行运算已经炉火纯青了。使用线性代数来判断三角形的其余两个顶点是不是在共用边的两侧，我感觉是一个比较好的办法（似乎比直接计算法线要稍好一点？）。唯一的问题是发现效果基本正确之后就不会再回过头去验证自己的代码了哈哈。\nShader \u0026#34;zznewclear13/OutlineShader\u0026#34; { Properties { _OutlineColor (\u0026#34;Outline Color\u0026#34;, color) = (1.0, 1.0, 1.0, 1.0) _OutlineExt (\u0026#34;Outline Extension\u0026#34;, range(-1, 1)) = 0.1 _OutlineWidth(\u0026#34;Outline Width\u0026#34;, float) = 1.0 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; float4x4 _ObjToWorldMat; float4 _OutlineColor; float _OutlineExt; float _OutlineWidth; StructuredBuffer\u0026lt;float3\u0026gt; _VerticesBuffer; StructuredBuffer\u0026lt;int\u0026gt; _VertRemappingBuffer; //实际可以简化成一个int4，只储存顶点序号 struct LineStruct { int2 lineVertices; // v0, v1 int4 trianglesAndVertices; // t0, v2, t1, v3 }; StructuredBuffer\u0026lt;LineStruct\u0026gt; _LineBuffer; struct Attributes { uint vertexID : SV_VERTEXID; }; struct Geoms { float4 positionCS : TEXCOORD0; bool2 edgeAndSide : TEXCOORD1; }; struct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; }; Geoms OutlineVert(Attributes input) { Geoms output = (Geoms)0; //获取顶点序号和边的序号 int vertexID = input.vertexID % 2; int lineID = input.vertexID / 2; //获取边上较近的顶点和较远的顶点的物体空间坐标 float3 vertexOne = _VerticesBuffer[_VertRemappingBuffer[_LineBuffer[lineID].lineVertices[vertexID]]]; float3 vertexTwo = _VerticesBuffer[_VertRemappingBuffer[_LineBuffer[lineID].lineVertices[1 - vertexID]]]; //获取第一个三角面的最后一个顶点和第二个三角面的最后一个顶点 float3 vertexThree = _VerticesBuffer[_VertRemappingBuffer[_LineBuffer[lineID].trianglesAndVertices[vertexID * 2 + 1]]]; float3 vertexFour = _VerticesBuffer[_VertRemappingBuffer[_LineBuffer[lineID].trianglesAndVertices[3 - vertexID * 2]]]; //转换到裁剪空间 float4x4 mvp = mul(UNITY_MATRIX_VP, _ObjToWorldMat); float4 positionCSOne = mul(mvp, float4(vertexOne, 1.0)); float4 positionCSTwo = mul(mvp, float4(vertexTwo, 1.0)); float4 positionCSThree = mul(mvp, float4(vertexThree, 1.0)); float4 positionCSFour = mul(mvp, float4(vertexFour, 1.0)); //获取屏幕空间的坐标 float2 uvOne = positionCSOne.xy / positionCSOne.w; float2 uvTwo = positionCSTwo.xy / positionCSTwo.w; float2 uvThree = positionCSThree.xy / positionCSThree.w; float2 uvFour = positionCSFour.xy / positionCSFour.w; //这里简化了求法线的过程，相当于判断两个三角面的最后一个顶点是不是在边的两侧 float valueA = uvOne.y - uvTwo.y; float valueB = uvOne.x * uvTwo.y - uvTwo.x * uvOne.y; float valueC = uvOne.x - uvTwo.x; float signThree = valueA * uvThree.x + valueB - valueC * uvThree.y; float signFour = valueA * uvFour.x + valueB - valueC * uvFour.y; //X：该顶点是不是轮廓边的顶点；Y：该顶点应该向边的顺时针方向还是逆时针方向外扩 output.edgeAndSide = bool2((signThree * signFour \u0026gt;= 0), (signThree \u0026gt;= 0)); output.positionCS = positionCSOne; return output; } [maxvertexcount(6)] void OutlineGeomTriangle(line Geoms input[2], inout TriangleStream\u0026lt;Varyings\u0026gt; triangleStream) { Varyings output = (Varyings)0; //判断是不是边界边，其实只需要判断一个顶点就可以了 if(input[0].edgeAndSide.x \u0026amp;\u0026amp; input[1].edgeAndSide.x) { //可以通过顶点色来调整描边宽度 float outlineWidthOne = _OutlineWidth;// * input[0].color.a; float outlineWidthTwo = _OutlineWidth;// * input[1].color.a; float4 positionCSOne = input[0].positionCS; float4 positionCSTwo = input[1].positionCS; //获取屏幕空间的两个顶点之间的向量 float2 offset = positionCSOne.xy / positionCSOne.w - positionCSTwo.xy / positionCSTwo.w; float lengthOffset = length(offset); float2 normalizedOffset = normalize(offset * (_ScreenParams.wz - 1.0)); //X：从一个顶点到另一个顶点的向量 //Y：与之垂直的另一个向量 //使用min(1.0, rcp(positionCSOne.w)来使描边按照距离变细，这里也可以使用平方根倒数来控制 float2 pointOffsetX = float2(normalizedOffset.x, normalizedOffset.y) * min(1.0, rcp(positionCSOne.w)); float2 pointOffsetY = float2(-normalizedOffset.y, normalizedOffset.x) * min(1.0, rcp(positionCSOne.w)); //第一第二个顶点不用外扩 //第三第四个顶点按照edgeAndSide来判断外扩的方向 //同时使用_OutlineExt来控制外扩出的梯形的形状 float4 csOne = positionCSOne; float4 csTwo = positionCSTwo; float4 csThree = float4(-positionCSTwo.w * (pointOffsetX * (1.0 + _OutlineExt) + pointOffsetY * (2 * input[1].edgeAndSide.y - 1)) * (_ScreenParams.zw - 1.0) * outlineWidthTwo, 0, 0) + positionCSTwo; float4 csFour = float4(-positionCSOne.w * (-pointOffsetX * (1.0 + _OutlineExt) - pointOffsetY * (2 * input[0].edgeAndSide.y - 1)) * (_ScreenParams.zw - 1.0) * outlineWidthOne, 0, 0) + positionCSOne; //四个顶点的UV float2 uvOne = float2(0.0, 0.0); float2 uvTwo = float2(1.0, 0.0); float2 uvThree = float2(1.0, 1.0); float2 uvFour = float2(0.0, 1.0); //绘制外扩梯形的其中三个顶点 output.positionCS = csOne; output.uv = uvOne; triangleStream.Append(output); output.positionCS = csTwo; output.uv = uvTwo; triangleStream.Append(output); output.positionCS = csThree; output.uv = uvThree; triangleStream.Append(output); //出于我不能解释的原因，这里的顶点顺序和前面是相反的 output.positionCS = csOne; output.uv = uvOne; triangleStream.Append(output); output.positionCS = csFour; output.uv = uvFour; triangleStream.Append(output); output.positionCS = csThree; output.uv = uvThree; triangleStream.Append(output); } triangleStream.RestartStrip(); } float4 OutlineFrag(Varyings input) : SV_TARGET { float2 uv = input.uv; //使用fwidth进行抗锯齿 return float4(_OutlineColor.rgb, smoothstep(1.0, 1.0 - fwidth(uv.y), input.uv.y)); } ENDHLSL SubShader { Tags {\u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34; } ZWrite Off Blend SrcAlpha OneMinusSrcAlpha Pass { HLSLPROGRAM #pragma target 5.0 #pragma vertex OutlineVert #pragma geometry OutlineGeomTriangle #pragma fragment OutlineFrag ENDHLSL } } } 最后的思考 本博客的模型是网上下载的，我也没有看过乐园追放，好像衣服部分的uv出了点问题，不过跟描边的效果没有关系，就这样了。\n在模型导入的时候最好能选择中等质量，这样模型的面数会少一些，也不会出现因为美术失误导致奇怪的地方产生描边的问题。最终描边效果确实还蛮不错的，确实都是一样的宽，使用fwidth的抗锯齿效果也蛮不错的，就是并没有把外描边和内描边分开来做（我也不知道该怎么做了），根据共用边两个三角面法线的夹角还能绘制额外的描边，这里就不额外做了（不过应该不太好做，深度测试的问题比较大），两种材质之间的描边也没做（完全不知道该怎么做）。但整体看上去还是挺可以的了，嘿嘿。\n","permalink":"https://zznewclear13.github.io/posts/draw-equal-width-outline-in-unity/","summary":"对于描边的思考 描边可以说是一个特别关键的效果，不仅仅是二次元卡通渲染需要用到描边，在用户交互的方面，描边也是一个增强用户交互的关键效果。\n一般的描边的做法是绘制一个沿物体空间顶点法线（或是记录在顶点色中的描边方向）外扩的模型背面，这种做法在绝大部分情况都看上去不错，但是描边的深度测试会有一些小瑕疵，同时在物体距离摄像机较近的时候，描边会显得较粗，此外这种描边没有抗锯齿的效果，绘制模型的背面也让造成了性能的浪费。另外一种方法是使用Multiple Render Targets，渲染出一个模型的剪影，然后使用类似高斯模糊的办法，对采样进行偏移，这样可以渲染出一个较好的可以有抗锯齿效果的描边，但是仅限于模型向外的描边，缺少模型内部的描边效果。\n最好的描边应该是能够支持模型外描边、内描边、材质描边的描边效果，pencil +实现了这些效果，但是效率不是很高，这里有相关的演示（我也是看了这个之后才决定用安吉拉的模型的）。我看到的较好的方案应该还是L-灵刃的使用退化四边形生成描边的办法，github上也分享了源码。\n这篇博客中介绍的描边，是基于我上一篇博客中讲的世界空间中绘制等宽线条的方法，使用DrawProcedural绘制的等宽的描边。我认为只有等宽的描边，才是最能表现二次元画面特征的描边。这里的“等宽”，并不是说线条的宽度处处相等，线条当然可以控制每一部分的粗细，但是这个控制的粗细是基于一个固定值的相对粗细（也就是存在顶点色中的描边粗细值），当粗细值相同时，不管是画面的哪个部分的描边的粗细（不管是内描边还是外描边），都应该是相同的。\n实现描边时需要注意的点 首先参考退化四边形的案例，需要先对模型文件进行预处理。这里我做了简化，只去寻找两个三角面共用的边，忽略了只属于一个三角面的边的情况（事实上我觉得这样看上去的视觉效果也蛮不错的）。一条共用边对应了这条边的两个顶点，两侧的两个三角形和这两个三角形对应的额外的两个顶点。这里都用序号来表示，需要6个int值（事实上可以忽略两个三角形的编号，就能存在一个int4里了）。判断一条边共同属于两个三角形，就相当于判断两个三角形中的某两个顶点的序号是相同的（实际上顺序是相反的）。但是实际操作中，即使是相同的顶点，在两个三角形中顶点的序号也不一定是相同的，因此需要先把两个相同的顶点（使用距离来判断）合并成一个顶点，这也就是我使用vertRemapping这个数组的目的。剩下的就是循环所有三角形，获取共用边的算法部分了，尽可能的优化一下，不然三角面一多运算的时间要很久。\n有了共用边的数据，通过SkinnedMeshRenderer.BakeMesh()可以获取到当前帧每个顶点的物体空间的坐标，就能进行描边的计算了。使用DrawProcedural时顶点的数量可以是公用边数量的两倍，这样需要在Geometry Shader中把顶点数目从2扩充到6，或者是在绘制时将顶点数量设置成共用边数量的六倍，可能后者效率会高一点，不过思考的时候会有点乱，这里就使用Geometry Shader的方法了。\n如果是像之前的博客介绍的，以共用边两个顶点为中心，同时向左右两侧外扩的话，会因为深度测试的原因，导致描边部分被模型遮挡，这个问题比较严重，他直接导致了外描边和内描边的粗细不一样，也导致了在一条描边中会露出一部分模型的问题。这里采用的方法是仅向外侧描边，在计算是不是轮廓边的时候同时计算需要描边的方向，使用这个方向向外扩展描边，最后效果还蛮不错的。\n要实现风格化描边的话，除了使用顶点色来控制描边的粗细之外，还能使用一张贴图作为描边的笔刷，在绘制描边的时候采样这张贴图，本篇博客就暂不使用这种方法了。\n具体的实现描边的操作 对当前模型获取到所有的共用边对应的四个顶点序号，严格保持顺序！ 每一帧使用SkinnedMeshRenderer.BakeMesh，获取所有顶点当前的物体空间的坐标。 传入顶点坐标，顶点重映射数组，共用边信息，如果需要的话还要传顶点色到描边的Shader中。 使用DrawProcedural绘制描边，顶点数量为共用边的数量的两倍。 在顶点着色器中，计算共用边四个顶点的裁剪空间（实际上用的是屏幕空间）的坐标，判断这条边是不是轮廓边，同时记录描边外扩的方向，相当于对于每一条边（每两个点）存两个bool变量。 在几何体着色器中，计算共用边两个顶点的屏幕空间的坐标，计算出两个点之间的向量，计算与之相垂直的外扩的方向，根据两个向量，计算出描边的四个顶点的裁剪空间的坐标，并赋予uv的值。 在片元着色器中，根据uv计算出描边的颜色，可以采样贴图，也可以直接返回计算的颜色。 由于整个描边的操作较为复杂，我尽可能多的写了注释。\nOutlineObject.cs 定义共用边的结构体，也定义用于保存共用边数据的ScriptableObject。\nusing UnityEngine; namespace ZZNEWCLEAR13.Outline { [System.Serializable] public class OutlineObject : ScriptableObject { [System.Serializable] public class MeshOutlineInfo { public string meshName; //尽量不要都显示出来，不然很卡。。 //顶点、法线、切线和顶点色 [HideInInspector] public Vector3[] vertices; [HideInInspector] public Vector3[] normals; [HideInInspector] public Vector4[] tangents; [HideInInspector] public Color[] colors; //vertRemapping把相同位置的顶点编号映射到第一个该位置顶点的编号 [HideInInspector] public int[] vertRemapping; //三角形对应的顶点编号 [HideInInspector] public Vector3Int[] triangles; public Line[] commonLines; } public MeshOutlineInfo outlineInfo; } [System.","title":"在Unity中绘制等宽的描边"},{"content":"动机 直接动机是想要在unity中制作一个描边效果。对于卡通渲染的描边效果，已经有很多很多的案例了，但是我觉得这些案例不一定能完全满足我的需求，于是想要从画直线开始研究。\n从普通绘画的角度来看，很重要的一点就是描边的宽度基本上是一致的：考虑一下远景的物体，画家在绘画的时候使用和近景相同粗细的笔（用一样或者稍弱的力），绘制一个较不精细的物体，而不是使用很细的笔去绘制一个精细的物体，结论就是远处的描边可能稍细一些，但最好是相同粗细，其颜色可能会变浅。\n从另一个角度来看，描边往往需要能够控制其宽度，对于较细的线，则会有较明显的锯齿（事实上只要角度不太好的描边，就会有很明显的边缘锯齿），那么控制粗细和进行一定程度的抗锯齿也是一个研究的方向。\n从第三个角度，碰巧看到了Freya Holmér制作的Shapes插件，能够绘制高质量的线条画，看上去渲染的效果很好。但是价格过于高昂，于是想要研究一个能够做到差不多效果的工具。\n抗锯齿和宽度的思考 在Unity中画直线有蛮多办法，Debug.DrawLine或者Gizmos.DrawLine都能绘制等宽的直线，不过只能固定一个像素宽，而且因为只有一个像素宽，所以会有明显的锯齿。使用LineRenderer可以绘制任意宽度的直线，写特定的shader通过透明度混合能够防止锯齿的出现（不过线段两端不太好做抗锯齿），但是不能保证在不同角度下直线的宽度相等。那么答案就很明显了，通过线段的顶点的数据，生成两个三角面（一个Quad），在GPU中计算出三角面每个顶点的屏幕空间的位置，确保线段的宽度一致。也由于有宽度的存在，给后续的透明度混合抗锯齿留下了操作的空间。\n具体的绘制线条的操作 首先看一张图（完了三角形顶点顺序画反了，已经积重难返了，代码表现对就当做对的吧） 我们将使用Graphics.DrawProcedural这个方法来绘制我们的直线，每一段直线由两个三角形组成（这里要注意在unity中三角形顶点顺序是顺时针的），也就是说要绘制N条直线的话，要传入N+1个节点位置数据，而实际绘制时会绘制6N个顶点。这样传入Shader的是一个表示线段节点位置的长度是N+1的Vector3数据_VerticesBuffer，和一个表示顶点序号的从0到6N-1的uint数据，也就是在绘制时图形API自动传输的SV_VERTEXID。\n这也就导致了我们的顶点着色器输入数据和普通的Shader有所区别，只有一个uint数据：\nstruct Attributes { uint vertexID : SV_VERTEXID; }; 在顶点着色器获取到SV_VERTEXID之后，我们可以以此来计算出线段的序号lineID和每个顶点在绘制线段时的序号vertexID。在绘制时我们又要知道当前线段的两个节点的位置，在传入_VerticesBuffer之后，我们需要对每个顶点确定其对应的线段节点的序号，即lineID + 0或者lineID + 1，我们可以将这个可以通过vertexID确定的0或者1储存到一个数组vertexIndexes中，方便使用vertexID读取。\n为了让线段取得较好的抗锯齿效果，我们把线段整体外扩了_OutlineWidth个像素宽（实际线段宽度是这个的两倍）。此时对于每一个顶点，我们需要知道外扩之后的屏幕空间的顶点位置.由于我们需要使用从LineStart到LineEnd的方向offset（对于1, 2, 4这几个顶点来说，offset是从LineEnd到LineStart的方向）和与其相垂直的方向来做外扩，我们需要把每个顶点的两个方向值储存到数组vertexOffsets中。可以看到顶点永远是往远离较远节点的方向移动的，因此vertexOffset的x值都是-1。而我们选取offset逆时针旋转90°的方向作为相垂直的方向，因此vertexOffset的y值会有正负之分。\n最后是uv的数值，我们还需要给每个顶点传入uv的数值，就比较简单了，储存到数组vertexUVs中。\n此外还要注意一点，在片元着色器中，我们需要知道LineStart和LineEnd对应的uv位置来做线段的圆形端点，因此我们还需要把这个长方体的高宽比ratio传给片元着色器。\n这样，我们片元着色器的输入数据就是positionCS，uv和ratio了：\nstruct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; float ratio : TEXCOORD1; }; 绘制线条的流程 C#脚本获取要绘制线条的节点数据，传入_VerticesBuffer，调用Graphics.DrawProcedural方法进行绘制。 Shader的顶点着色器根据SV_VERTEXID计算出每个顶点对应的线段序号lineID和顶点序号vertexID。 通过线段序号、顶点序号和vertexIndexes从_VerticesBuffer中找到较近的线段节点的世界坐标，和较远的线段节点的世界坐标。 将两个世界坐标转换到屏幕空间，得到屏幕空间两点对应的向量和与之相垂直的向量，进行归一化。 将每个顶点根据vertexOffsets和描边宽度进行外扩。 将外扩后的裁剪空间的坐标，从vertexUVs中获得的uv和高宽比ratio传给片元着色器。 片元着色器根据ratio和uv，绘制出一个两头圆形的线段。 DrawEqualWidthLine.cs C#脚本比较简单了，没什么特别需要注意的地方。\nusing UnityEngine; public class DrawEqualWidthLine : MonoBehaviour { public Vector3[] vertices; private int vertexCount; public Material equalWidthMaterial; ComputeBuffer verticesBuffer; private void EnsureBuffer(ref ComputeBuffer buffer, int count, int stride) { if (buffer == null) { buffer = new ComputeBuffer(count, stride, ComputeBufferType.Structured); } else if (buffer.count != count || buffer.stride != stride) { buffer.Release(); buffer = new ComputeBuffer(count, stride, ComputeBufferType.Structured); } } private void Awake() { vertexCount = vertices.Length; EnsureBuffer(ref verticesBuffer, vertexCount, 3 * 4); } private void Update() { if(vertexCount != vertices.Length) { vertexCount = vertices.Length; EnsureBuffer(ref verticesBuffer, vertexCount, 3 * 4); } Bounds bounds = new Bounds(Vector3.zero, Vector3.one * 100.0f); verticesBuffer.SetData(vertices); MaterialPropertyBlock mpb = new MaterialPropertyBlock(); mpb.SetBuffer(\u0026#34;_VerticesBuffer\u0026#34;, verticesBuffer); Graphics.DrawProcedural(equalWidthMaterial, bounds, MeshTopology.Triangles, (vertexCount - 1) * 6, properties: mpb); } void OnDestroy() { verticesBuffer.Dispose(); } } EqualWidthLineShader.shader 基本上需要注意的都写在shader的注释里面了，使用_OutlineColor来控制线段的颜色，_OutlineWidth来控制线段的宽度，_Sharpness来控制锐利程度以达到抗锯齿的效果。\n特别要注意的就是屏幕比例的问题，要确保在屏幕中两个偏移方向是互相垂直的，这样才能获得正确的新的裁剪空间的坐标，也才能获得正确的线段节点的uv值。还有要注意仿射变换和透视变换的区别，参考维基百科上的说明，决定了uv在屏幕空间中是不是线性的。\n当然三个数组可以合并成一个，不过也没那个必要就是了。\nShader \u0026#34;zznewclear13/EqualWidthLineShader\u0026#34; { Properties { _OutlineColor (\u0026#34;Outline Color\u0026#34;, color) = (1.0, 1.0, 1.0, 1.0) _OutlineWidth(\u0026#34;Outline Width\u0026#34;, float) = 10.0 _Sharpness(\u0026#34;Sharpness\u0026#34;, range(0, 0.99)) = 0.5 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; StructuredBuffer\u0026lt;float3\u0026gt; _VerticesBuffer; float4 _OutlineColor; float _OutlineWidth; float _Sharpness; static int vertexIndexes[] = { 0, 1, 1, 0, 1, 0 }; static float2 vertexOffsets[] = { float2(-1, -1), float2(-1, 1), float2(-1, -1), float2(-1, -1), float2(-1, -1), float2(-1, 1), }; static float2 vertexUVs[] = { float2(0, 0), float2(1, 0), float2(1, 1), float2(0, 0), float2(1, 1), float2(0, 1), }; struct Attributes { uint vertexID : SV_VERTEXID; }; struct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; float ratio : TEXCOORD1; }; Varyings Vert(Attributes input) { Varyings output = (Varyings)0; int vertexID = input.vertexID % 6; int lineID = input.vertexID / 6; //获取较近的线段节点的世界坐标，和较远的线段节点的世界坐标 float3 vertexPos = _VerticesBuffer[lineID + vertexIndexes[vertexID]]; float3 anotherVert = _VerticesBuffer[lineID + 1 - vertexIndexes[vertexID]]; //转换到裁剪空间 float4 positionCS = mul(UNITY_MATRIX_VP, float4(vertexPos, 1.0)); float4 anotherCS = mul(UNITY_MATRIX_VP, float4(anotherVert, 1.0)); //Unity相机是看向Z轴负方向的，所以裁剪空间w分量小于0 //这里就不需要用从远的节点减去近的节点了 float2 offset = positionCS.xy / positionCS.w - anotherCS.xy / anotherCS.w; //得到长方形宽度 float lengthOffset = length(offset); //要考虑到屏幕的比例 float2 normalizedOffset = normalize(offset * (_ScreenParams.wz - 1.0)); float2 pointOffset = vertexOffsets[vertexID]; float2 pointOffsetX = float2(normalizedOffset.x, normalizedOffset.y) * pointOffset.x; float2 pointOffsetY = float2(-normalizedOffset.y, normalizedOffset.x) * pointOffset.y; //考虑到屏幕的比例，根据上面的两个Offset，得到新的裁剪空间的坐标 float4 newClipPos = float4(-positionCS.w * (pointOffsetX + pointOffsetY) * (_ScreenParams.zw - 1.0) * _OutlineWidth, 0, 0) + positionCS; //算的不一定对，不过也大差不差了。。 float lengthRadius = _OutlineWidth * length(pointOffsetX * (_ScreenParams.zw - 1.0)); output.positionCS = newClipPos; //如果不乘上深度的话，会有透视变形的问题 //https://en.wikipedia.org/wiki/Texture_mapping#Affine_texture_mapping output.uv = vertexUVs[vertexID] * (-newClipPos.w); output.ratio = 2.0 * lengthRadius / (2.0 * lengthRadius + lengthOffset); return output; } float GetDist(float2 uv, float ratio) { uv.x = 0.5 - abs(0.5 - uv.x); float2 coord = float2(uv.x * rcp(ratio), uv.y); float2 center = float2(0.5, 0.5); float distToCenter = length(coord - center); float distToLine = abs(uv.y - 0.5); return coord.x \u0026lt; 0.5 ? distToCenter : distToLine; } float4 Frag(Varyings input) : SV_TARGET { float2 uv = input.uv / (-input.positionCS.w); float distValue = GetDist(uv, input.ratio); //用smoothstep进行抗锯齿 distValue = smoothstep(0.5, _Sharpness * 0.5, distValue); return float4(_OutlineColor.rgb, distValue * _OutlineColor.a); } ENDHLSL SubShader { Tags {\u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34; } ZWrite Off Blend SrcAlpha OneMinusSrcAlpha Pass { HLSLPROGRAM #pragma vertex Vert #pragma fragment Frag ENDHLSL } } } 结语 我对目前的效果还是很满意的！也没看到有什么人做这个画线的操作（除了Shapes，当然我也没去搜索别的案例就是了）。感觉对绘制顶点时的处理和空间变换的理解又大大加深了！下一个目标就是描边的计算了，目前感觉上会和原始模型有深度测试的问题，只能走一步看一步了。\n这段时间一直在思考景深和写ShaderToy，也算收获不少，不过没能写出博客来，拿这个等宽且抗锯齿的世界坐标直线来弥补一下。\n","permalink":"https://zznewclear13.github.io/posts/draw-equal-width-line-in-unity/","summary":"动机 直接动机是想要在unity中制作一个描边效果。对于卡通渲染的描边效果，已经有很多很多的案例了，但是我觉得这些案例不一定能完全满足我的需求，于是想要从画直线开始研究。\n从普通绘画的角度来看，很重要的一点就是描边的宽度基本上是一致的：考虑一下远景的物体，画家在绘画的时候使用和近景相同粗细的笔（用一样或者稍弱的力），绘制一个较不精细的物体，而不是使用很细的笔去绘制一个精细的物体，结论就是远处的描边可能稍细一些，但最好是相同粗细，其颜色可能会变浅。\n从另一个角度来看，描边往往需要能够控制其宽度，对于较细的线，则会有较明显的锯齿（事实上只要角度不太好的描边，就会有很明显的边缘锯齿），那么控制粗细和进行一定程度的抗锯齿也是一个研究的方向。\n从第三个角度，碰巧看到了Freya Holmér制作的Shapes插件，能够绘制高质量的线条画，看上去渲染的效果很好。但是价格过于高昂，于是想要研究一个能够做到差不多效果的工具。\n抗锯齿和宽度的思考 在Unity中画直线有蛮多办法，Debug.DrawLine或者Gizmos.DrawLine都能绘制等宽的直线，不过只能固定一个像素宽，而且因为只有一个像素宽，所以会有明显的锯齿。使用LineRenderer可以绘制任意宽度的直线，写特定的shader通过透明度混合能够防止锯齿的出现（不过线段两端不太好做抗锯齿），但是不能保证在不同角度下直线的宽度相等。那么答案就很明显了，通过线段的顶点的数据，生成两个三角面（一个Quad），在GPU中计算出三角面每个顶点的屏幕空间的位置，确保线段的宽度一致。也由于有宽度的存在，给后续的透明度混合抗锯齿留下了操作的空间。\n具体的绘制线条的操作 首先看一张图（完了三角形顶点顺序画反了，已经积重难返了，代码表现对就当做对的吧） 我们将使用Graphics.DrawProcedural这个方法来绘制我们的直线，每一段直线由两个三角形组成（这里要注意在unity中三角形顶点顺序是顺时针的），也就是说要绘制N条直线的话，要传入N+1个节点位置数据，而实际绘制时会绘制6N个顶点。这样传入Shader的是一个表示线段节点位置的长度是N+1的Vector3数据_VerticesBuffer，和一个表示顶点序号的从0到6N-1的uint数据，也就是在绘制时图形API自动传输的SV_VERTEXID。\n这也就导致了我们的顶点着色器输入数据和普通的Shader有所区别，只有一个uint数据：\nstruct Attributes { uint vertexID : SV_VERTEXID; }; 在顶点着色器获取到SV_VERTEXID之后，我们可以以此来计算出线段的序号lineID和每个顶点在绘制线段时的序号vertexID。在绘制时我们又要知道当前线段的两个节点的位置，在传入_VerticesBuffer之后，我们需要对每个顶点确定其对应的线段节点的序号，即lineID + 0或者lineID + 1，我们可以将这个可以通过vertexID确定的0或者1储存到一个数组vertexIndexes中，方便使用vertexID读取。\n为了让线段取得较好的抗锯齿效果，我们把线段整体外扩了_OutlineWidth个像素宽（实际线段宽度是这个的两倍）。此时对于每一个顶点，我们需要知道外扩之后的屏幕空间的顶点位置.由于我们需要使用从LineStart到LineEnd的方向offset（对于1, 2, 4这几个顶点来说，offset是从LineEnd到LineStart的方向）和与其相垂直的方向来做外扩，我们需要把每个顶点的两个方向值储存到数组vertexOffsets中。可以看到顶点永远是往远离较远节点的方向移动的，因此vertexOffset的x值都是-1。而我们选取offset逆时针旋转90°的方向作为相垂直的方向，因此vertexOffset的y值会有正负之分。\n最后是uv的数值，我们还需要给每个顶点传入uv的数值，就比较简单了，储存到数组vertexUVs中。\n此外还要注意一点，在片元着色器中，我们需要知道LineStart和LineEnd对应的uv位置来做线段的圆形端点，因此我们还需要把这个长方体的高宽比ratio传给片元着色器。\n这样，我们片元着色器的输入数据就是positionCS，uv和ratio了：\nstruct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; float ratio : TEXCOORD1; }; 绘制线条的流程 C#脚本获取要绘制线条的节点数据，传入_VerticesBuffer，调用Graphics.DrawProcedural方法进行绘制。 Shader的顶点着色器根据SV_VERTEXID计算出每个顶点对应的线段序号lineID和顶点序号vertexID。 通过线段序号、顶点序号和vertexIndexes从_VerticesBuffer中找到较近的线段节点的世界坐标，和较远的线段节点的世界坐标。 将两个世界坐标转换到屏幕空间，得到屏幕空间两点对应的向量和与之相垂直的向量，进行归一化。 将每个顶点根据vertexOffsets和描边宽度进行外扩。 将外扩后的裁剪空间的坐标，从vertexUVs中获得的uv和高宽比ratio传给片元着色器。 片元着色器根据ratio和uv，绘制出一个两头圆形的线段。 DrawEqualWidthLine.cs C#脚本比较简单了，没什么特别需要注意的地方。\nusing UnityEngine; public class DrawEqualWidthLine : MonoBehaviour { public Vector3[] vertices; private int vertexCount; public Material equalWidthMaterial; ComputeBuffer verticesBuffer; private void EnsureBuffer(ref ComputeBuffer buffer, int count, int stride) { if (buffer == null) { buffer = new ComputeBuffer(count, stride, ComputeBufferType.","title":"在Unity中绘制等宽线条"},{"content":"制作这个颜色查看器的目的 一直以来我都觉得调试GPU代码是一件很困难的事情，在屏幕上显示的颜色会被限制在0和1之间，所以我很希望能够有那么一个工具，能够让我切切实实的看到输出的数据。如果是写的CPU代码的话，又会对伽马空间的矫正之类的有一些担心，而且可复用程度不够高。因此我就希望能够在GPU传回的图像上直接显示颜色和其对应的数值。\n碰巧之前在Shader Toy上看到了一些在GPU上输出文字的案例，研究了一下之后，就制作了这么一个颜色查看器。\nGPU显示文字的原理 一种是将文字储存到一张贴图上，通过采样这张贴图来显示这些文字，可以通过SDF的方法，自由的调节文字的粗细，也不会有锯齿；另一种是将文字直接用01数据来储存，对应到屏幕上，0的区域显示黑色，1的区域显示白色，本文就采用了这种方式；当然Shader Toy上还有一种是将文字用曲线来储存的，应该是涉及到傅里叶变换。\n本文的标准文字是8*12个像素大，每三行进行合并的话，就是一个24 * 4的01矩阵，这就能将一个文字使用uint4来储存，每个通道还多余8bit。具体可以看Shader Toy案例里的描述，我觉得写的十分直观了。\n但是这个案例里面有一点我觉得不太好的地方，他对于每一个像素，都要计算每一个文字的颜色，也就是说文字越多计算的复杂度越大。我想的是最好能够使用StructuredBuffer的方式，先用一个pass计算要显示的文字，然后对于每一个像素，计算出要显示的文字的编号，再在Structured Buffer中找到对应的文字，最终显示出来。这样每个像素应该之多只需要做一次文字到颜色的计算，能够提高效率。\n颜色查看器具体的操作 首先要先把需要显示的文字对应的uint4值计算出来，这一步之前的案例里已经有现成的数据了。最好是将这些数据组成一个数列，这样我们之后能够用其编号来访问文字对应的数据。 执行一个(1, 1, 1)的Compute Shader，获取要采样的像素点的颜色。对像素点的每一位，都将其转换成要输出的文字对应的编号，储存到_Append_CharacterIndexBuffer中。像本文中需要显示7这个数字的话，需要添加23这个编号到我们的输出数据中。同时，由于每一个像素点有rgba四个通道，但我们输出的是一个RWStructuredBuffer\u0026lt;int\u0026gt;的数据，我们还需要一个列表_RW_LengthBuffer（相当于指针），记录每一个通道对应的数据数列的区间。 对整个画面执行Compute Shader，根据每一个像素点的位置，判断是显示放大的图像、每个通道对应的文字或是原始的画面。如果要显示文字的话，根据需要显示的RGBA通道，找到对应的_RW_LengthBuffer获得编号区间，再找到_Struct_CharacterIndexBuffer中获得对应的编号，通过编号找到对应的文字。最后再根据像素的位置，求出在该文字中应当显示的颜色。 在Unity中我选择了用Volume和Renderer Feature的方法，对后处理之前的画面进行颜色查看的操作。 整体想法还是尽量的减少采样，减少文字到颜色的计算，以及减少分支判断。 Font.hlsl 就是案例中的8*12的文字对应的uint4表，这里将其用数列来保存。\n#define character_spc characterSet[0] #define character_exc characterSet[1] #define character_quo characterSet[2] #define character_hsh characterSet[3] #define character_dol characterSet[4] #define character_pct characterSet[5] #define character_amp characterSet[6] #define character_apo characterSet[7] #define character_lbr characterSet[8] #define character_rbr characterSet[9] #define character_ast characterSet[10] #define character_crs characterSet[11] #define character_com characterSet[12] #define character_dsh characterSet[13] #define character_per characterSet[14] #define character_lsl characterSet[15] #define character_0 characterSet[16] #define character_1 characterSet[17] #define character_2 characterSet[18] #define character_3 characterSet[19] #define character_4 characterSet[20] #define character_5 characterSet[21] #define character_6 characterSet[22] #define character_7 characterSet[23] #define character_8 characterSet[24] #define character_9 characterSet[25] #define character_col characterSet[26] #define character_scl characterSet[27] #define character_les characterSet[28] #define character_equ characterSet[29] #define character_grt characterSet[30] #define character_que characterSet[31] #define character_ats characterSet[32] #define character_A characterSet[33] #define character_B characterSet[34] #define character_C characterSet[35] #define character_D characterSet[36] #define character_E characterSet[37] #define character_F characterSet[38] #define character_G characterSet[39] #define character_H characterSet[40] #define character_I characterSet[41] #define character_J characterSet[42] #define character_K characterSet[43] #define character_L characterSet[44] #define character_M characterSet[45] #define character_N characterSet[46] #define character_O characterSet[47] #define character_P characterSet[48] #define character_Q characterSet[49] #define character_R characterSet[50] #define character_S characterSet[51] #define character_T characterSet[52] #define character_U characterSet[53] #define character_V characterSet[54] #define character_W characterSet[55] #define character_X characterSet[56] #define character_Y characterSet[57] #define character_Z characterSet[58] #define character_lsb characterSet[59] #define character_rsl characterSet[60] #define character_rsb characterSet[61] #define character_pow characterSet[62] #define character_usc characterSet[63] #define character_a characterSet[64] #define character_b characterSet[65] #define character_c characterSet[66] #define character_d characterSet[67] #define character_e characterSet[68] #define character_f characterSet[69] #define character_g characterSet[70] #define character_h characterSet[71] #define character_i characterSet[72] #define character_j characterSet[73] #define character_k characterSet[74] #define character_l characterSet[75] #define character_m characterSet[76] #define character_n characterSet[77] #define character_o characterSet[78] #define character_p characterSet[79] #define character_q characterSet[80] #define character_r characterSet[81] #define character_s characterSet[82] #define character_t characterSet[83] #define character_u characterSet[84] #define character_v characterSet[85] #define character_w characterSet[86] #define character_x characterSet[87] #define character_y characterSet[88] #define character_z characterSet[89] #define character_lpa characterSet[90] #define character_bar characterSet[91] #define character_rpa characterSet[92] #define character_tid characterSet[93] #define character_lar characterSet[94] static int4 characterSet[] = { int4(0x000000,0x000000,0x000000,0x000000), int4(0x003078,0x787830,0x300030,0x300000), int4(0x006666,0x662400,0x000000,0x000000), int4(0x006C6C,0xFE6C6C,0x6CFE6C,0x6C0000), int4(0x30307C,0xC0C078,0x0C0CF8,0x303000), int4(0x000000,0xC4CC18,0x3060CC,0x8C0000), int4(0x0070D8,0xD870FA,0xDECCDC,0x760000), int4(0x003030,0x306000,0x000000,0x000000), int4(0x000C18,0x306060,0x603018,0x0C0000), int4(0x006030,0x180C0C,0x0C1830,0x600000), int4(0x000000,0x663CFF,0x3C6600,0x000000), int4(0x000000,0x18187E,0x181800,0x000000), int4(0x000000,0x000000,0x000038,0x386000), int4(0x000000,0x0000FE,0x000000,0x000000), int4(0x000000,0x000000,0x000038,0x380000), int4(0x000002,0x060C18,0x3060C0,0x800000), int4(0x007CC6,0xD6D6D6,0xD6D6C6,0x7C0000), int4(0x001030,0xF03030,0x303030,0xFC0000), int4(0x0078CC,0xCC0C18,0x3060CC,0xFC0000), int4(0x0078CC,0x0C0C38,0x0C0CCC,0x780000), int4(0x000C1C,0x3C6CCC,0xFE0C0C,0x1E0000), int4(0x00FCC0,0xC0C0F8,0x0C0CCC,0x780000), int4(0x003860,0xC0C0F8,0xCCCCCC,0x780000), int4(0x00FEC6,0xC6060C,0x183030,0x300000), int4(0x0078CC,0xCCEC78,0xDCCCCC,0x780000), int4(0x0078CC,0xCCCC7C,0x181830,0x700000), int4(0x000000,0x383800,0x003838,0x000000), int4(0x000000,0x383800,0x003838,0x183000), int4(0x000C18,0x3060C0,0x603018,0x0C0000), int4(0x000000,0x007E00,0x7E0000,0x000000), int4(0x006030,0x180C06,0x0C1830,0x600000), int4(0x0078CC,0x0C1830,0x300030,0x300000), int4(0x007CC6,0xC6DEDE,0xDEC0C0,0x7C0000), int4(0x003078,0xCCCCCC,0xFCCCCC,0xCC0000), int4(0x00FC66,0x66667C,0x666666,0xFC0000), int4(0x003C66,0xC6C0C0,0xC0C666,0x3C0000), int4(0x00F86C,0x666666,0x66666C,0xF80000), int4(0x00FE62,0x60647C,0x646062,0xFE0000), int4(0x00FE66,0x62647C,0x646060,0xF00000), int4(0x003C66,0xC6C0C0,0xCEC666,0x3E0000), int4(0x00CCCC,0xCCCCFC,0xCCCCCC,0xCC0000), int4(0x007830,0x303030,0x303030,0x780000), int4(0x001E0C,0x0C0C0C,0xCCCCCC,0x780000), int4(0x00E666,0x6C6C78,0x6C6C66,0xE60000), int4(0x00F060,0x606060,0x626666,0xFE0000), int4(0x00C6EE,0xFEFED6,0xC6C6C6,0xC60000), int4(0x00C6C6,0xE6F6FE,0xDECEC6,0xC60000), int4(0x00386C,0xC6C6C6,0xC6C66C,0x380000), int4(0x00FC66,0x66667C,0x606060,0xF00000), int4(0x00386C,0xC6C6C6,0xCEDE7C,0x0C1E00), int4(0x00FC66,0x66667C,0x6C6666,0xE60000), int4(0x0078CC,0xCCC070,0x18CCCC,0x780000), int4(0x00FCB4,0x303030,0x303030,0x780000), int4(0x00CCCC,0xCCCCCC,0xCCCCCC,0x780000), int4(0x00CCCC,0xCCCCCC,0xCCCC78,0x300000), int4(0x00C6C6,0xC6C6D6,0xD66C6C,0x6C0000), int4(0x00CCCC,0xCC7830,0x78CCCC,0xCC0000), int4(0x00CCCC,0xCCCC78,0x303030,0x780000), int4(0x00FECE,0x981830,0x6062C6,0xFE0000), int4(0x003C30,0x303030,0x303030,0x3C0000), int4(0x000080,0xC06030,0x180C06,0x020000), int4(0x003C0C,0x0C0C0C,0x0C0C0C,0x3C0000), int4(0x10386C,0xC60000,0x000000,0x000000), int4(0x000000,0x000000,0x000000,0x00FF00), int4(0x000000,0x00780C,0x7CCCCC,0x760000), int4(0x00E060,0x607C66,0x666666,0xDC0000), int4(0x000000,0x0078CC,0xC0C0CC,0x780000), int4(0x001C0C,0x0C7CCC,0xCCCCCC,0x760000), int4(0x000000,0x0078CC,0xFCC0CC,0x780000), int4(0x00386C,0x6060F8,0x606060,0xF00000), int4(0x000000,0x0076CC,0xCCCC7C,0x0CCC78), int4(0x00E060,0x606C76,0x666666,0xE60000), int4(0x001818,0x007818,0x181818,0x7E0000), int4(0x000C0C,0x003C0C,0x0C0C0C,0xCCCC78), int4(0x00E060,0x60666C,0x786C66,0xE60000), int4(0x007818,0x181818,0x181818,0x7E0000), int4(0x000000,0x00FCD6,0xD6D6D6,0xC60000), int4(0x000000,0x00F8CC,0xCCCCCC,0xCC0000), int4(0x000000,0x0078CC,0xCCCCCC,0x780000), int4(0x000000,0x00DC66,0x666666,0x7C60F0), int4(0x000000,0x0076CC,0xCCCCCC,0x7C0C1E), int4(0x000000,0x00EC6E,0x766060,0xF00000), int4(0x000000,0x0078CC,0x6018CC,0x780000), int4(0x000020,0x60FC60,0x60606C,0x380000), int4(0x000000,0x00CCCC,0xCCCCCC,0x760000), int4(0x000000,0x00CCCC,0xCCCC78,0x300000), int4(0x000000,0x00C6C6,0xD6D66C,0x6C0000), int4(0x000000,0x00C66C,0x38386C,0xC60000), int4(0x000000,0x006666,0x66663C,0x0C18F0), int4(0x000000,0x00FC8C,0x1860C4,0xFC0000), int4(0x001C30,0x3060C0,0x603030,0x1C0000), int4(0x001818,0x181800,0x181818,0x180000), int4(0x00E030,0x30180C,0x183030,0xE00000), int4(0x0073DA,0xCE0000,0x000000,0x000000), int4(0x000000,0x10386C,0xC6C6FE,0x000000), }; ColorInspector.cs 提供文字的颜色，背景的颜色，放大镜的放大倍率，文字的大小这些参数，方便实际使用时进行调整。当游戏不运行的时候，传入需要采样的坐标；当游戏运行的时候，传入鼠标的位置作为采样的坐标。\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenu(\u0026#34;Debugger/Color Inspector\u0026#34;)] public class ColorInspector : VolumeComponent, IPostProcessComponent { public BoolParameter isEnabled = new BoolParameter(false); public ColorParameter textColor = new ColorParameter(Color.white); public ColorParameter backgroundColor = new ColorParameter(Color.black); public Vector2Parameter samplePosition = new Vector2Parameter(new Vector2(200, 400)); public ClampedIntParameter sampleWindowSize = new ClampedIntParameter(20, 1, 100); public ClampedFloatParameter sampleResolution = new ClampedFloatParameter(5f, 1f, 20f); public Vector2Parameter fontSize = new Vector2Parameter(new Vector2(100, 100)); public bool IsActive() { return isEnabled.value; } public bool IsTileCompatible() { return false; } } } ColorInspectorRendererFeature.cs 很普通的将ComputeShader参数暴露给使用者，当场景中存在ColorInspector这个Volume的时候，执行Color Inspector Pass。\nusing UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; using System.Collections.Generic; public class ColorInspectorRendererFeature : ScriptableRendererFeature { [System.Serializable] public class ColorInspectorSetting { public ComputeShader colorInspectorComputeShader; } private ColorInspectorRenderPass colorInspectorRenderPass; public ColorInspectorSetting settings = new ColorInspectorSetting(); public override void Create() { colorInspectorRenderPass = new ColorInspectorRenderPass(settings); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { ColorInspector colorInspector = VolumeManager.instance.stack.GetComponent\u0026lt;ColorInspector\u0026gt;(); if(colorInspector != null) { if(colorInspector.IsActive()) { if (renderingData.cameraData.isSceneViewCamera || renderingData.cameraData.camera.cameraType == CameraType.Game) { colorInspectorRenderPass.Setup(colorInspector); renderer.EnqueuePass(colorInspectorRenderPass); } } } } } ColorInspectorRenderPass.cs 需要执行两次Compute Shader，第一次只执行一个线程，用来获取颜色并输出两个Buffer，一个用于确定RGBA通道对应的编号，另一个用于查找编号对应的文字对应的uint4值，第二次对整个画面进行计算，判断出需要具体绘制的东西。Buffer的创建和销毁可能有点问题，不过关系不大。\nusing System; using UnityEngine; using System.Collections.Generic; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class ColorInspectorRenderPass : ScriptableRenderPass, IDisposable { static int MAX_DEBUG_LENGTH = 35; static int[] zeroArray; private const string profilerTag = \u0026#34;Color Inspector\u0026#34;; private ProfilingSampler inspectSampler = new ProfilingSampler(\u0026#34;Color Inspect Pass\u0026#34;); private RenderTargetHandle cameraColorHandle; private RenderTargetIdentifier cameraColorIden; private static readonly int colorInspectTextureID = Shader.PropertyToID(\u0026#34;_ColorInspectBuffer\u0026#34;); private RenderTargetHandle colorInspectTextureHandle; private RenderTargetIdentifier colorInspectTextureIden; private ColorInspectorRendererFeature.ColorInspectorSetting settings; private ComputeShader colorInspectorComputeShader; private ColorInspector colorInspector; private Vector2Int textureSize; private ComputeBuffer lengthBuffer; private ComputeBuffer characterIndexBuffer; public ColorInspectorRenderPass(ColorInspectorRendererFeature.ColorInspectorSetting settings) { profilingSampler = new ProfilingSampler(profilerTag); this.settings = settings; renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; colorInspectorComputeShader = settings.colorInspectorComputeShader; cameraColorHandle.Init(\u0026#34;_CameraColorTexture\u0026#34;); cameraColorIden = cameraColorHandle.Identifier(); colorInspectTextureHandle.Init(colorInspectTextureID); colorInspectTextureIden = colorInspectTextureHandle.Identifier(); zeroArray = new int[35] {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,}; if (lengthBuffer != null) { lengthBuffer.Release(); } lengthBuffer = new ComputeBuffer(5, 4, ComputeBufferType.Structured); if(characterIndexBuffer != null) { characterIndexBuffer.Release(); } characterIndexBuffer = new ComputeBuffer(MAX_DEBUG_LENGTH, 4, ComputeBufferType.Append); } public void Setup(ColorInspector colorDebugger) { this.colorInspector = colorDebugger; } public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor) { RenderTextureDescriptor desc = cameraTextureDescriptor; textureSize = new Vector2Int(cameraTextureDescriptor.width, cameraTextureDescriptor.height); desc.enableRandomWrite = true; cmd.GetTemporaryRT(colorInspectTextureID, desc); } private void DoDebug(CommandBuffer cmd, RenderTargetIdentifier colorid, RenderTargetIdentifier inspectid, ComputeShader computeShader) { characterIndexBuffer.SetCounterValue(0); characterIndexBuffer.SetData(zeroArray); if (Application.isPlaying) { cmd.SetComputeVectorParam(computeShader, \u0026#34;_SamplePosition\u0026#34;, Input.mousePosition); } else { cmd.SetComputeVectorParam(computeShader, \u0026#34;_SamplePosition\u0026#34;, colorInspector.samplePosition.value); } cmd.SetComputeVectorParam(computeShader, \u0026#34;_TextureSize\u0026#34;, new Vector4(textureSize.x, textureSize.y, 1f / textureSize.x, 1f / textureSize.y)); int fetchColorKernel = computeShader.FindKernel(\u0026#34;FetchMain\u0026#34;); cmd.SetComputeTextureParam(computeShader, fetchColorKernel, \u0026#34;_ColorTexture\u0026#34;, colorid); cmd.SetComputeBufferParam(computeShader, fetchColorKernel, \u0026#34;_Append_CharacterIndexBuffer\u0026#34;, characterIndexBuffer); cmd.SetComputeBufferParam(computeShader, fetchColorKernel, \u0026#34;_RW_LengthBuffer\u0026#34;, lengthBuffer); cmd.DispatchCompute(computeShader, fetchColorKernel, 1, 1, 1); int inspectKernel = computeShader.FindKernel(\u0026#34;ColorInspectMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(inspectKernel, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, inspectKernel, \u0026#34;_ColorTexture\u0026#34;, colorid); cmd.SetComputeBufferParam(computeShader, inspectKernel, \u0026#34;_RW_LengthBuffer\u0026#34;, lengthBuffer); cmd.SetComputeBufferParam(computeShader, inspectKernel, \u0026#34;_Struct_CharacterIndexBuffer\u0026#34;, characterIndexBuffer); cmd.SetComputeTextureParam(computeShader, inspectKernel, \u0026#34;_RW_ColorInspectTexture\u0026#34;, inspectid); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TextColor\u0026#34;, colorInspector.textColor.value.linear); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TextBackgroundColor\u0026#34;, colorInspector.backgroundColor.value.linear); cmd.SetComputeFloatParam(computeShader, \u0026#34;_SampleWindowSize\u0026#34;, (float)colorInspector.sampleWindowSize.value); cmd.SetComputeFloatParam(computeShader, \u0026#34;_SampleResolution\u0026#34;, (float)colorInspector.sampleResolution.value); cmd.SetComputeVectorParam(computeShader, \u0026#34;_FontSize\u0026#34;, colorInspector.fontSize.value); cmd.DispatchCompute(computeShader, inspectKernel, Mathf.CeilToInt((float)textureSize.x / x), Mathf.CeilToInt((float)textureSize.y / y), 1); cmd.Blit(colorInspectTextureIden, cameraColorIden); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(profilerTag); context.ExecuteCommandBuffer(cmd); cmd.Clear(); using (new ProfilingScope(cmd, inspectSampler)) { DoDebug(cmd, cameraColorIden, colorInspectTextureIden, colorInspectorComputeShader); } context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } public override void FrameCleanup(CommandBuffer cmd) { cmd.ReleaseTemporaryRT(colorInspectTextureID); } public void Release() { lengthBuffer.Release(); characterIndexBuffer.Release(); } public void Dispose() { Release(); } } ColorInspectorComputeShader.compute 大部分之前已经讲过了。这里用AppendStructuredBuffer或者普通的StructuredBuffer来保存RGBA通道的编号应该是都可行的。for循环如果不加[unroll]的话，可能会导致计算出来的某几位和实际的数字不相同，就当是个bug好了。colRowIndex和textColRowIndex在计算文字中某个像素的颜色时特别关键（我在Shder Toy上分别记作textColRowIndex和characterColRowIndex，懒得改了），一个是用来得到当前的文字，另一个则是得到当前文字里具体的像素的颜色。\n很需要注意uint和int的除法，有时候弄得很混乱，有时候甚至和floor混用，有点抽象。\n#pragma kernel FetchMain #pragma kernel ColorInspectMain #include \u0026#34;Font.hlsl\u0026#34; Texture2D\u0026lt;float4\u0026gt; _ColorTexture; RWTexture2D\u0026lt;float4\u0026gt; _RW_ColorInspectTexture; RWStructuredBuffer\u0026lt;uint\u0026gt; _RW_LengthBuffer; AppendStructuredBuffer\u0026lt;uint\u0026gt; _Append_CharacterIndexBuffer; StructuredBuffer\u0026lt;uint\u0026gt; _Struct_CharacterIndexBuffer; SamplerState sampler_LinearClamp; float2 _SamplePosition; float4 _TextureSize; float4 _TextColor; float _SampleWindowSize; float _SampleResolution; float2 _FontSize; float4 _TextBackgroundColor; void FloatToCharacterIndex(float floatValue, inout uint totalLength) { float signValue = sign(floatValue); floatValue = signValue * floatValue; if(signValue == -1) { _Append_CharacterIndexBuffer.Append(13); totalLength ++; } if (signValue == 0) { _Append_CharacterIndexBuffer.Append(16); totalLength ++; } else { int valueLength = max((int)log10(floatValue) + 1, 1); //Specify unroll to prevent bugs [unroll(5)] for (int i = 0; i \u0026lt; valueLength; i++) { _Append_CharacterIndexBuffer.Append(((uint)floatValue / (uint)pow(10, valueLength - i - 1)) % 10 + 16); totalLength ++; } } //Handle with two decimals float fracValue = frac(floatValue); _Append_CharacterIndexBuffer.Append(14); totalLength ++; _Append_CharacterIndexBuffer.Append((int)(fracValue * 10) % 10 + 16); totalLength ++; _Append_CharacterIndexBuffer.Append((int)(fracValue * 100) % 10 + 16); totalLength ++; } [numthreads(1, 1, 1)] void FetchMain(uint3 id : SV_DispatchThreadID) { uint2 clamppedPosition = clamp(floor(_SamplePosition), 0, _TextureSize.xy - 1); float4 sampleColor = _ColorTexture.Load(uint3(clamppedPosition, 0)); float4 vectorFour = sampleColor; uint totalLength = 0; _RW_LengthBuffer[0] = 0; FloatToCharacterIndex(vectorFour.x, totalLength); _RW_LengthBuffer[1] = totalLength; FloatToCharacterIndex(vectorFour.y, totalLength); _RW_LengthBuffer[2] = totalLength; FloatToCharacterIndex(vectorFour.z, totalLength); _RW_LengthBuffer[3] = totalLength; FloatToCharacterIndex(vectorFour.w, totalLength); _RW_LengthBuffer[4] = totalLength; } float GetBit(int characterInt, int bitIndex) { //in bBound detection can be removed bool inBound = (bitIndex \u0026gt;= 0) \u0026amp;\u0026amp; (bitIndex \u0026lt;= 23); return inBound ? (characterInt \u0026gt;\u0026gt; bitIndex) \u0026amp; 0x1 : 0.0; } bool GetTextBracket(int2 currentPosition, int2 textWindowUpperLeft, int2 fontSize, inout int2 colRowIndex, inout int2 textColRowIndex) { float2 offset = currentPosition - textWindowUpperLeft; offset.y = -offset.y; colRowIndex = floor(offset * rcp(fontSize)); textColRowIndex = (offset - colRowIndex * fontSize) / (int2)_FontSize; int rowLimit = 3; int colLimit = _RW_LengthBuffer[colRowIndex.y + 1] - _RW_LengthBuffer[colRowIndex.y]; bool inTextBracket = (colRowIndex.y \u0026gt;= 0) \u0026amp;\u0026amp; (colRowIndex.y \u0026lt;= rowLimit) \u0026amp;\u0026amp; (colRowIndex.x \u0026gt;= 0) \u0026amp;\u0026amp; (colRowIndex.x \u0026lt;= colLimit - 1); return inTextBracket; } float4 GetTextColor(int2 currentPosition, int2 colRowIndex, int2 textColRowIndex) { int characterIndex = _RW_LengthBuffer[colRowIndex.y] + colRowIndex.x; int4 character = characterSet[_Struct_CharacterIndexBuffer[characterIndex]]; textColRowIndex = int2(7, 11) - textColRowIndex; int bitIndex = textColRowIndex.x + textColRowIndex.y * 8; int bracket = bitIndex / 24u; float characterWeight = GetBit(character[3 - bracket], bitIndex - bracket * 24); return lerp(_TextBackgroundColor, _TextColor, characterWeight); } float4 GetSampleColor(int2 currentPosition, int2 samplePosition, int resolution) { int2 sampleOffset = currentPosition - samplePosition; int2 sampleCoord = floor((float2)sampleOffset / resolution); float4 sampleColor = _ColorTexture.SampleLevel(sampler_LinearClamp, (samplePosition + sampleCoord + 0.5) * _TextureSize.zw, 0); return sampleColor; } bool CheckInRectangle(uint2 currentPosition, uint2 bottomLeft, uint2 topRight, inout bool onBorder) { onBorder = any(currentPosition == bottomLeft) || any(currentPosition == topRight); return all(currentPosition \u0026gt;= bottomLeft) \u0026amp;\u0026amp; all(currentPosition \u0026lt;= topRight); } [numthreads(8, 8, 1)] void ColorInspectMain(uint3 id : SV_DispatchThreadID) { int2 clamppedPosition = clamp(floor(_SamplePosition), 0, _TextureSize.xy - 1); float4 mainTex = _ColorTexture.Load(int3(id.xy, 0)); int sampleWindowSize = _SampleWindowSize; int halfSampleWindowSize = sampleWindowSize \u0026gt;\u0026gt; 1; int halfSampleWindowSizePlus = (sampleWindowSize ++) \u0026gt;\u0026gt; 1; //Draw sample window bool onBorder = 0; bool inColor = CheckInRectangle(id.xy, clamppedPosition - halfSampleWindowSize, clamppedPosition + halfSampleWindowSizePlus, onBorder); float4 sampleColor = GetSampleColor(id.xy, clamppedPosition, _SampleResolution); sampleColor = lerp(sampleColor, frac(sampleColor + 0.5), onBorder); //Draw text window int2 bracketSize = int2(8, 12) * (int2)_FontSize; int2 colRowIndex = 0; int2 textColRowIndex = 0; int2 textUpperLeftConor = clamppedPosition + halfSampleWindowSizePlus + int2(1, 0); bool inTextBracket = GetTextBracket(id.xy, textUpperLeftConor, bracketSize, colRowIndex, textColRowIndex); float4 textColor = GetTextColor(id.xy, colRowIndex, textColRowIndex); float4 finalColor = lerp(mainTex, sampleColor, inColor); finalColor.rgb = lerp(finalColor.rgb, textColor.rgb, textColor.a * inTextBracket); _RW_ColorInspectTexture[id.xy] = finalColor; } 写后感 好久没有写新的博客啦，好不容易整了点有趣的活。\n在GPU中显示文字确实是个有趣的事情。做的过程中要思考每一个比特的偏移，每一个像素在文字当中的偏移，每一个文字在整个画面中的偏移，确实蛮考验耐心的。使用各种index，利用index来访问数列，就有一种在写普通的C语言的感觉。目前显示的数字只支持从-9999.99到9999.99，不过多了也没什么特别的必要罢了，再细致的话还可以判断颜色是不是NAN和INF，这里就暂不考虑了。\n","permalink":"https://zznewclear13.github.io/posts/gpu-color-inspector/","summary":"制作这个颜色查看器的目的 一直以来我都觉得调试GPU代码是一件很困难的事情，在屏幕上显示的颜色会被限制在0和1之间，所以我很希望能够有那么一个工具，能够让我切切实实的看到输出的数据。如果是写的CPU代码的话，又会对伽马空间的矫正之类的有一些担心，而且可复用程度不够高。因此我就希望能够在GPU传回的图像上直接显示颜色和其对应的数值。\n碰巧之前在Shader Toy上看到了一些在GPU上输出文字的案例，研究了一下之后，就制作了这么一个颜色查看器。\nGPU显示文字的原理 一种是将文字储存到一张贴图上，通过采样这张贴图来显示这些文字，可以通过SDF的方法，自由的调节文字的粗细，也不会有锯齿；另一种是将文字直接用01数据来储存，对应到屏幕上，0的区域显示黑色，1的区域显示白色，本文就采用了这种方式；当然Shader Toy上还有一种是将文字用曲线来储存的，应该是涉及到傅里叶变换。\n本文的标准文字是8*12个像素大，每三行进行合并的话，就是一个24 * 4的01矩阵，这就能将一个文字使用uint4来储存，每个通道还多余8bit。具体可以看Shader Toy案例里的描述，我觉得写的十分直观了。\n但是这个案例里面有一点我觉得不太好的地方，他对于每一个像素，都要计算每一个文字的颜色，也就是说文字越多计算的复杂度越大。我想的是最好能够使用StructuredBuffer的方式，先用一个pass计算要显示的文字，然后对于每一个像素，计算出要显示的文字的编号，再在Structured Buffer中找到对应的文字，最终显示出来。这样每个像素应该之多只需要做一次文字到颜色的计算，能够提高效率。\n颜色查看器具体的操作 首先要先把需要显示的文字对应的uint4值计算出来，这一步之前的案例里已经有现成的数据了。最好是将这些数据组成一个数列，这样我们之后能够用其编号来访问文字对应的数据。 执行一个(1, 1, 1)的Compute Shader，获取要采样的像素点的颜色。对像素点的每一位，都将其转换成要输出的文字对应的编号，储存到_Append_CharacterIndexBuffer中。像本文中需要显示7这个数字的话，需要添加23这个编号到我们的输出数据中。同时，由于每一个像素点有rgba四个通道，但我们输出的是一个RWStructuredBuffer\u0026lt;int\u0026gt;的数据，我们还需要一个列表_RW_LengthBuffer（相当于指针），记录每一个通道对应的数据数列的区间。 对整个画面执行Compute Shader，根据每一个像素点的位置，判断是显示放大的图像、每个通道对应的文字或是原始的画面。如果要显示文字的话，根据需要显示的RGBA通道，找到对应的_RW_LengthBuffer获得编号区间，再找到_Struct_CharacterIndexBuffer中获得对应的编号，通过编号找到对应的文字。最后再根据像素的位置，求出在该文字中应当显示的颜色。 在Unity中我选择了用Volume和Renderer Feature的方法，对后处理之前的画面进行颜色查看的操作。 整体想法还是尽量的减少采样，减少文字到颜色的计算，以及减少分支判断。 Font.hlsl 就是案例中的8*12的文字对应的uint4表，这里将其用数列来保存。\n#define character_spc characterSet[0] #define character_exc characterSet[1] #define character_quo characterSet[2] #define character_hsh characterSet[3] #define character_dol characterSet[4] #define character_pct characterSet[5] #define character_amp characterSet[6] #define character_apo characterSet[7] #define character_lbr characterSet[8] #define character_rbr characterSet[9] #define character_ast characterSet[10] #define character_crs characterSet[11] #define character_com characterSet[12] #define character_dsh characterSet[13] #define character_per characterSet[14] #define character_lsl characterSet[15] #define character_0 characterSet[16] #define character_1 characterSet[17] #define character_2 characterSet[18] #define character_3 characterSet[19] #define character_4 characterSet[20] #define character_5 characterSet[21] #define character_6 characterSet[22] #define character_7 characterSet[23] #define character_8 characterSet[24] #define character_9 characterSet[25] #define character_col characterSet[26] #define character_scl characterSet[27] #define character_les characterSet[28] #define character_equ characterSet[29] #define character_grt characterSet[30] #define character_que characterSet[31] #define character_ats characterSet[32] #define character_A characterSet[33] #define character_B characterSet[34] #define character_C characterSet[35] #define character_D characterSet[36] #define character_E characterSet[37] #define character_F characterSet[38] #define character_G characterSet[39] #define character_H characterSet[40] #define character_I characterSet[41] #define character_J characterSet[42] #define character_K characterSet[43] #define character_L characterSet[44] #define character_M characterSet[45] #define character_N characterSet[46] #define character_O characterSet[47] #define character_P characterSet[48] #define character_Q characterSet[49] #define character_R characterSet[50] #define character_S characterSet[51] #define character_T characterSet[52] #define character_U characterSet[53] #define character_V characterSet[54] #define character_W characterSet[55] #define character_X characterSet[56] #define character_Y characterSet[57] #define character_Z characterSet[58] #define character_lsb characterSet[59] #define character_rsl characterSet[60] #define character_rsb characterSet[61] #define character_pow characterSet[62] #define character_usc characterSet[63] #define character_a characterSet[64] #define character_b characterSet[65] #define character_c characterSet[66] #define character_d characterSet[67] #define character_e characterSet[68] #define character_f characterSet[69] #define character_g characterSet[70] #define character_h characterSet[71] #define character_i characterSet[72] #define character_j characterSet[73] #define character_k characterSet[74] #define character_l characterSet[75] #define character_m characterSet[76] #define character_n characterSet[77] #define character_o characterSet[78] #define character_p characterSet[79] #define character_q characterSet[80] #define character_r characterSet[81] #define character_s characterSet[82] #define character_t characterSet[83] #define character_u characterSet[84] #define character_v characterSet[85] #define character_w characterSet[86] #define character_x characterSet[87] #define character_y characterSet[88] #define character_z characterSet[89] #define character_lpa characterSet[90] #define character_bar characterSet[91] #define character_rpa characterSet[92] #define character_tid characterSet[93] #define character_lar characterSet[94] static int4 characterSet[] = { int4(0x000000,0x000000,0x000000,0x000000), int4(0x003078,0x787830,0x300030,0x300000), int4(0x006666,0x662400,0x000000,0x000000), int4(0x006C6C,0xFE6C6C,0x6CFE6C,0x6C0000), int4(0x30307C,0xC0C078,0x0C0CF8,0x303000), int4(0x000000,0xC4CC18,0x3060CC,0x8C0000), int4(0x0070D8,0xD870FA,0xDECCDC,0x760000), int4(0x003030,0x306000,0x000000,0x000000), int4(0x000C18,0x306060,0x603018,0x0C0000), int4(0x006030,0x180C0C,0x0C1830,0x600000), int4(0x000000,0x663CFF,0x3C6600,0x000000), int4(0x000000,0x18187E,0x181800,0x000000), int4(0x000000,0x000000,0x000038,0x386000), int4(0x000000,0x0000FE,0x000000,0x000000), int4(0x000000,0x000000,0x000038,0x380000), int4(0x000002,0x060C18,0x3060C0,0x800000), int4(0x007CC6,0xD6D6D6,0xD6D6C6,0x7C0000), int4(0x001030,0xF03030,0x303030,0xFC0000), int4(0x0078CC,0xCC0C18,0x3060CC,0xFC0000), int4(0x0078CC,0x0C0C38,0x0C0CCC,0x780000), int4(0x000C1C,0x3C6CCC,0xFE0C0C,0x1E0000), int4(0x00FCC0,0xC0C0F8,0x0C0CCC,0x780000), int4(0x003860,0xC0C0F8,0xCCCCCC,0x780000), int4(0x00FEC6,0xC6060C,0x183030,0x300000), int4(0x0078CC,0xCCEC78,0xDCCCCC,0x780000), int4(0x0078CC,0xCCCC7C,0x181830,0x700000), int4(0x000000,0x383800,0x003838,0x000000), int4(0x000000,0x383800,0x003838,0x183000), int4(0x000C18,0x3060C0,0x603018,0x0C0000), int4(0x000000,0x007E00,0x7E0000,0x000000), int4(0x006030,0x180C06,0x0C1830,0x600000), int4(0x0078CC,0x0C1830,0x300030,0x300000), int4(0x007CC6,0xC6DEDE,0xDEC0C0,0x7C0000), int4(0x003078,0xCCCCCC,0xFCCCCC,0xCC0000), int4(0x00FC66,0x66667C,0x666666,0xFC0000), int4(0x003C66,0xC6C0C0,0xC0C666,0x3C0000), int4(0x00F86C,0x666666,0x66666C,0xF80000), int4(0x00FE62,0x60647C,0x646062,0xFE0000), int4(0x00FE66,0x62647C,0x646060,0xF00000), int4(0x003C66,0xC6C0C0,0xCEC666,0x3E0000), int4(0x00CCCC,0xCCCCFC,0xCCCCCC,0xCC0000), int4(0x007830,0x303030,0x303030,0x780000), int4(0x001E0C,0x0C0C0C,0xCCCCCC,0x780000), int4(0x00E666,0x6C6C78,0x6C6C66,0xE60000), int4(0x00F060,0x606060,0x626666,0xFE0000), int4(0x00C6EE,0xFEFED6,0xC6C6C6,0xC60000), int4(0x00C6C6,0xE6F6FE,0xDECEC6,0xC60000), int4(0x00386C,0xC6C6C6,0xC6C66C,0x380000), int4(0x00FC66,0x66667C,0x606060,0xF00000), int4(0x00386C,0xC6C6C6,0xCEDE7C,0x0C1E00), int4(0x00FC66,0x66667C,0x6C6666,0xE60000), int4(0x0078CC,0xCCC070,0x18CCCC,0x780000), int4(0x00FCB4,0x303030,0x303030,0x780000), int4(0x00CCCC,0xCCCCCC,0xCCCCCC,0x780000), int4(0x00CCCC,0xCCCCCC,0xCCCC78,0x300000), int4(0x00C6C6,0xC6C6D6,0xD66C6C,0x6C0000), int4(0x00CCCC,0xCC7830,0x78CCCC,0xCC0000), int4(0x00CCCC,0xCCCC78,0x303030,0x780000), int4(0x00FECE,0x981830,0x6062C6,0xFE0000), int4(0x003C30,0x303030,0x303030,0x3C0000), int4(0x000080,0xC06030,0x180C06,0x020000), int4(0x003C0C,0x0C0C0C,0x0C0C0C,0x3C0000), int4(0x10386C,0xC60000,0x000000,0x000000), int4(0x000000,0x000000,0x000000,0x00FF00), int4(0x000000,0x00780C,0x7CCCCC,0x760000), int4(0x00E060,0x607C66,0x666666,0xDC0000), int4(0x000000,0x0078CC,0xC0C0CC,0x780000), int4(0x001C0C,0x0C7CCC,0xCCCCCC,0x760000), int4(0x000000,0x0078CC,0xFCC0CC,0x780000), int4(0x00386C,0x6060F8,0x606060,0xF00000), int4(0x000000,0x0076CC,0xCCCC7C,0x0CCC78), int4(0x00E060,0x606C76,0x666666,0xE60000), int4(0x001818,0x007818,0x181818,0x7E0000), int4(0x000C0C,0x003C0C,0x0C0C0C,0xCCCC78), int4(0x00E060,0x60666C,0x786C66,0xE60000), int4(0x007818,0x181818,0x181818,0x7E0000), int4(0x000000,0x00FCD6,0xD6D6D6,0xC60000), int4(0x000000,0x00F8CC,0xCCCCCC,0xCC0000), int4(0x000000,0x0078CC,0xCCCCCC,0x780000), int4(0x000000,0x00DC66,0x666666,0x7C60F0), int4(0x000000,0x0076CC,0xCCCCCC,0x7C0C1E), int4(0x000000,0x00EC6E,0x766060,0xF00000), int4(0x000000,0x0078CC,0x6018CC,0x780000), int4(0x000020,0x60FC60,0x60606C,0x380000), int4(0x000000,0x00CCCC,0xCCCCCC,0x760000), int4(0x000000,0x00CCCC,0xCCCC78,0x300000), int4(0x000000,0x00C6C6,0xD6D66C,0x6C0000), int4(0x000000,0x00C66C,0x38386C,0xC60000), int4(0x000000,0x006666,0x66663C,0x0C18F0), int4(0x000000,0x00FC8C,0x1860C4,0xFC0000), int4(0x001C30,0x3060C0,0x603030,0x1C0000), int4(0x001818,0x181800,0x181818,0x180000), int4(0x00E030,0x30180C,0x183030,0xE00000), int4(0x0073DA,0xCE0000,0x000000,0x000000), int4(0x000000,0x10386C,0xC6C6FE,0x000000), }; ColorInspector.","title":"GPU颜色查看器"},{"content":"2023年4月5日更新 再议高斯模糊，更实用的高斯模糊。\n为什么要用Compute Shader来做高斯模糊 在之前的博客中我说Compute Shader的优势就是快，因为GPU的并行运算比CPU强大很多。但是相比于同样运行在GPU上的Fragment Shader之类，Compute Shader是不是就毫无优势了呢？答案是否定的，在DX11的文档中我们可以看到Thread Group Shared Memory这么一个概念，Compute Shader的Thread Group中的每一个Thread，都可以极快速的访问到对应的Group Shared Memory中的数据，这个效率比采样一张贴图来的高。因此在进行高斯模糊这样的需要大量贴图采样的计算时，先将贴图数据缓存到Group Shared Memory中，再多次访问Group Shared Memory，这样运行的效率会高得多。\n相关的一些参考可以在英伟达的PPT里找到。\nCompute Shader进行高斯模糊的具体操作 这里暂时不使用半分辨率的优化方法，目的是把SrcIden对应的RenderTexture经过高斯模糊储存到DestIden中，这时我们需要一张临时的相同大小的RenderTextureBlurIden。 确定需要的最大的高斯模糊的像素宽度MAX_RADIUS，越大的高斯模糊宽度，需要越大的Group Shared Memory，而Group Shared Memory的大小是有上限的（cs_5.0是32768 bytes）。当然也没有必要在全分辨率的情况下做特别大的高斯模糊就是了。这里我设置最大的高斯模糊像素数是32，即高斯模糊当前像素点和最远采样点之间的距离不能超过32（双线性采样的话还要缩小一个像素）。 高斯模糊往往使用水平和竖直两个高斯核心进行模糊，对应的需要两个Compute Shader Kernel（也可以写成一个，不过思考起来有点绕），我们这里设置两个Kernel，对应水平和竖直两个pass。水平Kernel使用[numthreads(64, 1, 1)]，最高可以是1024，竖直Kernel则使用[numthreads(1, 64, 1)]。 由于高斯模糊中会采样像素点的左右（上下）两侧的像素，Group Shared Memory需要在GroupThreads的基础上向两侧扩大最大高斯模糊像素数MAX_RADIUS，用于保存额外的像素数据。这时我们需要的Group Shared Memory的大小是numthreads + 2 * MAX_RADIUS个，在本文章中是64 + 2 * 32个float3的数据。 GaussianBlur.cs 这里略去Unity SRP的设置pass的操作，仅展示高斯模糊相关的操作。当blurRadius过大时，就能看到明显的多重采样的痕迹了。\nprivate void DoGaussianBlurHorizontal(CommandBuffer cmd, RenderTargetIdentifier srcid, RenderTargetIdentifier dstid, ComputeShader computeShader, float blurRadius) { int gaussianBlurKernel = computeShader.FindKernel(\u0026#34;GaussianBlurHorizontalMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(gaussianBlurKernel, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, gaussianBlurKernel, \u0026#34;_InputTexture\u0026#34;, srcid); cmd.SetComputeTextureParam(computeShader, gaussianBlurKernel, \u0026#34;_OutputTexture\u0026#34;, dstid); cmd.SetComputeFloatParam(computeShader, \u0026#34;_BlurRadius\u0026#34;, blurRadius); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TextureSize\u0026#34;, new Vector4(halfRes.x, halfRes.y, 1f / halfRes.x, 1f / halfRes.y)); cmd.DispatchCompute(computeShader, gaussianBlurKernel, Mathf.CeilToInt((float)halfRes.x / x), Mathf.CeilToInt((float)halfRes.y / y), 1); } private void DoGaussianBlurVertical(CommandBuffer cmd, RenderTargetIdentifier srcid, RenderTargetIdentifier dstid, ComputeShader computeShader, float blurRadius) { int gaussianBlurKernel = computeShader.FindKernel(\u0026#34;GaussianBlurVerticalMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(gaussianBlurKernel, out uint x, out uint y, out uint z); cmd.SetComputeTextureParam(computeShader, gaussianBlurKernel, \u0026#34;_InputTexture\u0026#34;, srcid); cmd.SetComputeTextureParam(computeShader, gaussianBlurKernel, \u0026#34;_OutputTexture\u0026#34;, dstid); cmd.SetComputeFloatParam(computeShader, \u0026#34;_BlurRadius\u0026#34;, blurRadius); cmd.SetComputeVectorParam(computeShader, \u0026#34;_TextureSize\u0026#34;, new Vector4(halfRes.x, halfRes.y, 1f / halfRes.x, 1f / halfRes.y)); cmd.DispatchCompute(computeShader, gaussianBlurKernel, Mathf.CeilToInt((float)halfRes.x / x), Mathf.CeilToInt((float)halfRes.y / y), 1); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(profilerTag); context.ExecuteCommandBuffer(cmd); cmd.Clear(); using (new ProfilingScope(cmd, gaussianBlurSampler)) { DoGaussianBlurHorizontal(cmd, cameraColorIden, blurIden, depthOfFieldComputeShader, blurRadius); DoGaussianBlurVertical(cmd, blurIden, destIden, depthOfFieldComputeShader, blurRadius); cmd.Blit(destIden, cameraColorIden); } context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } GaussianBlur.compute 在Compute Shader的计算中，很需要注意clamp的操作，对于超出屏幕范围的像素，应该使用屏幕边缘的像素的颜色。同时还要注意由于存在如currentPosition - int2(MAX_RADIUS, 0)这样的减法操作，不能随意的使用uint类型的变量，否则出现意料之外的效果时很难查明。\n#pragma kernel GaussianBlurHorizontalMain #pragma kernel GaussianBlurVerticalMain float _BlurRadius; float4 _TextureSize; Texture2D\u0026lt;float4\u0026gt; _InputTexture; RWTexture2D\u0026lt;float4\u0026gt; _OutputTexture; static float gaussian17[] = { 0.00002611081194810, 0.00021522769030413, 0.00133919168719865, 0.00628987509902766, 0.02229954363469697, 0.05967667338326389, 0.12055019394312867, 0.18381709484250766, 0.21157217927735517, 0.18381709484250766, 0.12055019394312867, 0.05967667338326389, 0.02229954363469697, 0.00628987509902766, 0.00133919168719865, 0.00021522769030413, 0.00002611081194810, }; #define MAX_RADIUS 32 groupshared float3 gs_Color[64 + 2 * MAX_RADIUS]; [numthreads(64, 1, 1)] void GaussianBlurHorizontalMain(uint3 groupID : SV_GroupID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { int2 currentPosition = dispatchThreadID.xy; int2 tempPosition = clamp(currentPosition, 0, _TextureSize.xy - 1); gs_Color[groupIndex + MAX_RADIUS] = _InputTexture.Load(uint3(tempPosition, 0)).rgb; if (groupIndex \u0026lt; MAX_RADIUS) { int2 extraSample = currentPosition - int2(MAX_RADIUS, 0); extraSample = clamp(extraSample, 0, _TextureSize.xy - 1); gs_Color[groupIndex] = _InputTexture.Load(uint3(extraSample, 0)).rgb; } if(groupIndex \u0026gt;= 64 - MAX_RADIUS) { int2 extraSample = currentPosition + int2(MAX_RADIUS, 0); extraSample = clamp(extraSample, 0, _TextureSize.xy - 1); gs_Color[groupIndex + 2 * MAX_RADIUS] = _InputTexture.Load(uint3(extraSample, 0)).rgb; } GroupMemoryBarrierWithGroupSync(); float3 color = 0; for (uint i = 0; i \u0026lt; 17; i++) { float weight = gaussian17[i]; float sampleOffset = ((float)i - 8) * _BlurRadius * 0.125; int floorInt = floor(sampleOffset); float lerpValue = sampleOffset - floorInt; float3 sampleColorFloor = gs_Color[groupIndex + MAX_RADIUS + floorInt] ; float3 sampleColorCeil = gs_Color[groupIndex + MAX_RADIUS + floorInt + 1]; float3 sampleColor = lerp(sampleColorFloor, sampleColorCeil, lerpValue); color += sampleColor * weight; } _OutputTexture[dispatchThreadID.xy] = float4(color, 1); } [numthreads(1, 64, 1)] void GaussianBlurVerticalMain(uint3 groupID : SV_GroupID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { int2 currentPosition = dispatchThreadID.xy; int2 tempPosition = clamp(currentPosition, 0, _TextureSize.xy - 1); gs_Color[groupIndex + MAX_RADIUS] = _InputTexture.Load(uint3(tempPosition, 0)).rgb; if (groupIndex \u0026lt; MAX_RADIUS) { int2 extraSample = currentPosition - int2(0, MAX_RADIUS); extraSample = clamp(extraSample, 0, _TextureSize.xy - 1); gs_Color[groupIndex] = _InputTexture.Load(uint3(extraSample, 0)).rgb; } if (groupIndex \u0026gt;= 64 - MAX_RADIUS) { int2 extraSample = currentPosition + int2(0, MAX_RADIUS); extraSample = clamp(extraSample, 0, _TextureSize.xy - 1); gs_Color[groupIndex + 2 * MAX_RADIUS] = _InputTexture.Load(uint3(extraSample, 0)).rgb; } GroupMemoryBarrierWithGroupSync(); float3 color = 0; for (uint i = 0; i \u0026lt; 17; i++) { float weight = gaussian17[i]; float sampleOffset = ((float)i - 8) * _BlurRadius * 0.125; int floorInt = floor(sampleOffset); float lerpValue = sampleOffset - floorInt; float3 sampleColorFloor = gs_Color[groupIndex + MAX_RADIUS + floorInt]; float3 sampleColorCeil = gs_Color[groupIndex + MAX_RADIUS + floorInt + 1]; float3 sampleColor = lerp(sampleColorFloor, sampleColorCeil, lerpValue); color += sampleColor* weight; } _OutputTexture[dispatchThreadID.xy] = float4(color, 1); } 一些限制和思考 Group Shared Memory确实在Compute Shader的优化中有很关键的地位，但是一旦使用了Group shared Memory，会一定程度上降低代码的可阅读性，尤其是看到GroupMemoryBarrierWithGroupSync()的时候，会有莫名的恐慌。但是确实也拓宽了代码设计的思路，有一种透过小洞看到新的一片天地的感觉。在高斯模糊的时候，还会有双线性采样的问题，特别是使用2D的kernel的时候，需要自己来做双线性的采样，不太好debug。\n","permalink":"https://zznewclear13.github.io/posts/accelerate-gaussian-blur-using-group-shared-memory/","summary":"2023年4月5日更新 再议高斯模糊，更实用的高斯模糊。\n为什么要用Compute Shader来做高斯模糊 在之前的博客中我说Compute Shader的优势就是快，因为GPU的并行运算比CPU强大很多。但是相比于同样运行在GPU上的Fragment Shader之类，Compute Shader是不是就毫无优势了呢？答案是否定的，在DX11的文档中我们可以看到Thread Group Shared Memory这么一个概念，Compute Shader的Thread Group中的每一个Thread，都可以极快速的访问到对应的Group Shared Memory中的数据，这个效率比采样一张贴图来的高。因此在进行高斯模糊这样的需要大量贴图采样的计算时，先将贴图数据缓存到Group Shared Memory中，再多次访问Group Shared Memory，这样运行的效率会高得多。\n相关的一些参考可以在英伟达的PPT里找到。\nCompute Shader进行高斯模糊的具体操作 这里暂时不使用半分辨率的优化方法，目的是把SrcIden对应的RenderTexture经过高斯模糊储存到DestIden中，这时我们需要一张临时的相同大小的RenderTextureBlurIden。 确定需要的最大的高斯模糊的像素宽度MAX_RADIUS，越大的高斯模糊宽度，需要越大的Group Shared Memory，而Group Shared Memory的大小是有上限的（cs_5.0是32768 bytes）。当然也没有必要在全分辨率的情况下做特别大的高斯模糊就是了。这里我设置最大的高斯模糊像素数是32，即高斯模糊当前像素点和最远采样点之间的距离不能超过32（双线性采样的话还要缩小一个像素）。 高斯模糊往往使用水平和竖直两个高斯核心进行模糊，对应的需要两个Compute Shader Kernel（也可以写成一个，不过思考起来有点绕），我们这里设置两个Kernel，对应水平和竖直两个pass。水平Kernel使用[numthreads(64, 1, 1)]，最高可以是1024，竖直Kernel则使用[numthreads(1, 64, 1)]。 由于高斯模糊中会采样像素点的左右（上下）两侧的像素，Group Shared Memory需要在GroupThreads的基础上向两侧扩大最大高斯模糊像素数MAX_RADIUS，用于保存额外的像素数据。这时我们需要的Group Shared Memory的大小是numthreads + 2 * MAX_RADIUS个，在本文章中是64 + 2 * 32个float3的数据。 GaussianBlur.cs 这里略去Unity SRP的设置pass的操作，仅展示高斯模糊相关的操作。当blurRadius过大时，就能看到明显的多重采样的痕迹了。\nprivate void DoGaussianBlurHorizontal(CommandBuffer cmd, RenderTargetIdentifier srcid, RenderTargetIdentifier dstid, ComputeShader computeShader, float blurRadius) { int gaussianBlurKernel = computeShader.FindKernel(\u0026#34;GaussianBlurHorizontalMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(gaussianBlurKernel, out uint x, out uint y, out uint z); cmd.","title":"使用Group Shared Memory加速高斯模糊"},{"content":"TAA的原理 首先是要了解画面上的锯齿是如何产生的。锯齿发生在光栅化的阶段，光栅化的时候会丢失掉小于一个像素宽的细节，也就导致了锯齿的产生。\n从字面上来看，TAA (Temporal Anti-Aliasing)的抗锯齿效果来源于Temporal一词，是一种时间上的抗锯齿。TAA会结合当前渲染的画面和之前渲染的画面，通过这两个画面之间的融合，达成抗锯齿的效果。基本思想是在光栅化的时候对画面进行抖动，让亚像素的细节在不同帧渲染到不同的像素上，最后再对这些像素按时间的权重来混合，就能达到抗锯齿的效果。\nTemporal Reprojection Anti-Aliasing Temporal Reprojection Anti-Aliasing是由PlayDead在他们的游戏Inside中使用的一种TAA的方法，他们在GDC2016的演示中分享了这个方法。相较于普通的TAA来说，Temporal Reprojection Anti-Aliasing中使用了Velocity Buffer中的屏幕像素的速度信息和Depth Buffer中对应的屏幕像素的世界坐标信息，这样当物体移动或者相机移动的时候，在做到抗锯齿的同时也减少了TAA带来的拖影效果，同时也把TAA和运动模糊相结合达到更理想的抗锯齿的效果。\nPlayDead提供了对应的源代码。本博客中TAA在SRP中的实现也参考了sienaiwun的TAA代码。\n在Unity SRP中实现TAA的操作 我们通过RendererFeature的方式在渲染管线中加入TAA。在ForwardRendererData中加入RendererFeature后，往Global Volume中添加Temporal Anti-Aliasing以在场景中启用TAA效果。启用TAA效果后，会现在渲染不透明物体之前调用一个Jitter Pass对相机的栅格化阶段进行抖动；在渲染TAA Pass时（在Bloom等跟物体渲染相关的后处理效果之后，在Chromatic Aberration等跟屏幕空间位置相关的后处理效果之前）根据抖动值还原出正常的不抖动的画面，并和AccumTexture进行混合，获得最终的渲染画面。因此我们需要TAARendererFeature、TAAJitterPass、TAARenderPass这三个脚本来处理渲染管线，TemporalAntiAliasing这个脚本来处理Volume，TAAShader这个Shader文件来进行TAA的混合操作。 对栅格化阶段进行抖动，也就相当于是修改了相机的透视变换矩阵的第一第二行的第三位的值，抖动值最好和TexelSize相结合，这样在TAA反向抖动还原正常值的时候，在shader中会比较好写。抖动值和TAA的反向抖动是正比关系，因此可以不需要特别纠结于计算，在shader中传入一个debug值再和抖动值相乘用作反向抖动，观察最后的画面是否存在抖动，就能很好的判断出这两个值的比例了。抖动的方式有很多，纯随机的抖动也可以选择，不过稍不如使用均匀分布的随机抖动的效果好，这里使用Inside中的方式即利用Halton数列进行抖动。 为了让相机移动时也能有较好的抗锯齿效果且削弱拖影现象，Temporal Reprojection Anti-Aliasing需要采样当前的深度贴图，还原出物体的世界空间的坐标，再计算出这个世界空间在AccumTexture中的UV值(Reprojection)，使用这个UV值采样AccumTexture再和当前渲染画面进行融合。 因为Velocity Buffer比较麻烦，这里暂且忽略掉物体移动对TAA带来的影响。 在ScriptableRenderPass中使用cmd.GetTemporaryRT()获得的Render Texture，在当帧过后就会被回收，因此AccumTexture需要使用RenderTexture.GetTemporary()来获取。这里我把AccumTexture放在TemporalAntiAliasing.cs中，方便使用。 TemporalAntiAliasing.cs 除了普通的Volume的设置之外，还需要提供Render Texture的接口。lastFrame的x值和y值分别对应最后渲染画面中对AccumTexture进行线性插值的最小和最大系数。\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenu(\u0026#34;Post-processing/Temporal Anti-Aliasing\u0026#34;)] public class TemporalAntiAliasing : VolumeComponent, IPostProcessComponent { public BoolParameter isEnabled = new BoolParameter(false); public NoInterpFloatRangeParameter lastFrame = new NoInterpFloatRangeParameter(new Vector2(0.2f, 0.8f), 0f, 1f); public Vector2Parameter jitterIntensity = new Vector2Parameter(Vector2.one); private RenderTexture[] accumTextures; public bool IsActive() { return isEnabled.value; } public bool IsTileCompatible() { return false; } void EnsureArray\u0026lt;T\u0026gt;(ref T[] array, int size, T initialValue = default(T)) { if (array == null || array.Length != size) { array = new T[size]; for (int i = 0; i != size; i++) array[i] = initialValue; } } bool EnsureRenderTarget(ref RenderTexture rt, int width, int height, RenderTextureFormat format, FilterMode filterMode, string name, int depthBits = 0, int antiAliasing = 1) { if (rt != null \u0026amp;\u0026amp; (rt.width != width || rt.height != height || rt.format != format || rt.filterMode != filterMode || rt.antiAliasing != antiAliasing)) { RenderTexture.ReleaseTemporary(rt); rt = null; } if (rt == null) { rt = RenderTexture.GetTemporary(width, height, depthBits, format, RenderTextureReadWrite.Default, antiAliasing); rt.name = name; rt.filterMode = filterMode; rt.wrapMode = TextureWrapMode.Clamp; return true;// new target } return false;// same target } public void EnsureRT(RenderTextureDescriptor descriptor) { EnsureArray(ref accumTextures, 2); EnsureRenderTarget(ref accumTextures[0], descriptor.width, descriptor.height, descriptor.colorFormat, FilterMode.Bilinear, \u0026#34;TAA_Accum_One\u0026#34;); EnsureRenderTarget(ref accumTextures[1], descriptor.width, descriptor.height, descriptor.colorFormat, FilterMode.Bilinear, \u0026#34;TAA_Accum_Two\u0026#34;); } public RenderTexture GetRT(int index) { return accumTextures[index]; } } } TAARendererFeature.cs 在TAARendererFeature中生成相机抖动的值，通过TAAJitterPass对相机的投影矩阵进行抖动，通过TAARenderPass反向抖动还原正常的画面。Halton序列的生成方式可以进行优化，这里暂且略过。这里也暂且忽略了DX11和OpenGL的平台差异化处理（无非就是UV的Y轴翻转的问题）。\nnamespace UnityEngine.Rendering.Universal { public class TAARendererFeature : ScriptableRendererFeature { [System.Serializable] public class TAASettings { public bool isEnabled = true; //最好是AfterRenderingPostProcessing，不过会有CameraTarget的问题，需要更多的设置 public RenderPassEvent renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; } public TAASettings settings = new TAASettings(); TAARenderPass taaRenderPass; TAAJitterPass taaJitterPass; private TAAData taaData; private int haltonIndex = 0; private Vector2 lastOffset; private Matrix4x4 lastProj = Matrix4x4.identity; private Matrix4x4 lastView = Matrix4x4.identity; [SerializeField, HideInInspector] private Shader taaShader; public override void Create() { taaShader = Shader.Find(\u0026#34;Hidden/Universal Render Pipeline/TAAShader\u0026#34;); if (taaShader == null) { Debug.LogWarning(\u0026#34;Shader was not found. Please ensure it compiles correctly\u0026#34;); return; } taaData = new TAAData(); taaData.Initialize(); taaJitterPass = new TAAJitterPass(); taaRenderPass = new TAARenderPass(settings); } public struct TAAData { public Vector2 offset; public Vector2 lastOffset; public Matrix4x4 lastProj; public Matrix4x4 lastView; public Matrix4x4 jitteredProj; public Matrix4x4 currentView; public void Initialize() { offset = Vector2.zero; lastOffset = Vector2.zero; lastProj = Matrix4x4.identity; lastView = Matrix4x4.identity; jitteredProj = Matrix4x4.identity; currentView = Matrix4x4.identity; } } private float HaltonSeq(int prime, int index = 1/* NOT! zero-based */) { float r = 0.0f; float f = 1.0f; int i = index; while (i \u0026gt; 0) { f /= prime; r += f * (i % prime); i = (int)Mathf.Floor(i / (float)prime); } return r; } //dx11 only? private Matrix4x4 GetJitteredProjectionMatrix(Camera camera, Vector2 offset, Vector2 jitterIntensity) { Matrix4x4 originalProjMatrix = camera.nonJitteredProjectionMatrix; float near = camera.nearClipPlane; float far = camera.farClipPlane; Vector2 matrixOffset = offset * new Vector2(1f / camera.pixelWidth, 1f / camera.pixelHeight) * jitterIntensity; //[row, column] originalProjMatrix[0, 2] = matrixOffset.x; originalProjMatrix[1, 2] = matrixOffset.y; return originalProjMatrix; } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { Camera camera = renderingData.cameraData.camera; TemporalAntiAliasing taaComponent = VolumeManager.instance.stack.GetComponent\u0026lt;TemporalAntiAliasing\u0026gt;(); if(renderingData.cameraData.cameraType == CameraType.Game \u0026amp;\u0026amp; taaComponent.IsActive()) { // 获取Offset值 haltonIndex = (haltonIndex + 1) \u0026amp; 1023; Vector2 offset = new Vector2( HaltonSeq(2, haltonIndex + 1) - 0.5f, HaltonSeq(3, haltonIndex + 1) - 0.5f); // 获取jittered projection matrix，并记录之前的matrix // jittered projection的ab应该对应0.5 * texel_size.xy lastOffset = taaData.offset; taaData.lastOffset = lastOffset; taaData.offset = new Vector2(offset.x / camera.pixelWidth, offset.y / camera.pixelHeight) * taaComponent.jitterIntensity.value; taaData.jitteredProj = GetJitteredProjectionMatrix(camera, offset, taaComponent.jitterIntensity.value); taaData.lastProj = lastProj; taaData.lastView = lastView; lastProj = camera.projectionMatrix; lastView = camera.worldToCameraMatrix; taaData.currentView = lastView; // 第一个Pass对相机使用jittered projection matrix taaJitterPass.Setup(taaData); renderer.EnqueuePass(taaJitterPass); //第二个Pass执行真正的TAA //暂时不考虑motion blur/运动物体的TAA taaRenderPass.Setup(taaShader, taaData, taaComponent); renderer.EnqueuePass(taaRenderPass); } } } } TAAJitterPass.cs 这个Pass仅用于改变相机的透视变换矩阵，在某些情况下渲染透明物体时会重置透视变换矩阵，这种情况下需要在BeforeRenderingTransparent的时候再额外执行一遍TAAJitterPass。\nnamespace UnityEngine.Rendering.Universal { public class TAAJitterPass : ScriptableRenderPass { private const string profilerTag = \u0026#34;TAA Jitter Pass\u0026#34;; private ProfilingSampler taaSampler = new ProfilingSampler(\u0026#34;TAA Jitter Pass\u0026#34;); private TAARendererFeature.TAAData taaData; public TAAJitterPass() { profilingSampler = taaSampler; renderPassEvent = RenderPassEvent.BeforeRenderingOpaques; } public void Setup(TAARendererFeature.TAAData taaData) { this.taaData = taaData; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(profilerTag); context.ExecuteCommandBuffer(cmd); cmd.Clear(); using (new ProfilingScope(cmd, profilingSampler)) { CameraData cameraData = renderingData.cameraData; cmd.SetViewProjectionMatrices(cameraData.camera.worldToCameraMatrix, taaData.jitteredProj); } context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } } } TAARenderPass.cs 由于_CameraColorTexture没有开启随机读写，使用Compute Shader进行TAA的计算会需要额外的blit，这里就使用普通的shader来进行TAA的操作了。\nusing System; namespace UnityEngine.Rendering.Universal { public class TAARenderPass : ScriptableRenderPass { private const string profilerTag = \u0026#34;My TAA Pass\u0026#34;; private ProfilingSampler taaSampler = new ProfilingSampler(\u0026#34;TAA Pass\u0026#34;); RenderTargetHandle cameraColorHandle; RenderTargetIdentifier cameraColorIden; RenderTargetHandle cameraDepthHandle; RenderTargetIdentifier cameraDepthIden; private Vector2Int screenSize; private int accumIndex; private TAARendererFeature.TAASettings settings; private TAARendererFeature.TAAData taaData; private Material taaMaterial; private TemporalAntiAliasing taaComponent; private Vector2 lastFrame; public TAARenderPass(TAARendererFeature.TAASettings settings) { profilingSampler = new ProfilingSampler(profilerTag); cameraColorHandle.Init(\u0026#34;_CameraColorTexture\u0026#34;); cameraColorIden = cameraColorHandle.Identifier(); cameraDepthHandle.Init(\u0026#34;_CameraDepthTexture\u0026#34;); cameraDepthIden = cameraDepthHandle.Identifier(); this.settings = settings; } public void Setup(Shader taaShader, TAARendererFeature.TAAData taaData, TemporalAntiAliasing taaComponent) { this.taaData = taaData; renderPassEvent = settings.renderPassEvent; this.lastFrame = taaComponent.lastFrame.value; taaMaterial = new Material(taaShader); this.taaComponent = taaComponent; } public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor) { RenderTextureDescriptor descriptor = cameraTextureDescriptor; screenSize = new Vector2Int(descriptor.width, descriptor.height); taaComponent.EnsureRT(descriptor); } private void DoTAA(CommandBuffer cmd, TemporalAntiAliasing taa, RenderTargetIdentifier colorid, int index) { RenderTexture accumRead = taa.GetRT(index); int tempIndex = 1 - index; RenderTexture accumWrite = taa.GetRT(tempIndex); taaMaterial.SetVector(\u0026#34;_TAAOffsets\u0026#34;, new Vector4(taaData.offset.x, taaData.offset.y, taaData.lastOffset.x, taaData.lastOffset.y)); taaMaterial.SetVector(\u0026#34;_TAALastFrame\u0026#34;, taaComponent.lastFrame.value); taaMaterial.SetVector(\u0026#34;_TextureSize\u0026#34;, new Vector4(screenSize.x, screenSize.y, 1f / screenSize.x, 1f / screenSize.y)); Matrix4x4 lastViewProj = taaData.lastProj * taaData.lastView; taaMaterial.SetMatrix(\u0026#34;_LastViewProj\u0026#34;, lastViewProj); taaMaterial.SetTexture(\u0026#34;_AccumTexture\u0026#34;, accumRead); cmd.Blit(colorid, accumWrite, taaMaterial); cmd.Blit(accumWrite, colorid); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(profilerTag); context.ExecuteCommandBuffer(cmd); cmd.Clear(); using (new ProfilingScope(cmd, taaSampler)) { DoTAA(cmd, taaComponent, cameraColorIden, accumIndex); accumIndex = 1 - accumIndex; } context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } public override void FrameCleanup(CommandBuffer cmd) { if (cmd == null) throw new ArgumentNullException(\u0026#34;cmd\u0026#34;); } } } TAAShader.shader 多次采样能够明显的削弱拖影效果，tempMain = min(mainTexture, color_avg * 1.25);这一行能够在一定程度上减少高光的闪烁，clip_aabb和k_feedback似乎用处不是很大，不过我还是写进去了。\nShader \u0026#34;Hidden/Universal Render Pipeline/TAAShader\u0026#34; { Properties { _MainTex(\u0026#34;Main Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34;{} } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl\u0026#34; texture2D _MainTex; texture2D _CameraDepthTexture; texture2D _AccumTexture; SamplerState sampler_LinearClamp; SamplerState sampler_PointClamp; float4 _TAAOffsets; float2 _TAALastFrame; float4 _TextureSize; float4x4 _LastViewProj; struct Attributes { float4 positionOS : POSITION; float2 texcoord : TEXCOORD0; }; struct Varyings { float4 positionCS : SV_POSITION; float2 texcoord : TEXCOORD0; }; float3 clip_aabb(float3 aabb_min, float3 aabb_max, float3 avg, float3 input_texel) { float3 p_clip = 0.5 * (aabb_max + aabb_min); float3 e_clip = 0.5 * (aabb_max - aabb_min) + FLT_EPS; float3 v_clip = input_texel - p_clip; float3 v_unit = v_clip / e_clip; float3 a_unit = abs(v_unit); float ma_unit = max(a_unit.x, max(a_unit.y, a_unit.z)); if (ma_unit \u0026gt; 1.0) return p_clip + v_clip / ma_unit; else return input_texel; } Varyings TAAVert(Attributes input) { Varyings output = (Varyings)0; VertexPositionInputs vertexPositionInputs = GetVertexPositionInputs(input.positionOS.xyz); output.positionCS = vertexPositionInputs.positionCS; output.texcoord = input.texcoord; return output; } float4 TAAFrag(Varyings input) : SV_TARGET { float2 sampleUV = input.texcoord; float2 currentOffset = _TAAOffsets.xy; float2 lastOffset = _TAAOffsets.zw; float2 unJitteredUV = sampleUV - 0.5 * currentOffset; float3 mainTexture = _MainTex.SampleLevel(sampler_LinearClamp, unJitteredUV, 0).rgb; float2 du = float2(_TextureSize.z, 0); float2 dv = float2(0, _TextureSize.w); float3 ctl = _MainTex.Sample(sampler_LinearClamp, unJitteredUV - dv - du).rgb; float3 ctc = _MainTex.Sample(sampler_LinearClamp, unJitteredUV - dv).rgb; float3 ctr = _MainTex.Sample(sampler_LinearClamp, unJitteredUV - dv + du).rgb; float3 cml = _MainTex.Sample(sampler_LinearClamp, unJitteredUV - du).rgb; float3 cmc = _MainTex.Sample(sampler_LinearClamp, unJitteredUV).rgb; float3 cmr = _MainTex.Sample(sampler_LinearClamp, unJitteredUV + du).rgb; float3 cbl = _MainTex.Sample(sampler_LinearClamp, unJitteredUV + dv - du).rgb; float3 cbc = _MainTex.Sample(sampler_LinearClamp, unJitteredUV + dv).rgb; float3 cbr = _MainTex.Sample(sampler_LinearClamp, unJitteredUV + dv + du).rgb; float3 color_min = min(ctl, min(ctc, min(ctr, min(cml, min(cmc, min(cmr, min(cbl, min(cbc, cbr)))))))); float3 color_max = max(ctl, max(ctc, max(ctr, max(cml, max(cmc, max(cmr, max(cbl, max(cbc, cbr)))))))); float3 color_avg = (ctl + ctc + ctr + cml + cmc + cmr + cbl + cbc + cbr) / 9.0; float depthTexture = _CameraDepthTexture.SampleLevel(sampler_PointClamp, unJitteredUV, 0).r; float4 positionNDC = float4(sampleUV * 2 - 1, depthTexture, 1); #if UNITY_UV_STARTS_AT_TOP positionNDC.y = -positionNDC.y; #endif float4 worldPos = mul(UNITY_MATRIX_I_VP, positionNDC); worldPos /= worldPos.w; float4 lastPositionCS = mul(_LastViewProj, worldPos); float2 lastUV = lastPositionCS.xy / lastPositionCS.w; lastUV = lastUV * 0.5 + 0.5; float3 accumTexture = _AccumTexture.SampleLevel(sampler_LinearClamp, lastUV, 0).rgb; float3 tempMain = 0; accumTexture = clip_aabb(color_min, color_max, color_avg, accumTexture); tempMain = min(mainTexture, color_avg * 1.25); float lum0 = Luminance(mainTexture); float lum1 = Luminance(accumTexture); float unbiased_diff = abs(lum0 - lum1) / max(lum0, max(lum1, 0.2)); float unbiased_weight = 1.0 - unbiased_diff; float unbiased_weight_sqr = unbiased_weight * unbiased_weight; float k_feedback = lerp(_TAALastFrame.x, _TAALastFrame.y, unbiased_weight_sqr); float3 returnColor = lerp(tempMain, accumTexture, k_feedback); if (unJitteredUV.x \u0026gt;= 0.5) { returnColor = mainTexture; } return float4(returnColor, 1); } ENDHLSL SubShader { ZTest Always Cull Back ZWrite Off pass { Name \u0026#34;TAA Pass\u0026#34; HLSLPROGRAM #pragma vertex TAAVert #pragma fragment TAAFrag ENDHLSL } } } 一些思考 效果总体来说还是不错的，但是TAAShader中lerp当前渲染画面和AccumTexture的算法应该有待提高，目前特别细长的亚像素特征（如呈线状的高光）会有锯齿爬行的感觉，画面不够稳定，特别小的细节会有一闪一闪的感觉，应该还能再优化优化。PlayDead的源代码有点看不下去。。。就僵在这里了，大概有个80分吧。\n我也想多放图来着，但是实在没啥好放的。。。导致博客越来越枯燥了。\n","permalink":"https://zznewclear13.github.io/posts/temporal-reprojection-anti-aliasing/","summary":"TAA的原理 首先是要了解画面上的锯齿是如何产生的。锯齿发生在光栅化的阶段，光栅化的时候会丢失掉小于一个像素宽的细节，也就导致了锯齿的产生。\n从字面上来看，TAA (Temporal Anti-Aliasing)的抗锯齿效果来源于Temporal一词，是一种时间上的抗锯齿。TAA会结合当前渲染的画面和之前渲染的画面，通过这两个画面之间的融合，达成抗锯齿的效果。基本思想是在光栅化的时候对画面进行抖动，让亚像素的细节在不同帧渲染到不同的像素上，最后再对这些像素按时间的权重来混合，就能达到抗锯齿的效果。\nTemporal Reprojection Anti-Aliasing Temporal Reprojection Anti-Aliasing是由PlayDead在他们的游戏Inside中使用的一种TAA的方法，他们在GDC2016的演示中分享了这个方法。相较于普通的TAA来说，Temporal Reprojection Anti-Aliasing中使用了Velocity Buffer中的屏幕像素的速度信息和Depth Buffer中对应的屏幕像素的世界坐标信息，这样当物体移动或者相机移动的时候，在做到抗锯齿的同时也减少了TAA带来的拖影效果，同时也把TAA和运动模糊相结合达到更理想的抗锯齿的效果。\nPlayDead提供了对应的源代码。本博客中TAA在SRP中的实现也参考了sienaiwun的TAA代码。\n在Unity SRP中实现TAA的操作 我们通过RendererFeature的方式在渲染管线中加入TAA。在ForwardRendererData中加入RendererFeature后，往Global Volume中添加Temporal Anti-Aliasing以在场景中启用TAA效果。启用TAA效果后，会现在渲染不透明物体之前调用一个Jitter Pass对相机的栅格化阶段进行抖动；在渲染TAA Pass时（在Bloom等跟物体渲染相关的后处理效果之后，在Chromatic Aberration等跟屏幕空间位置相关的后处理效果之前）根据抖动值还原出正常的不抖动的画面，并和AccumTexture进行混合，获得最终的渲染画面。因此我们需要TAARendererFeature、TAAJitterPass、TAARenderPass这三个脚本来处理渲染管线，TemporalAntiAliasing这个脚本来处理Volume，TAAShader这个Shader文件来进行TAA的混合操作。 对栅格化阶段进行抖动，也就相当于是修改了相机的透视变换矩阵的第一第二行的第三位的值，抖动值最好和TexelSize相结合，这样在TAA反向抖动还原正常值的时候，在shader中会比较好写。抖动值和TAA的反向抖动是正比关系，因此可以不需要特别纠结于计算，在shader中传入一个debug值再和抖动值相乘用作反向抖动，观察最后的画面是否存在抖动，就能很好的判断出这两个值的比例了。抖动的方式有很多，纯随机的抖动也可以选择，不过稍不如使用均匀分布的随机抖动的效果好，这里使用Inside中的方式即利用Halton数列进行抖动。 为了让相机移动时也能有较好的抗锯齿效果且削弱拖影现象，Temporal Reprojection Anti-Aliasing需要采样当前的深度贴图，还原出物体的世界空间的坐标，再计算出这个世界空间在AccumTexture中的UV值(Reprojection)，使用这个UV值采样AccumTexture再和当前渲染画面进行融合。 因为Velocity Buffer比较麻烦，这里暂且忽略掉物体移动对TAA带来的影响。 在ScriptableRenderPass中使用cmd.GetTemporaryRT()获得的Render Texture，在当帧过后就会被回收，因此AccumTexture需要使用RenderTexture.GetTemporary()来获取。这里我把AccumTexture放在TemporalAntiAliasing.cs中，方便使用。 TemporalAntiAliasing.cs 除了普通的Volume的设置之外，还需要提供Render Texture的接口。lastFrame的x值和y值分别对应最后渲染画面中对AccumTexture进行线性插值的最小和最大系数。\nusing System; namespace UnityEngine.Rendering.Universal { [Serializable, VolumeComponentMenu(\u0026#34;Post-processing/Temporal Anti-Aliasing\u0026#34;)] public class TemporalAntiAliasing : VolumeComponent, IPostProcessComponent { public BoolParameter isEnabled = new BoolParameter(false); public NoInterpFloatRangeParameter lastFrame = new NoInterpFloatRangeParameter(new Vector2(0.2f, 0.8f), 0f, 1f); public Vector2Parameter jitterIntensity = new Vector2Parameter(Vector2.","title":"在Unity SRP中实现TAA效果"},{"content":"为什么要用GPU来进行蒙皮 对于一个SkinnedMeshRenderer，在做蒙皮的时候，对于每一个顶点，会先计算出这个顶点对应的四根骨骼的从骨骼空间到物体空间的矩阵\\(M_{bone\\localtoobject}\\)，然后使用\\(M{bone\\localtoobject} * M{bone\\bindpose} * Vertex{objectspace}\\)得到经过骨骼平移旋转缩放后的四个带权重的顶点数据位置和切线，对于法线则是使用上面矩阵的逆矩阵的转置。然后对获得的位置、法线和切线，用权重计算得到经过骨骼平移旋转缩放后的实际的顶点信息。在通常的渲染过程中，上述操作是在CPU中进行的，最后把顶点数据传递给GPU中进行渲染。在顶点数较多且主要是矩阵运算的情况下，CPU进行蒙皮的效率就不如高并行的GPU了，因此会考虑到在GPU中进行蒙皮处理。\nGPU蒙皮的一些想法 从上面可以看到，要从CPU中传给GPU的数据有以下几种：一是\\(M_{bone\\localtoobject} * M{bone\\_bindpose}\\)这样骨骼数个float4x4的矩阵，但是由于其最后一行是(0, 0, 0, 1)，在传递时可以简化成骨骼数个float3x4矩阵，这些矩阵每一帧都要传递一次；二是每个顶点对应的骨骼编号和骨骼的权重，骨骼编号用来查询骨骼矩阵中对应的矩阵，是一个整型的数据，骨骼权重是一个[0, 1]的小数，可以用\\(BoneIndex + BoneWeight * 0.5\\)的方式，把编号和权重结合成一个float的数据，这样每个顶点的骨骼编号和权重数据是一个float4的数据，可以保存在UV中，也可以用数组的方式传递给GPU，这些顶点数个float4的数据，只需要传递一次就可以了；再有就是模型本身的顶点位置、法线和切线，这些引擎会自动为我们传递给GPU。\n在实际操作中，网上通常找到的方案是把动画保存在一张贴图或者是一个自定义的数据结构中，这里可以直接保存顶点数据，甚至不需要在GPU中做蒙皮的操作，但是随着顶点数增加会占用大量的空间；或者是保存骨骼的变换矩阵，在GPU中进行蒙皮，相对来说储存空间会小很多。然而我认为这两种都不是很好的做GPU skinning的方法，将动画信息保存到贴图或者数据结构中，会很大程度上失去Animator Controller的功能，如两个动作之间的插值、触发事件等，对于动画来说甚至是得不偿失的一种效果。因此，我希望能够保留Animator Controller的特性，实时的把骨骼数据传送给GPU，在GPU中进行蒙皮操作。\nGPU蒙皮的操作 我的想法是，先离线从SkinnedMeshRenderer中获得骨骼的ID和权重，然后实时的从Animator Controller对应的骨骼中获取每根骨骼的骨骼矩阵，再统一传给一个普通的MeshRenderer，在GPU中进行蒙皮的操作。这中间有一个小坑，Unity同一个模型的SkinnedMeshRenderer和MeshRenderer，他们虽然都能获取到boneweight和bindpose，但是SkinnedMeshRenderer和MeshRnederer的骨骼的顺序有时候会有一些差异，因此最好的办法是，抛弃这两者的骨骼顺序，用Hierarchy中的骨骼顺序来确定我们传给GPU的boneindex，boneweight和bonematrix是一致的。\n这里使用的模型及动作是mixamo的hip hop dancing资源。\nBoneMatchInfo.cs 这个脚本的作用是，在离线时把一个GameObjectRoot下的所有SkinnedMeshRenderer和Hierarchy中的骨骼的信息结合起来，保存成一个Asset，用于实时的GPU Skinning。这个Asset包含两部分的信息，一个是BoneMatchNode用于记录Hierarchy骨骼列表中骨骼的名称和其bindpose，另一个是BindIndex用于记录所有SkinnedMeshRenderer的骨骼在Hierarchy骨骼列表中的顺序。\nusing System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEditor; using System; using System.IO; namespace GPUSkinning { [System.Serializable] public class BoneMatchNode { //[HideInInspector] public string boneName; //在查找位于所有Transfom的位置时，设置并使用boneIndex public int boneIndex = 0; public Matrix4x4 bindPose; public BoneMatchNode(string _boneName) { boneName = _boneName; bindPose = Matrix4x4.identity; } } [System.Serializable] public class BindList { //[HideInInspector] public string skinnedMeshName; public int[] bindIndexs; public BindList(string _skinnedMeshName) { skinnedMeshName = _skinnedMeshName; } } public class BoneMatchInfo : ScriptableObject { public BoneMatchNode[] boneMatchNodes; public BindList[] bindLists; } public class GenerateBoneMatchInfo : EditorWindow { public Transform rootBone; public Transform skinnedParent; public int skinnedMeshCount = 0; public SkinnedMeshRenderer[] smrArray; public BoneMatchInfo boneMatchInfo; private Rect topToolBarRect { get { return new Rect(20, 10, position.width - 40, position.height - 20); } } [MenuItem(\u0026#34;zznewclear13/Generate Bone Match Info\u0026#34;)] public static GenerateBoneMatchInfo GetWindow() { GenerateBoneMatchInfo window = GetWindow\u0026lt;GenerateBoneMatchInfo\u0026gt;(); window.titleContent = new GUIContent(\u0026#34;Generate Bone Match Info\u0026#34;); window.Focus(); window.Repaint(); return window; } void OnGUI() { TopToolBar(topToolBarRect); } private void TopToolBar(Rect rect) { GUILayout.BeginArea(rect); rootBone = (Transform)EditorGUILayout.ObjectField(\u0026#34;Root Bone Transform\u0026#34;, rootBone, typeof(Transform), true); skinnedParent = (Transform)EditorGUILayout.ObjectField(\u0026#34;Skinned Parent Transform\u0026#34;, skinnedParent, typeof(Transform), true); if(skinnedParent!=null) { smrArray = skinnedParent.GetComponentsInChildren\u0026lt;SkinnedMeshRenderer\u0026gt;(); if(smrArray != null) { using (new EditorGUI.DisabledGroupScope(true)) { EditorGUILayout.ObjectField(\u0026#34;Skinned Mesh Renderers\u0026#34;, smrArray[0], typeof(SkinnedMeshRenderer), false); for (int i = 1; i \u0026lt; smrArray.Length; i++) { EditorGUILayout.ObjectField(\u0026#34; \u0026#34;, smrArray[i], typeof(SkinnedMeshRenderer), false); } } using (new EditorGUI.DisabledGroupScope(smrArray.Length \u0026lt;= 0)) { if (GUILayout.Button(\u0026#34;Generate Animator Map\u0026#34;, new GUILayoutOption[] { GUILayout.Height(30f) })) { boneMatchInfo = CompareBones(); //LogBindPoses(); Save(); } } } } GUILayout.EndArea(); } private BoneMatchInfo CompareBones() { BoneMatchInfo tempInfo = new BoneMatchInfo(); Transform[] boneTrans = rootBone.GetComponentsInChildren\u0026lt;Transform\u0026gt;(); List\u0026lt;BoneMatchNode\u0026gt; boneMatchNodeList = new List\u0026lt;BoneMatchNode\u0026gt;(); List\u0026lt;BindList\u0026gt; bindLists = new List\u0026lt;BindList\u0026gt;(); List\u0026lt;int[]\u0026gt; tempIntLists = new List\u0026lt;int[]\u0026gt;(); List\u0026lt;Transform[]\u0026gt; smrBoneTransList = new List\u0026lt;Transform[]\u0026gt;(); List\u0026lt;Matrix4x4[]\u0026gt; smrBindPoseList = new List\u0026lt;Matrix4x4[]\u0026gt;(); foreach (SkinnedMeshRenderer smr in smrArray) { Transform[] smrBoneTrans = smr.bones; Matrix4x4[] smrBindPos = smr.sharedMesh.bindposes; smrBoneTransList.Add(smrBoneTrans); smrBindPoseList.Add(smrBindPos); tempIntLists.Add(new int[smr.bones.Length]); } int boneTranIndex = 0; foreach (Transform boneTran in boneTrans) { BoneMatchNode bmn = new BoneMatchNode(boneTran.name); bool isInSMRBones = false; for (int i = 0; i \u0026lt; smrBoneTransList.Count; i++) { int index = Array.IndexOf(smrBoneTransList[i], boneTran); if (index \u0026gt;= 0) { isInSMRBones = true; bmn.bindPose = smrBindPoseList[i][index]; tempIntLists[i][index] = boneTranIndex; } } if (isInSMRBones) { bmn.boneIndex = boneTranIndex; boneMatchNodeList.Add(bmn); boneTranIndex++; } } for (int i = 0; i \u0026lt; smrArray.Length; i++) { bindLists.Add(new BindList(smrArray[i].name)); bindLists[i].bindIndexs = tempIntLists[i]; } tempInfo.boneMatchNodes = boneMatchNodeList.ToArray(); tempInfo.bindLists = bindLists.ToArray(); return tempInfo; } private void LogBindPoses() { using(StreamWriter sw = new StreamWriter(\u0026#34;Assets/GPUSkinning/BindPoses.txt\u0026#34;)) { foreach (SkinnedMeshRenderer smr in smrArray) { Transform[] smrBoneTrans = smr.bones; Matrix4x4[] smrBindPos = smr.sharedMesh.bindposes; for (int j = 0; j \u0026lt; smrBoneTrans.Length; j++) { sw.WriteLine(smr.name + \u0026#34;\\t\u0026#34; + smrBoneTrans[j].name + \u0026#34;\\r\\n\u0026#34; + smrBindPos[j].ToString()); } } } } private void Save() { AssetDatabase.CreateAsset(boneMatchInfo, \u0026#34;Assets/GPUSkinning/BoneMatchInfo.asset\u0026#34;); AssetDatabase.Refresh(); Debug.Log(\u0026#34;\u0026lt;color=blue\u0026gt;Bone Match Info has been saved to Assets/GPUSkinning/BoneMatchInfo.asset.\u0026lt;/color\u0026gt;\u0026#34;); } } } 最后得到的BoneMatchInfo.asset和Hierarchy的关系如图所示，部分不参与实际蒙皮的骨骼，就不需要记录到BoneMatchInfo.asset中了：\nBoneGPUSkinning.cs 在BoneGPUSkinning.cs这个脚本中，要做的事情是：把骨骼的编号和权重写到UV中，只用执行一次；把骨骼矩阵和bindpose的乘积传到GPU中，每帧执行一次，我把这个操作放在了compute shader中进行计算。根据前面的描述，我们需要获取\\(M_{bone\\localtoobject}\\)，这个值等价于\\(M{object\\worldtolocal} * M{bone\\localtoworld}\\)。但是在实际的操作中，获取一根骨骼的\\(M{bone\\_localtoworld}\\)矩阵会导致额外的运算，使得GPU Skinning的效率受到了很大的限制。这里有可能是我不够熟悉Unity的API的原因，当然也有可能是Unity本身就没开放相关的API的原因。照理来说，Unity要把Animator的平移旋转缩放动画应用到每一个骨骼上时，已经计算过了每根骨骼的localToWorldMatrix，获取这个矩阵应该能做到没有任何消耗的。但是没有办法，我只能试图使用Unity Jobs和Unity Burst来加速获取localToWorldMatrix的过程，在我的测试中，相比于直接用for循环获取大概能有至少50%的速度提升（记不太清了），然而对于整个GPU Skinning的过程来说，消耗还是太高了。\n因为是比较久之前写的代码了，也懒得去再仔细地修正，[ExecuteInEditMode]在设置好各个引用之前会疯狂的报错，不过在设置好正确的引用之后重新启用脚本就不会有任何的问题了。似乎操作不当也会出现内存泄漏的问题，不过无伤大雅。\nusing System.Collections.Generic; using UnityEngine; using System; using Unity.Collections; using Unity.Jobs; using UnityEngine.Jobs; using Unity.Burst; namespace GPUSkinning { [ExecuteInEditMode] public class BoneGPUSkinning : MonoBehaviour { public const int BONE_WEIGHT_DECODE_VALUE = 2; public const float BONE_WEIGHT_INVERSE_DECODE_VALUE = 0.5f; public ComputeShader computeShader; public Transform rootBone; public BoneMatchInfo boneMatchInfo; public List\u0026lt;MeshRenderer\u0026gt; meshRenderers; [SerializeField] private int boneSize; private Transform[] minBoneTrans; private TransformAccessArray transformAccessArray; #region ComputeShader int kernel; private Matrix4x4[] bindPosesArray; private Matrix4x4[] LTWMatrixArray; readonly int bindPoseID = Shader.PropertyToID(\u0026#34;_BoneBindPoseBuffer\u0026#34;); readonly int LTWMatrixID = Shader.PropertyToID(\u0026#34;_BoneLTWMatrixBuffer\u0026#34;); readonly int outputBufferID = Shader.PropertyToID(\u0026#34;_BoneOutputBuffer\u0026#34;); private ComputeBuffer outputBuffer; private ComputeBuffer bindPoseBuffer; private ComputeBuffer ltwMatrixBuffer; #endregion #region InitializeFunction void OnEnable() { Initialize(); } /// \u0026lt;summary\u0026gt; /// 全部的初始化 /// \u0026lt;/summary\u0026gt; public void Initialize() { InitializeBoneTrans(); InitializeBoneUV(1); InitializeComputeShader(); } /// \u0026lt;summary\u0026gt; /// 整合所有mesh绑定的骨骼，从rootBone的子物件中找出minBoneTrans /// \u0026lt;/summary\u0026gt; private void InitializeBoneTrans() { //标记每个mesh对应的骨骼 Transform[] allTrans = rootBone.GetComponentsInChildren\u0026lt;Transform\u0026gt;(); Dictionary\u0026lt;string, Transform\u0026gt; allTransDict = new Dictionary\u0026lt;string, Transform\u0026gt;(); foreach (Transform tran in allTrans) { allTransDict.Add(tran.name, tran); } boneSize = boneMatchInfo.boneMatchNodes.Length; minBoneTrans = new Transform[boneSize]; for (int i = 0; i \u0026lt; boneSize; i++) { Transform tempTran = allTransDict[boneMatchInfo.boneMatchNodes[i].boneName]; boneMatchInfo.boneMatchNodes[i].boneIndex = i; minBoneTrans[i] = tempTran; } transformAccessArray = new TransformAccessArray(minBoneTrans); } /// \u0026lt;summary\u0026gt; /// 把骨骼的编号和权重写入到targetUVIndex对应的UV中 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;targetUVIndex\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; public void InitializeBoneUV(int targetUVIndex) { Dictionary\u0026lt;string, BindList\u0026gt; bindListDict = new Dictionary\u0026lt;string, BindList\u0026gt;(); for (int i = 0; i \u0026lt; boneMatchInfo.bindLists.Length; i++) { bindListDict.Add(boneMatchInfo.bindLists[i].skinnedMeshName, boneMatchInfo.bindLists[i]); } for (int i = 0; i \u0026lt; meshRenderers.Count; i++) { BindList tempBindList; bool hasBindList = bindListDict.TryGetValue(meshRenderers[i].name, out tempBindList); if(!hasBindList) { throw new ArgumentException(String.Format(\u0026#34;SkinnedMeshName:{0}在BoneMatchInfo中找不到！\u0026#34;, boneMatchInfo.bindLists[i].skinnedMeshName)); } Mesh mesh = meshRenderers[i].GetComponent\u0026lt;MeshFilter\u0026gt;().sharedMesh; BoneWeight[] boneWeights = mesh.boneWeights; List\u0026lt;Vector4\u0026gt; boneAndWeights = new List\u0026lt;Vector4\u0026gt;(); int[] bindIndexes = tempBindList.bindIndexs; foreach (BoneWeight weight in boneWeights) { Vector4 boneAndWeight = new Vector4(0, 0, 0, 0); //Shader中个BoneUV都会查找全局的骨骼编号 boneAndWeight.x = bindIndexes[weight.boneIndex0] + weight.weight0 * BONE_WEIGHT_INVERSE_DECODE_VALUE; boneAndWeight.y = bindIndexes[weight.boneIndex1] + weight.weight1 * BONE_WEIGHT_INVERSE_DECODE_VALUE; boneAndWeight.z = bindIndexes[weight.boneIndex2] + weight.weight2 * BONE_WEIGHT_INVERSE_DECODE_VALUE; boneAndWeight.w = bindIndexes[weight.boneIndex3] + weight.weight3 * BONE_WEIGHT_INVERSE_DECODE_VALUE; boneAndWeights.Add(boneAndWeight); } mesh.SetUVs(targetUVIndex, boneAndWeights.ToArray()); } } private void EnsureComputeBuffer(ref ComputeBuffer buffer, int count, int stride) { if (buffer != null) { buffer.Release(); } buffer = new ComputeBuffer(count, stride); } /// \u0026lt;summary\u0026gt; /// 初始化ComputeShader，用于计算每根骨骼的矩阵和bindpos的乘积 /// \u0026lt;/summary\u0026gt; private void InitializeComputeShader() { bindPosesArray = new Matrix4x4[boneSize]; LTWMatrixArray = new Matrix4x4[boneSize]; Debug.Log(LTWMatrixArray.Length); for (int i = 0; i \u0026lt; boneSize; i++) { bindPosesArray[i] = boneMatchInfo.boneMatchNodes[i].bindPose; } kernel = computeShader.FindKernel(\u0026#34;MatCompute\u0026#34;); EnsureComputeBuffer(ref outputBuffer, boneSize, sizeof(float) * 16); EnsureComputeBuffer(ref bindPoseBuffer, boneSize, sizeof(float) * 16); EnsureComputeBuffer(ref ltwMatrixBuffer, boneSize, sizeof(float) * 16); bindPoseBuffer.SetData(bindPosesArray); computeShader.SetBuffer(kernel, bindPoseID, bindPoseBuffer); computeShader.SetBuffer(kernel, outputBufferID, outputBuffer); } #endregion #region Update Function void Update() { InvokeComputeShader(); PassMeshRendererMatrix(); } [BurstCompile(CompileSynchronously = true)] struct GetLocalToWorldMatrixStructJob : IJobParallelForTransform { public NativeArray\u0026lt;Matrix4x4\u0026gt; matArray; public void Execute(int i, TransformAccess transform) { matArray[i] = transform.localToWorldMatrix; } } private void InvokeComputeShader() { if(computeShader) { #if true //NativeList\u0026lt;JobHandle\u0026gt; jobHandleList = new NativeList\u0026lt;JobHandle\u0026gt;(Allocator.Temp); NativeArray\u0026lt;Matrix4x4\u0026gt; matArray = new NativeArray\u0026lt;Matrix4x4\u0026gt;(boneSize, Allocator.Persistent); GetLocalToWorldMatrixStructJob job = new GetLocalToWorldMatrixStructJob { matArray = matArray }; JobHandle jobHandle = IJobParallelForTransformExtensions.Schedule(job, transformAccessArray);// job.Schedule(transformAccessArray); jobHandle.Complete(); ltwMatrixBuffer.SetData(matArray); matArray.Dispose(); #else for (int i = 0; i \u0026lt; boneSize; i++) { LTWMatrixArray[i] = minBoneTrans[i].localToWorldMatrix; } ltwMatrixBuffer.SetData(LTWMatrixArray); #endif computeShader.SetBuffer(kernel, LTWMatrixID, ltwMatrixBuffer); int dispatchCount = Mathf.CeilToInt(boneSize / 64f); computeShader.Dispatch(kernel, dispatchCount, 1, 1); Shader.SetGlobalBuffer(\u0026#34;_BoneMatArray\u0026#34;, outputBuffer); } } private void PassMeshRendererMatrix() { for (int i = 0; i \u0026lt; meshRenderers.Count; i++) { foreach (Material mat in meshRenderers[i].sharedMaterials) { mat.SetMatrix(\u0026#34;_BoneTransformMatrix\u0026#34;, meshRenderers[i].transform.worldToLocalMatrix); } } } #endregion private void OnDisable() { outputBuffer.Release(); outputBuffer = null; transformAccessArray.Dispose(); } } } BoneComputeShader.compute 这个compute shader仅仅做了矩阵的运算，甚至都不见得比在CPU中运算要快，不过这边还是使用了compute shader来做这个运算，稍微还能优化的是float4x4可以改成float3x4，不过这样CPU的代码写起来稍乱一些。\n#pragma kernel MatCompute StructuredBuffer\u0026lt;float4x4\u0026gt; _BoneBindPoseBuffer; StructuredBuffer\u0026lt;float4x4\u0026gt; _BoneLTWMatrixBuffer; RWStructuredBuffer\u0026lt;float4x4\u0026gt; _BoneOutputBuffer; [numthreads(64, 1, 1)] void MatCompute(uint3 id : SV_DispatchThreadID) { _BoneOutputBuffer[id.x] = mul(_BoneLTWMatrixBuffer[id.x], _BoneBindPoseBuffer[id.x]); } BoneGPUSkinning.hlsl 操作流程是这样的，对每一个顶点定义一个结构体VertexInputStructure，读取MeshRenderer的原始数据中的位置、法线、切线和我们传入的骨骼编号和权重。使用编号去寻找_BoneMatArray中对应的\\(M_{bone\\localtoworld}\\)，再左乘_BoneTransformMatrix也就是之前说过\\(M{object\\_worldtolocal}\\)，使用这两个矩阵的乘积就能分别计算蒙皮后的顶点位置、切线和法线了，要注意的是法线需要做一次逆矩阵的转置。最后对四对顶点位置、切线和法线进行加权计算，获得最终的顶点位置、切线和法线。\n#ifndef BONE_GPU_SKINNING #define BONE_GPU_SKINNING #define BONE_WEIGHT_DECODE_VALUE 2 StructuredBuffer\u0026lt;float4x4\u0026gt; _BoneMatArray; float4x4 _BoneTransformMatrix; struct VertexInputStructure { float3 positionOS; float3 normalOS; float4 tangentOS; float4 boneUV; }; struct VertexOutputStructure { float3 positionOS; float3 normalOS; float4 tangentOS; }; inline float3x3 InverseTranspose(float3x3 mat) { float determinant = mat._m00 * (mat._m11 * mat._m22 - mat._m12 * mat._m21) - mat._m01 * (mat._m10 * mat._m22 - mat._m12 * mat._m20) + mat._m02 * (mat._m10 * mat._m21 - mat._m11 * mat._m20); float3 vec0 = float3(mat._m11 * mat._m22 - mat._m12 * mat._m21, mat._m12 * mat._m20 - mat._m10 * mat._m22, mat._m10 * mat._m21 - mat._m11 * mat._m20); float3 vec1 = float3(mat._m02 * mat._m21 - mat._m01 * mat._m22, mat._m00 * mat._m22 - mat._m02 * mat._m20, mat._m01 * mat._m20 - mat._m00 * mat._m21); float3 vec2 = float3(mat._m01 * mat._m12 - mat._m02 * mat._m11, mat._m02 * mat._m10 - mat._m00 * mat._m12, mat._m00 * mat._m11 - mat._m01 * mat._m10); float3x3 returnMat; returnMat._m00_m01_m02 = vec0; returnMat._m10_m11_m12 = vec1; returnMat._m20_m21_m22 = vec2; return returnMat / determinant; } inline float3x3 InverseTransposeVec(float3 vec0, float3 vec1, float3 vec2) { float3x3 mat; mat._m00_m01_m02 = vec0; mat._m10_m11_m12 = vec1; mat._m20_m21_m22 = vec2; return InverseTranspose(mat); } inline float4x4 ReadBoneInfos(uint boneIndex) { return mul(_BoneTransformMatrix, _BoneMatArray[boneIndex]); } inline VertexOutputStructure BlendBonesPosNormalTangent(VertexInputStructure input) { float4 positionOS = float4(input.positionOS, 1); float3 normalOS = input.normalOS; float4 tangentOS = input.tangentOS; float4 boneUV = input.boneUV; uint boneIndexOne = floor(boneUV.x); float boneWeightOne = BONE_WEIGHT_DECODE_VALUE * frac(boneUV.x); uint boneIndexTwo = floor(boneUV.y); float boneWeightTwo = BONE_WEIGHT_DECODE_VALUE * frac(boneUV.y); uint boneIndexThree = floor(boneUV.z); float boneWeightThree = BONE_WEIGHT_DECODE_VALUE * frac(boneUV.z); uint boneIndexFour = floor(boneUV.w); float boneWeightFour = BONE_WEIGHT_DECODE_VALUE * frac(boneUV.w); float4x4 matOne = ReadBoneInfos(boneIndexOne); float4x4 matTwo = ReadBoneInfos(boneIndexTwo); float4x4 matThree = ReadBoneInfos(boneIndexThree); float4x4 matFour = ReadBoneInfos(boneIndexFour); //blend position float3 posOne = mul(matOne, positionOS).xyz; float3 posTwo = mul(matTwo, positionOS).xyz; float3 posThree = mul(matThree, positionOS).xyz; float3 posFour = mul(matFour, positionOS).xyz; float3 returnPos = posOne * boneWeightOne + posTwo * boneWeightTwo + posThree * boneWeightThree + posFour * boneWeightFour; //blend normal float3x3 newMatOne = InverseTransposeVec(matOne._m00_m01_m02, matOne._m10_m11_m12, matOne._m20_m21_m22); float3x3 newMatTwo = InverseTransposeVec(matTwo._m00_m01_m02, matTwo._m10_m11_m12, matTwo._m20_m21_m22); float3x3 newMatThree = InverseTransposeVec(matThree._m00_m01_m02, matThree._m10_m11_m12, matThree._m20_m21_m22); float3x3 newMatFour = InverseTransposeVec(matFour._m00_m01_m02, matFour._m10_m11_m12, matFour._m20_m21_m22); float3 normalOne = mul(newMatOne, normalOS).xyz; float3 normalTwo = mul(newMatTwo, normalOS).xyz; float3 normalThree = mul(newMatThree, normalOS).xyz; float3 normalFour = mul(newMatFour, normalOS).xyz; float3 returnNormal = normalOne * boneWeightOne + normalTwo * boneWeightTwo + normalThree * boneWeightThree + normalFour * boneWeightFour; returnNormal = normalize(returnNormal); //blend tangent float3 tangentOne = mul((float3x3)matOne, tangentOS.xyz); float3 tangentTwo = mul((float3x3)matTwo, tangentOS.xyz); float3 tangentThree = mul((float3x3)matThree, tangentOS.xyz); float3 tangentFour = mul((float3x3)matFour, tangentOS.xyz); float3 tempTangent = tangentOne * boneWeightOne + tangentTwo * boneWeightTwo + tangentThree * boneWeightThree + tangentFour * boneWeightFour; tempTangent = normalize(tempTangent); float4 returnTangent = float4(tempTangent, tangentOS.w); VertexOutputStructure output; output.positionOS = returnPos; output.normalOS = returnNormal; output.tangentOS = returnTangent; return output; } #endif BoneGPUSkinningShader.shader 用于渲染的shader中，要在顶点着色器中调用BoneGPUSkinning.hlsl中的方法获取蒙皮后的顶点位置、切线和法线。我这里使用了一个比较简单的渲染，给模型一点基础的光影。要注意的是，如果需要正确的阴影的话，在ShadowCaster这个pass中还需要计算一遍顶点位置，这里就暂且忽略了。\nShader \u0026#34;zznewclear13/BoneGPUSkinningShader\u0026#34; { Properties { _Color (\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _MainTex (\u0026#34;Albedo (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} } SubShader { Tags { \u0026#34;RenderType\u0026#34; = \u0026#34;Opaque\u0026#34; \u0026#34;RenderPipeline\u0026#34; = \u0026#34;UniversalPipeline\u0026#34; \u0026#34;IgnoreProjector\u0026#34; = \u0026#34;True\u0026#34; } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Assets/GPUSkinning/BoneGPUSkinning.hlsl\u0026#34; sampler2D _MainTex; CBUFFER_START(UnityPerMaterial) float4 _Color; CBUFFER_END struct a2v { float4 vertex : POSITION; float2 uv : TEXCOORD0; float4 boneUV : TEXCOORD1; float3 normal : NORMAL; float4 tangent : TANGENT; }; struct v2f { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; float4 tempColor : TEXCOORD1; float3 normalWS : TEXCOORD2; float4 tangentWS : TEXCOORD3; float3 eyeVec : TEXCOORD4; }; v2f animVert(a2v v) { v2f o; VertexInputStructure inputStructure; inputStructure.positionOS = v.vertex; inputStructure.normalOS = v.normal; inputStructure.tangentOS = v.tangent; inputStructure.boneUV = v.boneUV; VertexOutputStructure outputStructure = BlendBonesPosNormalTangent(inputStructure); float3 vertexPos = outputStructure.positionOS; float3 vertexNormal = outputStructure.normalOS; float3 vertexTangent = outputStructure.tangentOS; o.pos = TransformObjectToHClip(vertexPos); o.normalWS = TransformObjectToWorldNormal(vertexNormal); o.tangentWS = float4(TransformObjectToWorldDir(vertexTangent), v.tangent.w); o.uv = v.uv; o.tempColor = float4(1, 1, 1, 1); float3 worldPos = TransformObjectToWorld(float4(vertexPos, 1)); o.eyeVec = GetCameraPositionWS() - worldPos; return o; } float4 animFrag(v2f i) : SV_TARGET { float3 viewDir = normalize(i.eyeVec); float3 lightDir = normalize(float3(1, 1, 1)); float3 halfVec = normalize(lightDir + viewDir); float3 normalWS = normalize(i.normalWS); float NdotL = dot(normalWS, lightDir) * 0.6 + 0.4; float NdotH = saturate(dot(normalWS, halfVec)); float3 diffuseColor = _Color.xyz * NdotL; float3 specularColor = pow(NdotH, 30); return float4(diffuseColor + specularColor, 1); } ENDHLSL pass { Tags{ \u0026#34;LightMode\u0026#34; = \u0026#34;UniversalForward\u0026#34; } HLSLPROGRAM #pragma vertex animVert #pragma fragment animFrag ENDHLSL } } } 最后的思考 总的来说，上面的操作已经基本完成了GPU Skinning的需求，而且能够正确的与Animator组件相结合。当然仍有优化的空间，比如说把所有要做GPU Skinning的mesh使用同一个Update的方法来更新，等等。但是美中不足的是之前提到的获取localToWorldMatrix的问题，直接导致了使用这个方法不如Unity自带的GPU Skinning效率高，自带的GPU蒙皮应该是用到了Unity底层的一些优化吧，但是核心的操作应该和我这里做的差不多。\n","permalink":"https://zznewclear13.github.io/posts/unity-gpu-skinning-with-animator-controller/","summary":"为什么要用GPU来进行蒙皮 对于一个SkinnedMeshRenderer，在做蒙皮的时候，对于每一个顶点，会先计算出这个顶点对应的四根骨骼的从骨骼空间到物体空间的矩阵\\(M_{bone\\localtoobject}\\)，然后使用\\(M{bone\\localtoobject} * M{bone\\bindpose} * Vertex{objectspace}\\)得到经过骨骼平移旋转缩放后的四个带权重的顶点数据位置和切线，对于法线则是使用上面矩阵的逆矩阵的转置。然后对获得的位置、法线和切线，用权重计算得到经过骨骼平移旋转缩放后的实际的顶点信息。在通常的渲染过程中，上述操作是在CPU中进行的，最后把顶点数据传递给GPU中进行渲染。在顶点数较多且主要是矩阵运算的情况下，CPU进行蒙皮的效率就不如高并行的GPU了，因此会考虑到在GPU中进行蒙皮处理。\nGPU蒙皮的一些想法 从上面可以看到，要从CPU中传给GPU的数据有以下几种：一是\\(M_{bone\\localtoobject} * M{bone\\_bindpose}\\)这样骨骼数个float4x4的矩阵，但是由于其最后一行是(0, 0, 0, 1)，在传递时可以简化成骨骼数个float3x4矩阵，这些矩阵每一帧都要传递一次；二是每个顶点对应的骨骼编号和骨骼的权重，骨骼编号用来查询骨骼矩阵中对应的矩阵，是一个整型的数据，骨骼权重是一个[0, 1]的小数，可以用\\(BoneIndex + BoneWeight * 0.5\\)的方式，把编号和权重结合成一个float的数据，这样每个顶点的骨骼编号和权重数据是一个float4的数据，可以保存在UV中，也可以用数组的方式传递给GPU，这些顶点数个float4的数据，只需要传递一次就可以了；再有就是模型本身的顶点位置、法线和切线，这些引擎会自动为我们传递给GPU。\n在实际操作中，网上通常找到的方案是把动画保存在一张贴图或者是一个自定义的数据结构中，这里可以直接保存顶点数据，甚至不需要在GPU中做蒙皮的操作，但是随着顶点数增加会占用大量的空间；或者是保存骨骼的变换矩阵，在GPU中进行蒙皮，相对来说储存空间会小很多。然而我认为这两种都不是很好的做GPU skinning的方法，将动画信息保存到贴图或者数据结构中，会很大程度上失去Animator Controller的功能，如两个动作之间的插值、触发事件等，对于动画来说甚至是得不偿失的一种效果。因此，我希望能够保留Animator Controller的特性，实时的把骨骼数据传送给GPU，在GPU中进行蒙皮操作。\nGPU蒙皮的操作 我的想法是，先离线从SkinnedMeshRenderer中获得骨骼的ID和权重，然后实时的从Animator Controller对应的骨骼中获取每根骨骼的骨骼矩阵，再统一传给一个普通的MeshRenderer，在GPU中进行蒙皮的操作。这中间有一个小坑，Unity同一个模型的SkinnedMeshRenderer和MeshRenderer，他们虽然都能获取到boneweight和bindpose，但是SkinnedMeshRenderer和MeshRnederer的骨骼的顺序有时候会有一些差异，因此最好的办法是，抛弃这两者的骨骼顺序，用Hierarchy中的骨骼顺序来确定我们传给GPU的boneindex，boneweight和bonematrix是一致的。\n这里使用的模型及动作是mixamo的hip hop dancing资源。\nBoneMatchInfo.cs 这个脚本的作用是，在离线时把一个GameObjectRoot下的所有SkinnedMeshRenderer和Hierarchy中的骨骼的信息结合起来，保存成一个Asset，用于实时的GPU Skinning。这个Asset包含两部分的信息，一个是BoneMatchNode用于记录Hierarchy骨骼列表中骨骼的名称和其bindpose，另一个是BindIndex用于记录所有SkinnedMeshRenderer的骨骼在Hierarchy骨骼列表中的顺序。\nusing System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEditor; using System; using System.IO; namespace GPUSkinning { [System.Serializable] public class BoneMatchNode { //[HideInInspector] public string boneName; //在查找位于所有Transfom的位置时，设置并使用boneIndex public int boneIndex = 0; public Matrix4x4 bindPose; public BoneMatchNode(string _boneName) { boneName = _boneName; bindPose = Matrix4x4.","title":"支持Animator Controller的实时GPU蒙皮"},{"content":"为什么要从深度图重建世界坐标 一个很大的应用情景是在后处理的阶段，或是计算一些屏幕空间的效果（如SSR、SSAO等），只能获取到一张深度贴图，而不是每一个几何体的顶点数据，很多的计算中却又需要用到世界坐标或者是视空间的坐标，这时我们就需要通过深度图来重建世界空间的坐标。\n重建世界坐标的流程 首先要获取屏幕空间的UV，这里记为positionSS，范围是(0, 1)(0, 1)。 使用UV采样深度贴图，获取到当前的深度值。 使用UV和深度值，得到标准化设备坐标，这里记为positionNDC。 使用裁剪空间到视空间的变换矩阵乘以positionNDC，除以W分量，得到视空间坐标，这里记为positionVS。 使用视空间到世界空间的变换矩阵乘以positionVS，得到世界空间坐标，这里记为positionWS。 这里使用DepthToPositionShader.shader，假装是屏幕后处理的shader，来演示一下重建世界坐标的流程，这样比直接写屏幕后处理的shader能够更好的去了解Unity的空间变换的方式。\n这个shader有以下几个需要注意的点：\n为了使用_CameraDepthTexture这张深度贴图，需要在srp的设置中开启Depth Texture这个选项。这样子在渲染的时候会在DepthPrePass用shader中的Depth Only这个pass去先渲染出深度贴图。我们就能够在渲染物体的时候直接拿到包含当前物体的深度贴图了。 顶点着色器和片元着色器中的SV_POSITION并不完全相同。对于顶点着色器来说，SV_POSITION就是之前所说的\\((\\frac X {\\tan {\\frac {fovy} 2} \\cdot \\frac x y }, -\\frac Y {\\tan {\\frac {fovy} 2}}, \\frac {Zn} {f - n} + \\frac {fn} {f - n}, -Z)\\)；但是在片元着色器中，SV_POSITION的XY分量会乘上屏幕的宽高，Z分量则是已经除以W之后的深度值。屏幕的宽高信息保存在_ScreenParams这个内置的变量中，它的前两位是屏幕的宽高像素数，后两位是宽高的像素数的倒数加一。 要针对DX11和OpenGL不同的透视变换矩阵来调整UV的Y分量的数值，也就是要注意UNITY_UV_STARTS_AT_TOP这个宏的使用。出现获得的坐标跟随着摄像机的移动发生奇怪的倾斜的时候，往往都是忘记对Y分量的平台差异进行处理。 最后得到的视空间和世界空间的坐标值，要记得除以这个坐标值的W分量，相当于是做了一次归一化，才能得到正确的坐标。 DepthToPositionShader.shader Shader \u0026#34;zznewclear13/DepthToPositionShader\u0026#34; { Properties { [Toggle(REQUIRE_POSITION_VS)] _Require_Position_VS(\u0026#34;Require Position VS\u0026#34;, float) = 0 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/core.hlsl\u0026#34; #pragma multi_compile _ REQUIRE_POSITION_VS sampler2D _CameraDepthTexture; struct Attributes { float4 positionOS : POSITION; float2 texcoord : TEXCOORD0; }; struct Varyings { float4 positionCS : SV_POSITION; float2 texcoord : TEXCOORD0; }; Varyings Vert(Attributes input) { Varyings output = (Varyings)0; VertexPositionInputs vertexPositionInputs = GetVertexPositionInputs(input.positionOS.xyz); output.positionCS = vertexPositionInputs.positionCS; output.texcoord = input.texcoord; return output; } float4 Frag(Varyings input) : SV_TARGET { float2 positionSS = input.positionCS.xy * (_ScreenParams.zw - 1); float depth = tex2D(_CameraDepthTexture, positionSS).r; float3 positionNDC = float3(positionSS * 2 - 1, depth); #if UNITY_UV_STARTS_AT_TOP positionNDC.y = -positionNDC.y; #endif #if REQUIRE_POSITION_VS float4 positionVS = mul(UNITY_MATRIX_I_P, float4(positionNDC, 1)); positionVS /= positionVS.w; float4 positionWS = mul(UNITY_MATRIX_I_V, positionVS); #else float4 positionWS = mul(UNITY_MATRIX_I_VP, float4(positionNDC, 1)); positionWS /= positionWS.w; #endif return positionWS; } float4 DepthFrag(Varyings input) : SV_TARGET { return 0; } ENDHLSL SubShader { Tags{ \u0026#34;RenderType\u0026#34; = \u0026#34;Opaque\u0026#34; } LOD 100 Pass { Tags{\u0026#34;LightMode\u0026#34;=\u0026#34;UniversalForward\u0026#34;} ZWrite On ZTest LEqual HLSLPROGRAM #pragma vertex Vert #pragma fragment Frag ENDHLSL } Pass { Tags{\u0026#34;LightMode\u0026#34; = \u0026#34;DepthOnly\u0026#34;} ZWrite On ZTest LEqual HLSLPROGRAM #pragma vertex Vert #pragma fragment DepthFrag ENDHLSL } } } ","permalink":"https://zznewclear13.github.io/posts/get-world-space-position-from-depth-texture/","summary":"为什么要从深度图重建世界坐标 一个很大的应用情景是在后处理的阶段，或是计算一些屏幕空间的效果（如SSR、SSAO等），只能获取到一张深度贴图，而不是每一个几何体的顶点数据，很多的计算中却又需要用到世界坐标或者是视空间的坐标，这时我们就需要通过深度图来重建世界空间的坐标。\n重建世界坐标的流程 首先要获取屏幕空间的UV，这里记为positionSS，范围是(0, 1)(0, 1)。 使用UV采样深度贴图，获取到当前的深度值。 使用UV和深度值，得到标准化设备坐标，这里记为positionNDC。 使用裁剪空间到视空间的变换矩阵乘以positionNDC，除以W分量，得到视空间坐标，这里记为positionVS。 使用视空间到世界空间的变换矩阵乘以positionVS，得到世界空间坐标，这里记为positionWS。 这里使用DepthToPositionShader.shader，假装是屏幕后处理的shader，来演示一下重建世界坐标的流程，这样比直接写屏幕后处理的shader能够更好的去了解Unity的空间变换的方式。\n这个shader有以下几个需要注意的点：\n为了使用_CameraDepthTexture这张深度贴图，需要在srp的设置中开启Depth Texture这个选项。这样子在渲染的时候会在DepthPrePass用shader中的Depth Only这个pass去先渲染出深度贴图。我们就能够在渲染物体的时候直接拿到包含当前物体的深度贴图了。 顶点着色器和片元着色器中的SV_POSITION并不完全相同。对于顶点着色器来说，SV_POSITION就是之前所说的\\((\\frac X {\\tan {\\frac {fovy} 2} \\cdot \\frac x y }, -\\frac Y {\\tan {\\frac {fovy} 2}}, \\frac {Zn} {f - n} + \\frac {fn} {f - n}, -Z)\\)；但是在片元着色器中，SV_POSITION的XY分量会乘上屏幕的宽高，Z分量则是已经除以W之后的深度值。屏幕的宽高信息保存在_ScreenParams这个内置的变量中，它的前两位是屏幕的宽高像素数，后两位是宽高的像素数的倒数加一。 要针对DX11和OpenGL不同的透视变换矩阵来调整UV的Y分量的数值，也就是要注意UNITY_UV_STARTS_AT_TOP这个宏的使用。出现获得的坐标跟随着摄像机的移动发生奇怪的倾斜的时候，往往都是忘记对Y分量的平台差异进行处理。 最后得到的视空间和世界空间的坐标值，要记得除以这个坐标值的W分量，相当于是做了一次归一化，才能得到正确的坐标。 DepthToPositionShader.shader Shader \u0026#34;zznewclear13/DepthToPositionShader\u0026#34; { Properties { [Toggle(REQUIRE_POSITION_VS)] _Require_Position_VS(\u0026#34;Require Position VS\u0026#34;, float) = 0 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/core.hlsl\u0026#34; #pragma multi_compile _ REQUIRE_POSITION_VS sampler2D _CameraDepthTexture; struct Attributes { float4 positionOS : POSITION; float2 texcoord : TEXCOORD0; }; struct Varyings { float4 positionCS : SV_POSITION; float2 texcoord : TEXCOORD0; }; Varyings Vert(Attributes input) { Varyings output = (Varyings)0; VertexPositionInputs vertexPositionInputs = GetVertexPositionInputs(input.","title":"从深度图中获取世界空间的坐标"},{"content":"空间变换和平台差异 将一个物体渲染到我们的屏幕上，需要经过一系列的坐标变换，这些坐标变换是在shader中使用矩阵进行计算的。变换的顺序如下，从物体空间(Object Space)到世界空间(World Space)，从世界空间到视空间(View pace)，从相机控件到裁剪空间(Clip Space)，最后显示到我们的屏幕空间(Screen Space)。\n这一部分讲的空间变换是在Unity之外的空间变换，具体的Unity的空间变换还需要看这里。\n物体空间到世界空间 这里用到的矩阵相对比较简单。我们用\\(R\\), \\(T\\)和\\(S\\)来分别代表物体的旋转平移和缩放系数，\\(R\\)由物体的三个旋转角决定，这里的格式是一个3x3的矩阵，\\(T\\)和\\(S\\)是两个1x3的向量，用\\(P\\)来代表物体空间的一个点。那么这个点在世界空间中的坐标就可以使用下面的式子进行计算： $$ P_{world} = \\begin{pmatrix} R_{00} S_x\u0026amp; R_{01}\u0026amp; R_{02}\u0026amp; T_x \\cr R_{10}\u0026amp; R_{11} S_y\u0026amp; R_{12}\u0026amp; T_y \\cr R_{20}\u0026amp; R_{21}\u0026amp; R_{22} S_z\u0026amp; T_z \\cr 0\u0026amp; 0\u0026amp; 0\u0026amp; 1 \\end{pmatrix} \\times \\begin{pmatrix} P_x \\cr P_y \\cr P_z \\cr 1 \\end{pmatrix} $$ 这个4x4的矩阵就是物体空间到世界空间的变换矩阵，这里记为\\(M_{o\\to w}\\)，在\\(P\\)的基础上额外增加了一个维度的这个向量，是\\(P\\)对应的齐次坐标。对于普通的向量\\(V\\)（如物体表面切线方向、视线方向、光源方向）等，从物体空间变换到世界空间时，齐次坐标的高次位应当为0，上式变为： $$ V_{world} = \\begin{pmatrix} R_{00} S_x\u0026amp; R_{01}\u0026amp; R_{02}\u0026amp; T_x \\cr R_{10}\u0026amp; R_{11} S_y\u0026amp; R_{12}\u0026amp; T_y \\cr R_{20}\u0026amp; R_{21}\u0026amp; R_{22} S_z\u0026amp; T_z \\cr 0\u0026amp; 0\u0026amp; 0\u0026amp; 1 \\end{pmatrix} \\times \\begin{pmatrix} V_x \\cr V_y \\cr V_z \\cr 0 \\end{pmatrix} $$ 要注意的是：将物体空间的法线\\(N\\)变换到世界空间时，应当使用\\(M_{o\\to w}\\)的逆矩阵的转置\\((M_{o\\to w}^{-1})^T\\)，也就是世界空间到物体空间的变换矩阵的转置\\((M_{w\\to o})^T\\)乘物体空间的法线\\((M_{w\\to o})^T \\times N\\)，这条式子等价于\\(N \\times M_{w\\to o}\\)。而对于正交矩阵来说，其逆矩阵就是这个矩阵的转置，因此可以像普通向量一样处理法线。\n世界空间到视空间 和上面的物体空间到世界空间基本一致，但是从这里开始坐标系的手性会对变换使用的矩阵产生巨大的影响，可以说是一切混乱的开端。在OpenGL这样的右手系图形API中，摄像机往往看向Z轴的负方向，因此物体的视空间坐标的Z分量是负值；而对于左手系的坐标系，如DirectX的注明左手系的函数（D3DXMatrixLookAtLH），摄像机看向Z轴的正方向，物体的视空间坐标的Z分量是正值。\n视空间到裁剪空间 从这里开始就不再是像之前那样简单的坐标变换了，这里以最常用到的透视变换为例。\n摄像机的参数如fov、远近裁剪面在这个坐标变换的过程中影响了最终的结果，我们这里用\\(fovy\\)表示摄像机竖直方向上张开的角度，用\\(\\frac y x\\)表示摄像机图像的高宽比（AspectRatio，在DirectX中往往用的是\\(\\frac x y\\)），用\\(near\\)和\\(far\\)表示摄像机的远近裁剪面。这里给出一张OpenGL的摄像机裁剪空间示意图，来自OpenGL Performer™ Getting Started Guide：\n对于OpenGL，使用的透视变换矩阵如下：\n$$ \\begin{pmatrix} \\frac 1 {\\tan {\\frac {fovy} 2} \\cdot \\frac x y }\u0026amp; 0\u0026amp; 0\u0026amp; 0 \\cr 0\u0026amp; \\frac 1 {\\tan {\\frac {fovy} 2}}\u0026amp; 0\u0026amp; 0 \\cr 0\u0026amp; 0\u0026amp; - \\frac {f + n} {f - n}\u0026amp; - \\frac {2fn} {f - n} \\cr 0\u0026amp; 0\u0026amp; -1\u0026amp; 0 \\end{pmatrix} $$\n对于视空间点\\(P = (X, Y, Z, 1)\\)（这里的Z往往是负值），这个矩阵将其变换到\n$$ P_{clip space} = (\\frac X {\\tan {\\frac {fovy} 2} \\cdot \\frac x y }, \\frac Y {\\tan {\\frac {fovy} 2}}, -\\frac {Z(f + n)} {f - n} - \\frac {2fn} {f - n}, -Z) $$\n将这个新的坐标的XYZ分量除以W分量，就能把XYZ坐标在(-X, X)(-Y, Y)(-N, -F)范围的点映射到(-1, 1)(-1, 1)(-1, 1)了。\n对于左手系的坐标系，如DirectX中注明左手系的D3DXMatrixPerspectiveFovLH，使用的透视变换矩阵如下，这里相当于对DirectX的CPU中的矩阵进行了转置（和Shader中的矩阵一致），这样更好理解：\n$$ \\begin{pmatrix} \\frac 1 {\\tan {\\frac {fovy} 2} \\cdot \\frac x y }\u0026amp; 0\u0026amp; 0\u0026amp; 0 \\cr 0\u0026amp; \\frac 1 {\\tan {\\frac {fovy} 2}}\u0026amp; 0\u0026amp; 0 \\cr 0\u0026amp; 0\u0026amp; \\frac f {f - n}\u0026amp; -\\frac {fn} {f - n} \\cr 0\u0026amp; 0\u0026amp; 1\u0026amp; 0 \\end{pmatrix} $$ 对于视空间点\\(P = (X, Y, Z, 1)\\)（这里的Z往往是正值），这个矩阵将其变换到 $$ P_{clip space} = (\\frac X {\\tan {\\frac {fovy} 2} \\cdot \\frac x y }, \\frac Y {\\tan {\\frac {fovy} 2}}, \\frac {Zf} {f - n} - \\frac {fn} {f - n}, Z) $$\n将这个新的坐标的XYZ分量除以W分量，就能把XYZ坐标在(-X, X)(-Y, Y)(N, F)范围的点映射到(-1, 1)(-1, 1)(0, 1)了。\n这里可以看到左手系DirectX和OpenGL的区别：首先是视空间坐标Z分量的区别，DirectX是正值，而OpenGL是负值；其次是计算得到的深度值的区别，DirectX近裁剪面是0，远裁剪面是1，OpenGL近裁剪面是-1，远裁剪面是1。同时因为浮点数精度在0的时候比较高，为了让线性的深度能够尽可能平均的分布到浮点数中，会有Reverse Z的考虑，即会使远裁剪面为0，近裁剪面为1，对于DirectX，只需要交换near和far的数值就可以了。\n裁剪空间到屏幕空间 虽然经过上一步变换的坐标的，除以W分量后的XYZ分量乘2减1（DirectX的Z分量不需要乘2减1），已经是我们想要的标准化设备坐标(Normalized Device Coordinate, NDC)了：\n//DX11 float3 Pndc = float3((Pclip.xy / Pclip.w) * 0.5 + 0.5, Pclip.z / Pclip.w); //OpenGL float3 Pndc = (Pclip.xyz / Pclip.w) * 0.5 + 0.5; 由于裁剪空间并不是线性变化的，考虑到从顶点着色器到片元着色器时的线性插值会受到透视变形的影响，Unity中的NDC只做了乘2减1这一步，而没有除以W分量。\n相关的代码可以在ShaderVariablesFunctions.hlsl中看到：\nVertexPositionInputs GetVertexPositionInputs(float3 positionOS) { VertexPositionInputs input; input.positionWS = TransformObjectToWorld(positionOS); input.positionVS = TransformWorldToView(input.positionWS); input.positionCS = TransformWorldToHClip(input.positionWS); float4 ndc = input.positionCS * 0.5f; input.positionNDC.xy = float2(ndc.x, ndc.y * _ProjectionParams.x) + ndc.w; input.positionNDC.zw = input.positionCS.zw; return input; } _ProjectionParams.x用于处理DX11和OpenGL的Y分量相反的情况，类似于UNITY_UV_STARTS_AT_TOP。先乘0.5再加W分量，等价于上面的计算NDC的式子不除以W分量。\n切线空间到世界空间 在使用法线贴图的时候，也经常会用到切线空间和世界空间的转换。和之前的转换不同的是，我们并不能从预先设定好的宏中获取切线空间到世界空间的变换矩阵，而是知道世界空间的切线Tangent、双切线Bitangent和法线Normal的值，这里记为\\(T, B, N\\)。我们可以利用这三个世界空间 的向量来计算切线空间中向量\\(V\\)的世界空间表示： $$ V_{world} = \\begin{pmatrix} T_x\u0026amp; B_x\u0026amp; N_x \\cr T_y\u0026amp; B_y\u0026amp; N_y \\cr T_z\u0026amp; B_z\u0026amp; N_z \\end{pmatrix} \\times \\begin{pmatrix} V_x \\cr V_y \\cr V_z \\end{pmatrix} $$\n这里也稍微解释一下为什么Unity在计算双切线的时候要乘上切线的w分量。一般情况下切线是uv的x轴正方向，双切线是uv的y轴正方向。由于部分建模软件的手性与Unity不一致，Unity中使用法线叉乘切线计算得到的双切线与建模软件生产出的模型的uv的y轴正方向就会相反，这样在采样建模软件生产的法线贴图的时候，由于两者相反，计算出来的凹凸效果看上去就会是反着的。为了解决这个问题，Unity在模型导入检测到手性不一致的时候，会将切线的w分量设置成-1，这样在做切线空间的变换的时候，模型的uv能正确对应切线和双切线，从而可以得到正确的变换后的法线。\nUnity中的空间变换 为此Unity定义了一系列的宏用来记录这些变换中对应的矩阵：UNITY_MATRIX_M, UNITY_MATRIX_V, UNITY_MATRIX_P以及这些矩阵的逆矩阵和这些矩阵相乘所得的矩阵，这些宏的定义可以在Input.hlsl中找到。\n同时Unity也定义了一系列的方便进行坐标转换的函数，可以在SpaceTransforms.hlsl中找到。\n全平台和Unity历史遗留的矛盾 Unity最一开始是MacOS上的游戏引擎，因此Unity最初是使用OpenGL的，这也就导致了一开始Unity就使用的是右手系的坐标系。随着逐渐的发展，Unity也使用了DirectX之类的左手系的图形API，甚至Unity的世界空间就是一个左手系的坐标系。但是出于某些原因（积重难返），Unity的视空间矩阵UNITY_MATRIX_V使用的是OpenGL的右手系矩阵（相机看向Z轴负方向）。也就是说使用UNITY_MATRIX_V计算得到的视空间的坐标，其Z分量往往是小于0的。而unity_WorldToCamera才是正常应当使用的相机的变换矩阵（相机看向Z轴正方向的左手系矩阵）。这也就导致了UNITY_MATRIX_P和上面所说的矩阵也略有不同，除了Reverse Z的操作之外，在DX11的API下，Unity翻转了UV的Y分量，并且把投影矩阵第四行第三列的1改成了-1。\n详情可以看bgolus的解释。\n这里也放一下Unity中的别扭的DX11的矩阵：\n$$ M_{\\text{UNITY\\_MATRIX\\_P}} = \\begin{pmatrix} \\frac 1 {\\tan {\\frac {fovy} 2} \\cdot \\frac x y }\u0026amp; 0\u0026amp; 0\u0026amp; 0 \\cr 0\u0026amp; -\\frac 1 {\\tan {\\frac {fovy} 2}}\u0026amp; 0\u0026amp; 0 \\cr 0\u0026amp; 0\u0026amp; \\frac n {f - n}\u0026amp; \\frac {fn} {f - n} \\cr 0\u0026amp; 0\u0026amp; -1\u0026amp; 0 \\end{pmatrix} $$\n对于视空间点\\(P = (X, Y, Z, 1)\\)（这里的Z往往是负值），这个矩阵将其变换到\n$$ P_{clip space} = (\\frac X {\\tan {\\frac {fovy} 2} \\cdot \\frac x y }, -\\frac Y {\\tan {\\frac {fovy} 2}}, \\frac {Zn} {f - n} + \\frac {fn} {f - n}, -Z) $$\n将这个新的坐标的XYZ分量除以W分量，就能把XYZ坐标在(-X, X)(-Y, Y)(-N, -F)范围的点映射到(-1, 1)(1, -1)(1, 0)了。也就是说这个矩阵翻转了UV的Y分量，并且使用了Reverse Z来保证深度的精度。\n行矩阵和列矩阵 在GPU运算中，HLSL使用的是行矩阵（与之相反，GLSL使用的是列矩阵）。在Shader中输入float4x4 M_temp = float4x4(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p)，得到的矩阵是这样的：\n$$ M_{temp} = \\begin{pmatrix} a\u0026amp; b\u0026amp; c\u0026amp; d \\cr e\u0026amp; f\u0026amp; g\u0026amp; h \\cr i\u0026amp; j\u0026amp; k\u0026amp; l \\cr m\u0026amp; n\u0026amp; o\u0026amp; p \\end{pmatrix} $$\n如果输入的是float4x4 M_temp = float4x4(row0, row1, row2, row3)，得到的矩阵是这样的：\n$$ M_{temp} = \\begin{pmatrix} {row0}_x\u0026amp; {row0}_y\u0026amp; {row0}_z\u0026amp; {row0}_w \\cr {row1}_x\u0026amp; {row1}_y\u0026amp; {row1}_z\u0026amp; {row1}_w \\cr {row2}_x\u0026amp; {row2}_y\u0026amp; {row2}_z\u0026amp; {row2}_w \\cr {row3}_x\u0026amp; {row3}_y\u0026amp; {row3}_z\u0026amp; {row3}_w \\end{pmatrix} $$\n因此，我们常常说的TBN矩阵，输入的是float3x3 TBN = float3x3(tangent, bitangent, normal)，其对应的矩阵是：\n$$ M_{TBN} = \\begin{pmatrix} {tangent}_x\u0026amp; {tangent}_y\u0026amp; {tangent}_z \\cr {bitangent}_x\u0026amp; {bitangent}_y\u0026amp; {bitangent}_z \\cr {normal}_x\u0026amp; {normal}_y\u0026amp; {normal}_z \\end{pmatrix} $$\n这实际上是切线空间到世界空间的矩阵的转置，因此在计算的时候使用向量左乘这个矩阵mul(dirTS, TBN)能够将切线空间的向量转换到世界空间。\n顺带一提，HLSL中矩阵的成员变量是这样排列的：\n$$ M_{temp} = \\begin{pmatrix} \\_m00\u0026amp; \\_m01\u0026amp; \\_m02\u0026amp; \\_m03 \\cr \\_m10\u0026amp; \\_m11\u0026amp; \\_m12\u0026amp; \\_m13 \\cr \\_m20\u0026amp; \\_m21\u0026amp; \\_m22\u0026amp; \\_m23 \\cr \\_m30\u0026amp; \\_m31\u0026amp; \\_m32\u0026amp; \\_m33 \\end{pmatrix} $$\n这边有一小段Shadertoy代码，可以用来快速判断当前是行矩阵还是列矩阵，复制到Unity中也能用，亮色为当前矩阵与选定的矩阵类型相同：\nGLSL Column Major\n2024年3月15日修订 2022年修订的时候可想不到两年后我还得回来改这篇文章。当时写的时候一定是GLSL写太多了，而且并没有真正地用代码去检验是行矩阵还是列矩阵，以至于最近在重新做视差映射时，用列矩阵去看之前写的代码的时候，怎么想都觉得不对劲。实在是汗流浃背了。\n2022年1月25日修订 只能说反转再反转吧，最一开始我就认为Unity的相机是看向Z轴负方向的（因为当时UNITY_MATRIX_V用的比较多），直到昨天在编辑器的窗口里看了一眼，发现在窗口里相机是看向Z轴正方向的（那么大一个蓝色的箭头），然后就开始了深深的自我怀疑。\n直到询问了bgolus之后才真正的确定，Unity的视空间的变换矩阵UNITY_MATRIX_V是一个右手系的矩阵，其相机看向Z轴的负方向。只能说Unity确实有些别扭了。。。\n","permalink":"https://zznewclear13.github.io/posts/unity-space-transformation-overview/","summary":"空间变换和平台差异 将一个物体渲染到我们的屏幕上，需要经过一系列的坐标变换，这些坐标变换是在shader中使用矩阵进行计算的。变换的顺序如下，从物体空间(Object Space)到世界空间(World Space)，从世界空间到视空间(View pace)，从相机控件到裁剪空间(Clip Space)，最后显示到我们的屏幕空间(Screen Space)。\n这一部分讲的空间变换是在Unity之外的空间变换，具体的Unity的空间变换还需要看这里。\n物体空间到世界空间 这里用到的矩阵相对比较简单。我们用\\(R\\), \\(T\\)和\\(S\\)来分别代表物体的旋转平移和缩放系数，\\(R\\)由物体的三个旋转角决定，这里的格式是一个3x3的矩阵，\\(T\\)和\\(S\\)是两个1x3的向量，用\\(P\\)来代表物体空间的一个点。那么这个点在世界空间中的坐标就可以使用下面的式子进行计算： $$ P_{world} = \\begin{pmatrix} R_{00} S_x\u0026amp; R_{01}\u0026amp; R_{02}\u0026amp; T_x \\cr R_{10}\u0026amp; R_{11} S_y\u0026amp; R_{12}\u0026amp; T_y \\cr R_{20}\u0026amp; R_{21}\u0026amp; R_{22} S_z\u0026amp; T_z \\cr 0\u0026amp; 0\u0026amp; 0\u0026amp; 1 \\end{pmatrix} \\times \\begin{pmatrix} P_x \\cr P_y \\cr P_z \\cr 1 \\end{pmatrix} $$ 这个4x4的矩阵就是物体空间到世界空间的变换矩阵，这里记为\\(M_{o\\to w}\\)，在\\(P\\)的基础上额外增加了一个维度的这个向量，是\\(P\\)对应的齐次坐标。对于普通的向量\\(V\\)（如物体表面切线方向、视线方向、光源方向）等，从物体空间变换到世界空间时，齐次坐标的高次位应当为0，上式变为： $$ V_{world} = \\begin{pmatrix} R_{00} S_x\u0026amp; R_{01}\u0026amp; R_{02}\u0026amp; T_x \\cr R_{10}\u0026amp; R_{11} S_y\u0026amp; R_{12}\u0026amp; T_y \\cr R_{20}\u0026amp; R_{21}\u0026amp; R_{22} S_z\u0026amp; T_z \\cr 0\u0026amp; 0\u0026amp; 0\u0026amp; 1 \\end{pmatrix} \\times \\begin{pmatrix} V_x \\cr V_y \\cr V_z \\cr 0 \\end{pmatrix} $$ 要注意的是：将物体空间的法线\\(N\\)变换到世界空间时，应当使用\\(M_{o\\to w}\\)的逆矩阵的转置\\((M_{o\\to w}^{-1})^T\\)，也就是世界空间到物体空间的变换矩阵的转置\\((M_{w\\to o})^T\\)乘物体空间的法线\\((M_{w\\to o})^T \\times N\\)，这条式子等价于\\(N \\times M_{w\\to o}\\)。而对于正交矩阵来说，其逆矩阵就是这个矩阵的转置，因此可以像普通向量一样处理法线。","title":"Unity空间变换总览"},{"content":"什么是有向距离场以及它能用来干什么 有向距离场记录的是从一个点到集合边界的的距离值，其值的正负对应该点在集合外部或内部。有向距离场有很广的应用范围可以用来简单的生成Voronoi图形，可以用来做全局光照的计算，可以用来做两个形状的平滑的变形，可以用来做高清晰度的字体，也可以用来做Ray March（虽然我认为不如直接光线追踪求交来的效率高）。像原神就使用了SDF的方法，生成了角色脸部的阴影图，从而让角色脸部的阴影能自然的变化。\n那么什么又是Jump Flooding Algorithm呢 Jump Flooding Algorithm是荣国栋在他的博士论文Jump Flooding Algorithm On Graphics Hardware And Its Applications提出的一种在GPU上运行的能够快速传播某个像素的信息到其他像素的算法。\n普通的Flooding算法在一次运行中，固定向相邻的一个像素的像素传播信息，而Jump Flooding则是按照2的幂次递增或是递减来传播信息。这和之前提到的并行计算——Reduction的想法差不多。下图演示了普通的Flooding和Jump Flooding的过程： 使用JFA计算一张2D图片对应的SDF贴图 首先在Unity中创建JumpFlooding.cs, JFAComputeShader.compute, 和JFAVisualize.shader，分别用来执行Compute Shader，使用JFA算法计算SDF和可视化JFA算法的结果。\n这里使用一张RGB通道为灰色，Alpha通道写着“JFA”的贴图作为我们2D图片的输入。 整体思路和需要注意的事项 先从简单实现功能上来考虑，暂时忽略掉抗锯齿的需求，直接对Alpha通道的值按照0.5来划分出图形的内部和外部，大于等于0.5为外部，小于0.5为内部。 SDF需要要计算距离，这里使用像素点中心到另一个像素点中心的距离（也可以使用像素点左下角到另一个像素点左下角的距离，不过为了明确起见，还是加上这半个像素的偏移比较好）。距离可以用uv的大小来表示，也可以用像素数量来表示，针对图像长宽不同的情况，这里以一个像素宽度为1来表示两个像素点的距离。在最后采样JFA的Render Texture的时候，也要注意使用sampler_PointClamp来进行采样，计算距离时也不能仅仅使用uv来计算，而是要使用像素中间的点的位置来进行计算。 普通的JFA算法会使用到Render Texture的两个通道，来标记像素对应的最近边界像素的UV，由于记录的是UV的数值而不是颜色信息，Render Texture要储存在线性空间中。由于要同时计算内部和外部的点到边界的有向距离，JFA算法会使用到Render Texture全部的四个通道，这里使用前两位记录位于内部的像素对应的边界像素的坐标，用后两位记录位于外部的像素对应的边界像素的坐标，即对于内部的像素(nearestUV.x, nearestUV.y, Z, W)，对于外部的像素(X, Y, nearestUV.x, nearestUV.y)，XYZW则可以用来表示该点为最初始的内外部的点、包含JFA传递的信息的点。不包含JFA传递的信息的点。 在JFA计算之前，需要先让贴图对应的点包含JFA信息，也就是说，对于内部的点初始化为(UV.x, UV.y, -1, -1)，对于外部的点初始化为(-1, -1, UV.x, UV.y)。这要求我们使用的Render Texture格式为至少R16G16B16A16 SFloat。 在JFA的计算中，以外部的像素点为例，所进行的操作是：采样上次经过JFA操作的Render Texture；根据XY通道判断该点在边界的内部还是外部，内部就跳过；对外部的像素点，通过XY通道判断是否已包含JFA的信息，如果包含，根据ZW通道计算出当前点到其包含的最近像素的距离，如果不包含，将这个距离设置为一个极大的常数；分别采样距离像素点2的幂次的距离的八个像素，判断这些像素是否已包含JFA的信息，如果不包含，采样下一个点，如果包含，根据这些像素的ZW通道计算出当前点到这些像素包含的最近像素的距离，并和上一步算出的距离进行比较，如果小于上一步算出的距离，则证明该像素对应的最近像素为周围点包含的像素，更新该像素的ZW通道，并且将XY通道标记成已包含JFA的信息。 采样周边像素的步长从2D贴图的长宽的一半向上取整开始，每次JFA都取上一步步长的一半向上取整作为新的步长，一直进行到步长为(1, 1)，进行最后一次JFA计算。 经过JFA计算之后，还需要将分别表示内部的点对应的最近像素的UV和外部的点对应的最近像素的UV结合起来，储存为一张贴图，可以是(nearestUV.x, nearestUV.y, 0, inside?1:0)，也可以是(distance * inside?-1:1, 1)。 应该是跟当前平台有关，有时候会出现贴图上下颠倒的情况，可以用UNITY_UV_STARTS_AT_TOP来协助解决，不过compute shader可能需要自己启用这个宏，这里就直接硬写在shader里，不做平台判断了。 JFAComputeShader.compute 根据上面的整体思路，我们需要三个kernel，一个用来初始化，一个做JFA计算，最后一个用来合成最后的贴图。Compute Shader的关键字需要Unity 2020以上（我也不知道具体哪个版本）才能有，这里就暂时用UNITY_2020_2_OR_NEWER这个宏来屏蔽了。\n#pragma kernel CopyUVMain #define PIXEL_OFFSET 0.5 Texture2D\u0026lt;float4\u0026gt; _InputTexture; RWTexture2D\u0026lt;float4\u0026gt; _OutputTexture; float4 _TextureSize; float _Channel; [numthreads(8, 8, 1)] void CopyUVMain(uint3 id : SV_DispatchThreadID) { float2 samplePosition = id.xy + PIXEL_OFFSET; if (any(samplePosition \u0026gt;= _TextureSize.xy)) return; float4 inputTexture = _InputTexture.Load(float3(id.xy, 0)); float determin = 0; switch ((uint)_Channel) { case 0: determin = inputTexture.r; break; case 1: determin = inputTexture.g; break; case 2: determin = inputTexture.b; break; case 3: determin = inputTexture.a; break; default: break; } if (determin \u0026gt;= 0.5) { _OutputTexture[id.xy] = float4(-1, -1, samplePosition.x * _TextureSize.z, samplePosition.y * _TextureSize.w); } else { _OutputTexture[id.xy] = float4(samplePosition.x * _TextureSize.z, samplePosition.y * _TextureSize.w, -1, -1); } } #pragma kernel JFAMain float2 _Step; static int2 directions[] = { int2(-1, -1), int2(-1, 0), int2(-1, 1), int2(0, -1), int2(0, 1), int2(1, -1), int2(1, 0), int2(1, 1) }; float4 JFAOutside(float4 inputTex, float2 idxy) { float4 outputTex = inputTex; //cull inside if (inputTex.x != -1) { float2 nearestUV = inputTex.zw; float minDistance = 1e16; //if had min distance in previous flooding if (inputTex.z != -1) { minDistance = length(idxy + PIXEL_OFFSET - nearestUV * _TextureSize.xy); } bool hasMin = false; for (uint i = 0; i \u0026lt; 8; i++) { uint2 sampleOffset = idxy + directions[i] * _Step; sampleOffset = clamp(sampleOffset, 0, _TextureSize.xy - 1); float4 offsetTexture = _InputTexture.Load(float3(sampleOffset, 0)); //if had min distance in previous flooding if (offsetTexture.z != -1) { float2 tempUV = offsetTexture.zw; float tempDistance = length(idxy + PIXEL_OFFSET - tempUV * _TextureSize.xy); if (tempDistance \u0026lt; minDistance) { hasMin = true; minDistance = tempDistance; nearestUV = tempUV; } } } if (hasMin) { outputTex = float4(inputTex.xy, nearestUV); } } return outputTex; } float4 JFAInside(float4 inputTex, float2 idxy) { float4 outputTex = inputTex; //cull outside if (inputTex.z != -1) { float2 nearestUV = inputTex.xy; float minDistance = 1e16; //if had min distance in previous flooding if (inputTex.x != -1) { minDistance = length(idxy + PIXEL_OFFSET - nearestUV * _TextureSize.xy); } bool hasMin = false; for (uint i = 0; i \u0026lt; 8; i++) { uint2 sampleOffset = idxy + directions[i] * _Step; sampleOffset = clamp(sampleOffset, 0, _TextureSize.xy - 1); float4 offsetTexture = _InputTexture.Load(float3(sampleOffset, 0)); //if had min distance in previous flooding if (offsetTexture.x != -1) { float2 tempUV = offsetTexture.xy; float tempDistance = length(idxy + PIXEL_OFFSET - tempUV * _TextureSize.xy); if (tempDistance \u0026lt; minDistance) { hasMin = true; minDistance = tempDistance; nearestUV = tempUV; } } } if (hasMin) { outputTex = float4(nearestUV, inputTex.zw); } } return outputTex; } [numthreads(8,8,1)] void JFAMain(uint3 id : SV_DispatchThreadID) { float2 samplePosition = id.xy + PIXEL_OFFSET; if (any(samplePosition \u0026gt;= _TextureSize.xy)) return; float4 inputTexture = _InputTexture.Load(float3(id.xy, 0)); float4 outSide = JFAOutside(inputTexture, id.xy); _OutputTexture[id.xy] = JFAInside(outSide, id.xy); } #pragma kernel ComposeMain Texture2D\u0026lt;float4\u0026gt; _OriginalTexture; #if UNITY_2020_2_OR_NEWER #pragma multi_compile _USE_GRAYSCALE #else #define _USE_GRAYSCALE 0 #endif [numthreads(8, 8, 1)] void ComposeMain(uint3 id : SV_DispatchThreadID) { uint2 reverseY = id.xy; reverseY = uint2(id.x, _TextureSize.y - 1 - id.y); float4 inputTexture = _InputTexture.Load(float3(reverseY, 0)); float4 originalTexture = _OriginalTexture.Load(float3(reverseY, 0)); float determin = 0; switch ((uint)_Channel) { case 0: determin = originalTexture.r; break; case 1: determin = originalTexture.g; break; case 2: determin = originalTexture.b; break; case 3: determin = originalTexture.a; break; default: break; } #if _USE_GRAYSCALE float distance = 0; if (determin \u0026gt;= 0.5) { distance = -length(reverseY + PIXEL_OFFSET - inputTexture.xy * _TextureSize.xy); _OutputTexture[id.xy] = float4(distance, distance, distance, 1); } else { distance = length(reverseY + PIXEL_OFFSET - inputTexture.zw * _TextureSize.xy); _OutputTexture[id.xy] = float4(distance, distance, distance, 0); } #else if (determin \u0026gt;= 0.5) { _OutputTexture[id.xy] = float4(inputTexture.xy, 0, 1); } else { _OutputTexture[id.xy] = float4(inputTexture.zw, 0, 0); } #endif } JumpFlooding.cs C#脚本没什么特殊的地方了，只要准备好Render Texture和各类参数，传递给Compute Shader就好了。为了方便可视化，这里使用了MonoBehaviour的Coroutine，需要点击play之后在点击脚本里的\u0026quot;Calculate SDF\u0026quot;。这里保存的是记录了对应的像素点的UV的贴图，这样可以用jpg或者png来保存，如果要保存记录了有向距离的灰度贴图，就需要保存为float类型的exr格式了。\nusing System.Collections; using UnityEngine; using UnityEditor; public class JumpFlooding : MonoBehaviour { enum Channels { R, G, B, A } public Texture inputTexture; public ComputeShader computeShader; public float updateTime; public MeshRenderer meshRenderer; public bool useGrayScale = false; private RenderTexture[] renderTextures; private static void EnsureArray\u0026lt;T\u0026gt;(ref T[] array, int size, T initialValue = default(T)) { if (array == null || array.Length != size) { array = new T[size]; for (int i = 0; i != size; i++) array[i] = initialValue; } } private static void EnsureRenderTexture(ref RenderTexture rt, RenderTextureDescriptor descriptor, string RTName) { if (rt != null \u0026amp;\u0026amp; (rt.width != descriptor.width || rt.height != descriptor.height)) { RenderTexture.ReleaseTemporary(rt); rt = null; } if (rt == null) { RenderTextureDescriptor desc = descriptor; desc.depthBufferBits = 0; desc.msaaSamples = 1; rt = RenderTexture.GetTemporary(desc); rt.name = RTName; if (!rt.IsCreated()) rt.Create(); } } public static void EnsureRT(ref RenderTexture[] rts, RenderTextureDescriptor descriptor) { EnsureArray(ref rts, 2); EnsureRenderTexture(ref rts[0], descriptor, \u0026#34;Froxel Tex One\u0026#34;); EnsureRenderTexture(ref rts[1], descriptor, \u0026#34;Froxel Tex Two\u0026#34;); } public void Calculate() { StartCoroutine(CalculateCoroutine()); } public void CopyUV(Texture texture, RenderTexture renderTexture, uint channel) { int kernel = computeShader.FindKernel(\u0026#34;CopyUVMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(kernel, out uint x, out uint y, out uint z); Vector3Int dispatchCounts = new Vector3Int(Mathf.CeilToInt((float)inputTexture.width / x), Mathf.CeilToInt((float)inputTexture.height / y), 1); computeShader.SetTexture(kernel, \u0026#34;_InputTexture\u0026#34;, texture); computeShader.SetTexture(kernel, \u0026#34;_OutputTexture\u0026#34;, renderTexture); computeShader.SetVector(\u0026#34;_TextureSize\u0026#34;, new Vector4(texture.width, texture.height, 1.0f / texture.width, 1.0f / texture.height)); computeShader.SetFloat(\u0026#34;_Channel\u0026#34;, (float)channel); computeShader.Dispatch(kernel, dispatchCounts.x, dispatchCounts.y, dispatchCounts.z); } public void JFA(RenderTexture one, RenderTexture two, Vector2Int step, bool reverse) { int kernel = computeShader.FindKernel(\u0026#34;JFAMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(kernel, out uint x, out uint y, out uint z); Vector3Int dispatchCounts = new Vector3Int(Mathf.CeilToInt((float)inputTexture.width / x), Mathf.CeilToInt((float)inputTexture.height / y), 1); if (reverse) { computeShader.SetTexture(kernel, \u0026#34;_InputTexture\u0026#34;, two); computeShader.SetTexture(kernel, \u0026#34;_OutputTexture\u0026#34;, one); } else { computeShader.SetTexture(kernel, \u0026#34;_InputTexture\u0026#34;, one); computeShader.SetTexture(kernel, \u0026#34;_OutputTexture\u0026#34;, two); } computeShader.SetVector(\u0026#34;_TextureSize\u0026#34;, new Vector4(inputTexture.width, inputTexture.height, 1.0f / inputTexture.width, 1.0f / inputTexture.height)); computeShader.SetVector(\u0026#34;_Step\u0026#34;, (Vector2)step); computeShader.Dispatch(kernel, dispatchCounts.x, dispatchCounts.y, dispatchCounts.z); } public void Compose(RenderTexture one, RenderTexture two, Texture texture, uint channel, bool reverse) { int kernel = computeShader.FindKernel(\u0026#34;ComposeMain\u0026#34;); computeShader.GetKernelThreadGroupSizes(kernel, out uint x, out uint y, out uint z); Vector3Int dispatchCounts = new Vector3Int(Mathf.CeilToInt((float)inputTexture.width / x), Mathf.CeilToInt((float)inputTexture.height / y), 1); if (reverse) { computeShader.SetTexture(kernel, \u0026#34;_InputTexture\u0026#34;, two); computeShader.SetTexture(kernel, \u0026#34;_OutputTexture\u0026#34;, one); } else { computeShader.SetTexture(kernel, \u0026#34;_InputTexture\u0026#34;, one); computeShader.SetTexture(kernel, \u0026#34;_OutputTexture\u0026#34;, two); } computeShader.SetTexture(kernel, \u0026#34;_OriginalTexture\u0026#34;, texture); computeShader.SetFloat(\u0026#34;_Channel\u0026#34;, (float)channel); #if UNITY_2020_2_OR_NEWER if(useGrayScale) { computeShader.EnableKeyword(\u0026#34;_USE_GRAYSCALE\u0026#34;); } else { computeShader.DisableKeyword(\u0026#34;_USE_GRAYSCALE\u0026#34;); } #endif computeShader.Dispatch(kernel, dispatchCounts.x, dispatchCounts.y, dispatchCounts.z); } public void Visualize(RenderTexture one, RenderTexture two, bool reverse) { MaterialPropertyBlock mpb = new MaterialPropertyBlock(); mpb.SetTexture(\u0026#34;_MainTex\u0026#34;, reverse ? two : one); meshRenderer.SetPropertyBlock(mpb); } static public void SaveToTexture(string name, RenderTexture renderTexture, bool alphaIsTransparency) { RenderTexture currentRT = RenderTexture.active; RenderTexture.active = renderTexture; Texture2D texture2D = new Texture2D(renderTexture.width, renderTexture.height, TextureFormat.RGBAFloat, false); texture2D.ReadPixels(new Rect(0, 0, renderTexture.width, renderTexture.height), 0, 0); RenderTexture.active = currentRT; System.IO.Directory.CreateDirectory(\u0026#34;Assets/JumpFlooding/\u0026#34;); byte[] bytes = texture2D.EncodeToEXR(); string path = \u0026#34;Assets/JumpFlooding/\u0026#34; + name + \u0026#34;.exr\u0026#34;; System.IO.File.WriteAllBytes(path, bytes); TextureImporter importer = (TextureImporter)AssetImporter.GetAtPath(path); if (importer != null) { importer.alphaIsTransparency = alphaIsTransparency; importer.sRGBTexture = false; importer.mipmapEnabled = false; AssetDatabase.ImportAsset(path); } Debug.Log(\u0026#34;Saved to \u0026#34; + path); AssetDatabase.Refresh(); } IEnumerator CalculateCoroutine() { RenderTextureDescriptor desc = new RenderTextureDescriptor { width = inputTexture.width, height = inputTexture.height, volumeDepth = 1, msaaSamples = 1, graphicsFormat = UnityEngine.Experimental.Rendering.GraphicsFormat.R16G16B16A16_SFloat, enableRandomWrite = true, dimension = UnityEngine.Rendering.TextureDimension.Tex2D, sRGB = false }; EnsureRT(ref renderTextures, desc); RenderTexture rtOne = renderTextures[0]; RenderTexture rtTwo = renderTextures[1]; CopyUV(inputTexture, rtOne, (uint)Channels.A); yield return new WaitForSeconds(updateTime); Shader.DisableKeyword(\u0026#34;RENDERTEXTURE_UPSIDE_DOWN\u0026#34;); Vector2Int step = new Vector2Int((inputTexture.width + 1) \u0026gt;\u0026gt; 1, (inputTexture.height + 1) \u0026gt;\u0026gt; 1); bool reverse = false; do { Debug.Log(step); JFA(rtOne, rtTwo, step, reverse); reverse = !reverse; Visualize(rtOne, rtTwo, reverse); step = new Vector2Int((step.x + 1) \u0026gt;\u0026gt; 1, (step.y + 1) \u0026gt;\u0026gt; 1); yield return new WaitForSeconds(updateTime); } while (step.x \u0026gt; 1 || step.y \u0026gt; 1); Debug.Log(new Vector2Int(1, 1)); JFA(rtOne, rtTwo, new Vector2Int(1, 1), reverse); reverse = !reverse; Visualize(rtOne, rtTwo, reverse); yield return new WaitForSeconds(updateTime); Debug.Log(new Vector2Int(1, 1)); JFA(rtOne, rtTwo, new Vector2Int(1, 1), reverse); reverse = !reverse; Visualize(rtOne, rtTwo, reverse); yield return new WaitForSeconds(updateTime); Compose(rtOne, rtTwo, inputTexture, (uint)Channels.A, reverse); reverse = !reverse; Shader.EnableKeyword(\u0026#34;RENDERTEXTURE_UPSIDE_DOWN\u0026#34;); Visualize(rtOne, rtTwo, reverse); SaveToTexture(\u0026#34;WhatIsThis\u0026#34;, reverse ? rtTwo : rtOne, false); } } [CustomEditor(typeof(JumpFlooding))] public class JumpFloodingEditor : Editor { private JumpFlooding jumpFlooding; private void OnEnable() { jumpFlooding = (JumpFlooding)target; } public override void OnInspectorGUI() { base.OnInspectorGUI(); using (new EditorGUI.DisabledGroupScope(!Application.isPlaying)) { if (GUILayout.Button(\u0026#34;Calculate SDF\u0026#34;, GUILayout.Height(30))) { jumpFlooding.Calculate(); } } } } JFAVisualize.shader 这个shader应该用在宽高比2:1的Mesh上，这样左半部分是记录了最近像素的uv的贴图，有伴部分是记录了带方向的距离的灰度贴图或者其描边的可视化。需要注意采样的方式和使用像素点中心来计算距离。\nShader \u0026#34;Unlit/JFAVisualize\u0026#34; { Properties { _MainTex (\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Distance (\u0026#34;Distance\u0026#34;, float) = 10 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #pragma multi_compile _ RENDERTEXTURE_UPSIDE_DOWN Texture2D _MainTex; float4 _MainTex_TexelSize; SamplerState sampler_PointClamp; float _Distance; struct Attributes { float4 positionOS : POSITION; float2 texcoord : TEXCOORD0; }; struct Varyings { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; }; Varyings Vert(Attributes input) { Varyings output = (Varyings)0; VertexPositionInputs vertexInput = GetVertexPositionInputs(input.positionOS.xyz); output.positionCS = vertexInput.positionCS; output.uv = input.texcoord; return output; } float4 Frag(Varyings input) : SV_TARGET { float2 upsideDown = input.uv; #if RENDERTEXTURE_UPSIDE_DOWN upsideDown = float2(input.uv.x, 1 - input.uv.y); #endif float2 uvOne = float2(upsideDown.x * 2, upsideDown.y); float2 uvTwo = float2(upsideDown.x * 2 - 1, upsideDown.y); float4 returnColor = 0; if (input.uv.x \u0026gt;= 0.5) { float4 texColor = _MainTex.SampleLevel(sampler_PointClamp, uvTwo, 0); #if RENDERTEXTURE_UPSIDE_DOWN float2 realTexcoord = float2(input.uv.x * 2 - 1, input.uv.y); realTexcoord = floor(realTexcoord* _MainTex_TexelSize.zw) + 0.5; float distance = length(texColor.xy * _MainTex_TexelSize.zw - realTexcoord); #else float2 realTexcoord = uvTwo; realTexcoord = floor(realTexcoord * _MainTex_TexelSize.zw) + 0.5; float distance = length(texColor.xy * _MainTex_TexelSize.zw - realTexcoord); #endif float grayScale = smoothstep(_Distance, 0, distance); returnColor = float4(grayScale, grayScale, grayScale, 1); } else { returnColor = _MainTex.SampleLevel(sampler_PointClamp, uvOne, 0); } return returnColor; } ENDHLSL SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } LOD 100 Pass { Name \u0026#34;JFA Visualize Pass\u0026#34; Tags{\u0026#34;LightMode\u0026#34; = \u0026#34;UniversalForward\u0026#34;} Cull Back ZTest LEqual ZWrite On HLSLPROGRAM #pragma vertex Vert #pragma fragment Frag ENDHLSL } } } ","permalink":"https://zznewclear13.github.io/posts/calculate-signed-distance-field-using-compute-shader/","summary":"什么是有向距离场以及它能用来干什么 有向距离场记录的是从一个点到集合边界的的距离值，其值的正负对应该点在集合外部或内部。有向距离场有很广的应用范围可以用来简单的生成Voronoi图形，可以用来做全局光照的计算，可以用来做两个形状的平滑的变形，可以用来做高清晰度的字体，也可以用来做Ray March（虽然我认为不如直接光线追踪求交来的效率高）。像原神就使用了SDF的方法，生成了角色脸部的阴影图，从而让角色脸部的阴影能自然的变化。\n那么什么又是Jump Flooding Algorithm呢 Jump Flooding Algorithm是荣国栋在他的博士论文Jump Flooding Algorithm On Graphics Hardware And Its Applications提出的一种在GPU上运行的能够快速传播某个像素的信息到其他像素的算法。\n普通的Flooding算法在一次运行中，固定向相邻的一个像素的像素传播信息，而Jump Flooding则是按照2的幂次递增或是递减来传播信息。这和之前提到的并行计算——Reduction的想法差不多。下图演示了普通的Flooding和Jump Flooding的过程： 使用JFA计算一张2D图片对应的SDF贴图 首先在Unity中创建JumpFlooding.cs, JFAComputeShader.compute, 和JFAVisualize.shader，分别用来执行Compute Shader，使用JFA算法计算SDF和可视化JFA算法的结果。\n这里使用一张RGB通道为灰色，Alpha通道写着“JFA”的贴图作为我们2D图片的输入。 整体思路和需要注意的事项 先从简单实现功能上来考虑，暂时忽略掉抗锯齿的需求，直接对Alpha通道的值按照0.5来划分出图形的内部和外部，大于等于0.5为外部，小于0.5为内部。 SDF需要要计算距离，这里使用像素点中心到另一个像素点中心的距离（也可以使用像素点左下角到另一个像素点左下角的距离，不过为了明确起见，还是加上这半个像素的偏移比较好）。距离可以用uv的大小来表示，也可以用像素数量来表示，针对图像长宽不同的情况，这里以一个像素宽度为1来表示两个像素点的距离。在最后采样JFA的Render Texture的时候，也要注意使用sampler_PointClamp来进行采样，计算距离时也不能仅仅使用uv来计算，而是要使用像素中间的点的位置来进行计算。 普通的JFA算法会使用到Render Texture的两个通道，来标记像素对应的最近边界像素的UV，由于记录的是UV的数值而不是颜色信息，Render Texture要储存在线性空间中。由于要同时计算内部和外部的点到边界的有向距离，JFA算法会使用到Render Texture全部的四个通道，这里使用前两位记录位于内部的像素对应的边界像素的坐标，用后两位记录位于外部的像素对应的边界像素的坐标，即对于内部的像素(nearestUV.x, nearestUV.y, Z, W)，对于外部的像素(X, Y, nearestUV.x, nearestUV.y)，XYZW则可以用来表示该点为最初始的内外部的点、包含JFA传递的信息的点。不包含JFA传递的信息的点。 在JFA计算之前，需要先让贴图对应的点包含JFA信息，也就是说，对于内部的点初始化为(UV.x, UV.y, -1, -1)，对于外部的点初始化为(-1, -1, UV.x, UV.y)。这要求我们使用的Render Texture格式为至少R16G16B16A16 SFloat。 在JFA的计算中，以外部的像素点为例，所进行的操作是：采样上次经过JFA操作的Render Texture；根据XY通道判断该点在边界的内部还是外部，内部就跳过；对外部的像素点，通过XY通道判断是否已包含JFA的信息，如果包含，根据ZW通道计算出当前点到其包含的最近像素的距离，如果不包含，将这个距离设置为一个极大的常数；分别采样距离像素点2的幂次的距离的八个像素，判断这些像素是否已包含JFA的信息，如果不包含，采样下一个点，如果包含，根据这些像素的ZW通道计算出当前点到这些像素包含的最近像素的距离，并和上一步算出的距离进行比较，如果小于上一步算出的距离，则证明该像素对应的最近像素为周围点包含的像素，更新该像素的ZW通道，并且将XY通道标记成已包含JFA的信息。 采样周边像素的步长从2D贴图的长宽的一半向上取整开始，每次JFA都取上一步步长的一半向上取整作为新的步长，一直进行到步长为(1, 1)，进行最后一次JFA计算。 经过JFA计算之后，还需要将分别表示内部的点对应的最近像素的UV和外部的点对应的最近像素的UV结合起来，储存为一张贴图，可以是(nearestUV.x, nearestUV.y, 0, inside?1:0)，也可以是(distance * inside?-1:1, 1)。 应该是跟当前平台有关，有时候会出现贴图上下颠倒的情况，可以用UNITY_UV_STARTS_AT_TOP来协助解决，不过compute shader可能需要自己启用这个宏，这里就直接硬写在shader里，不做平台判断了。 JFAComputeShader.compute 根据上面的整体思路，我们需要三个kernel，一个用来初始化，一个做JFA计算，最后一个用来合成最后的贴图。Compute Shader的关键字需要Unity 2020以上（我也不知道具体哪个版本）才能有，这里就暂时用UNITY_2020_2_OR_NEWER这个宏来屏蔽了。\n#pragma kernel CopyUVMain #define PIXEL_OFFSET 0.","title":"使用Compute Shader计算有向距离场"},{"content":"为什么要用球谐函数来计算全局光照 在物体的渲染中，除了计算直接光照的BRDF之外，也要计算间接光照对物体的影响。在引擎中获取间接光照信息的方法通常是在场景中布置一个反射探针，离线渲染一个360度的场景贴图。这样在计算间接光照的高光部分的时候，可以使用视线在物体上的反射方向reflect(-viewDirection, normal)，对渲染好的贴图进行采样，再进行brdf的计算，因此这张贴图也会被称作是specular map；然而在计算间接光照的漫反射部分时，因为目标点会受到来自各个方向上的光线带来的漫反射，不能再简单的使用视线的反射来采样这张帖图。这时有两种解决办法，一种是采样这张贴图的mipmap，在一定程度上模糊的mipmap可以认为是综合了各个方向的光照的信息，另一种则是Ravi Ramamoorthi和Pat Hanrahan2001年在An Efficient Representation for Irradiance Environment Maps中提出的，通过球谐函数重新构建低频光照信息的方式，将其作为简介光漫反射部分的贴图。\n如何使用球谐函数重新构建光照信息 在Ravi Ramamoorthi的论文中他给出了球谐参数的计算公式和重构光照信息的公式： $$ \\tag*{球谐参数} L_{lm} = \\int_{\\theta = 0}^\\pi\\int_{\\theta = 0}^{2\\pi}L(\\theta)Y_{lm}(\\theta, \\phi)sin\\theta d\\theta d\\phi $$ $$ \\begin{align*}其中(x, y, z) \u0026amp;= (sin\\theta cos\\phi, sin\\theta sin\\phi, cos\\theta) \\cr Y_{00}(\\theta, \\phi) \u0026amp;= 0.282095 \\cr (Y_{11};Y_{10};Y_{1-1})(\\theta, \\phi) \u0026amp;= 0.488603(x;z;y) \\cr (Y_{21};Y_{2-1};Y_{2-2})(\\theta, \\phi) \u0026amp;= 1.092548(xz;yz;xy) \\cr Y_{20}(\\theta, \\phi) \u0026amp;= 0.315392(3z^2 - 1) \\cr Y_{22}(\\theta, \\phi) \u0026amp;= 0.546274(x^2 - y^2) \\end{align*} $$ $$ \\tag*{重构光照} \\begin{equation}\\begin{split} E(n) =\u0026amp;\\ c_1L_{22}(x^2 - y^2) + c_3L_{20}z^2 + c_4L_{00} - c_5L_{20} \\cr +\u0026amp;\\ 2c_1(L_{2-2}xy + L_{21}xz + L_{2-1}yz) \\cr +\u0026amp;\\ 2c_2(L_{11}x + L_{1-1}y + L_{10}z)\\end{split}\\end{equation} $$ $$ 其中c1 = 0.429043, c2 = 0.511664, c3 = 0.743125, c4 = 0.886227, c5 = 0.247708 $$\n根据第一条式子，有两种采样的方法：一种是使用蒙特卡洛方法，以\\(Y_{lm}(\\theta, \\phi)sin\\theta\\)作为权重，随机取球面上均匀分布的方向采样高光贴图；另一种是以\\(Y_{lm}(\\theta, \\phi)sin\\theta \\)乘当前像素对应的立体角作为权重，采样高光贴图的所有像素。在这篇文章中我参考了Accardi Piero的Unity_SphericalHarmonics_Tools中的用Computes Shader采样所有像素的方法来计算球谐参数的方法，出于一些不知名的原因，计算权重的时候去掉\\(sin\\theta\\)这一项反而能得到正确的结果，这里还希望有人能够指正一下。\n使用球谐函数计算全局光照的漫反射部分 首先在Unity中创建CalculateSphericalHarmonics.cs, SphericalHarmonicsComputeShader.compute, 和SphericalHarmonics.shader，分别用来执行Compute Shader，计算球谐参数和使用球谐参数重构光照。 我们这里使用grace作为我们的环境贴图，大概长这个样子。\nSphericalHarmonicsComputeShader.compute 在Compute Shader中我们需要做以下的操作：获取像素的位置；根据像素的位置，获取该像素的立体角和在球面上的方向，并采样球面获得像素的颜色；对每个像素计算出其方向对应的球谐基\\(Y_{lm}\\)；累积所有像素的立体角、颜色和球谐基的乘积，得到\\(4\\pi\\)倍的球谐参数。\n首先是获取像素的位置，立体角，方向和采样（Compute Shader中加不了中文注释，就只能将就了）：\n#pragma kernel CalculateSHMain TextureCube _CubeMapTexture; SamplerState sampler_LinearClamp; //_TextureSize and _DispatchCount are float3 but used as uint3 float3 _TextureSize; float3 _DispatchCount; //Check if we are sampling inside texture bool CheckInRange(uint3 texturesize, uint3 dispatchThreadID) { return !any(dispatchThreadID \u0026gt;= texturesize); } //Calculate direction from dispatchThreadID float3 GetDirectionFromIndex(uint3 textureSize, uint3 dispatchThreadID) { float2 uv = (dispatchThreadID.xy + 0.5) * rcp(textureSize.xy); float u = uv.x; float v = uv.y; float3 dir = float3(0, 0, 0); switch (dispatchThreadID.z) { case 0: //+X dir.x = 1; dir.y = v * -2.0f + 1.0f; dir.z = u * -2.0f + 1.0f; break; case 1: //-X dir.x = -1; dir.y = v * -2.0f + 1.0f; dir.z = u * 2.0f - 1.0f; break; case 2: //+Y dir.x = u * 2.0f - 1.0f; dir.y = 1.0f; dir.z = v * 2.0f - 1.0f; break; case 3: //-Y dir.x = u * 2.0f - 1.0f; dir.y = -1.0f; dir.z = v * -2.0f + 1.0f; break; case 4: //+Z dir.x = u * 2.0f - 1.0f; dir.y = v * -2.0f + 1.0f; dir.z = 1; break; case 5: //-Z dir.x = u * -2.0f + 1.0f; dir.y = v * -2.0f + 1.0f; dir.z = -1; break; } return normalize(dir); } float AreaElement(float x, float y) { return atan2(x * y, sqrt(x * x + y * y + 1)); } //Calculate solid angle float GetWeightFromIndex(uint3 textureSize, uint3 dispatchThreadID) { float2 invTextureSize = rcp(textureSize.xy); float2 uv = (dispatchThreadID.xy + 0.5) * invTextureSize; uv = uv * 2 - 1; float x0 = uv.x - invTextureSize.x; float y0 = uv.y - invTextureSize.y; float x1 = uv.x + invTextureSize.x; float y1 = uv.y + invTextureSize.y; return AreaElement(x0, y0) - AreaElement(x0, y1) - AreaElement(x1, y0) + AreaElement(x1, y1); } //Texture format is RGBA SFloat, so we do not need to decode sampleHDR float3 SampleCubeMap(TextureCube cubTex, float3 sampleDir) { float4 sampleHDR = cubTex.SampleLevel(sampler_LinearClamp, sampleDir, 0); return sampleHDR.rgb; } [numthreads(8, 8, 6)] void CalculateSHMain(uint3 groupID : SV_GroupID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { uint indexGroup = groupID.z * _DispatchCount.x * _DispatchCount.y + groupID.y * _DispatchCount.x + groupID.x; bool inRange = CheckInRange(_TextureSize, dispatchThreadID); if (inRange) { float3 direction = GetDirectionFromIndex(_TextureSize, dispatchThreadID); float weight = GetWeightFromIndex(_TextureSize, dispatchThreadID); //I leave out sin(theta) term here float3 sampleColor = SampleCubeMap(_CubeMapTexture, direction).rgb * weight;// * sqrt(1 - direction.z * direction.z); } } 按照之前的公式计算球谐基：\nstruct SHBasis { float y00; float y1p1; float y10; float y1n1; float y2p1; float y2n1; float y2n2; float y20; float y2p2; }; SHBasis Evaluate(float3 normal) { SHBasis shBasis = (SHBasis)0; shBasis.y00 = 0.2820947917f; shBasis.y1p1 = 0.4886025119f * normal.y; shBasis.y10 = 0.4886025119f * normal.z; shBasis.y1n1 = 0.4886025119f * normal.x; shBasis.y2p1 = 1.0925484306f * normal.x * normal.y; shBasis.y2n1 = 1.0925484306f * normal.y * normal.z; shBasis.y2n2= 0.3153915652f * (3 * normal.z * normal.z - 1.0f); shBasis.y20 = 1.0925484306f * normal.x * normal.z; shBasis.y2p2 = 0.5462742153f * (normal.x * normal.x - normal.y * normal.y); return shBasis; } 由于Compute Shader的groupshared memory有大小限制，这里我们传入一个_SHPackIndex，分三次计算球谐参数，每次计算三个float3：\n//Pack 3 coeffs in one struct struct CoeffsPack3 { float3 coeff0; float3 coeff1; float3 coeff2; }; float _SHPackIndex; groupshared CoeffsPack3 coeffs[8 * 8 * 6]; [numthreads(8, 8, 6)] void CalculateSHMain(uint3 groupID : SV_GroupID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { uint indexGroup = groupID.z * _DispatchCount.x * _DispatchCount.y + groupID.y * _DispatchCount.x + groupID.x; bool inRange = CheckInRange(_TextureSize, dispatchThreadID); if (inRange) { float3 direction = GetDirectionFromIndex(_TextureSize, dispatchThreadID); float weight = GetWeightFromIndex(_TextureSize, dispatchThreadID); //I leave out sin(theta) term here float3 sampleColor = SampleCubeMap(_CubeMapTexture, direction).rgb * weight;// * sqrt(1 - direction.z * direction.z); SHBasis shBasis = Evaluate(direction); CoeffsPack3 tempCoeffs = (CoeffsPack3)0; switch ((uint)_SHPackIndex) { case 0: tempCoeffs.coeff0 = shBasis.y00 * sampleColor; tempCoeffs.coeff1 = shBasis.y1p1 * sampleColor; tempCoeffs.coeff2 = shBasis.y10 * sampleColor; break; case 1: tempCoeffs.coeff0 = shBasis.y1n1 * sampleColor; tempCoeffs.coeff1 = shBasis.y2p1 * sampleColor; tempCoeffs.coeff2 = shBasis.y2n1 * sampleColor; break; case 2: tempCoeffs.coeff0 = shBasis.y2n2 * sampleColor; tempCoeffs.coeff1 = shBasis.y20 * sampleColor; tempCoeffs.coeff2 = shBasis.y2p2 * sampleColor; break; default: break; } coeffs[groupIndex] = tempCoeffs; } } 最后是对groupshared CoeffsPack3 coeffs[8 * 8 * 6]求和，在GPU Gems 3中介绍了一种多线程的求和算法Parallel Prefix Sum (Scan) with CUDA，这样就能使用GPU来对每一个Thread Group求和并输出到RWStructuredBuffer中了，最后再在CPU中求所有Thread Group的和：\nRWStructuredBuffer\u0026lt;CoeffsPack3\u0026gt; _GroupCoefficients; CoeffsPack3 CoeffsAdd(CoeffsPack3 coeffsA, CoeffsPack3 coeffsB) { CoeffsPack3 tempCoeffs = (CoeffsPack3)0; tempCoeffs.coeff0 = coeffsA.coeff0 + coeffsB.coeff0; tempCoeffs.coeff1 = coeffsA.coeff1 + coeffsB.coeff1; tempCoeffs.coeff2 = coeffsA.coeff2 + coeffsB.coeff2; return tempCoeffs; } void SumUp(uint groupIndex, uint indexGroup) { GroupMemoryBarrierWithGroupSync(); uint faceIndex = groupIndex % 64; if (faceIndex \u0026lt; 32) coeffs[groupIndex] = CoeffsAdd(coeffs[groupIndex], coeffs[groupIndex + 32]); GroupMemoryBarrierWithGroupSync(); if (faceIndex \u0026lt; 16) coeffs[groupIndex] = CoeffsAdd(coeffs[groupIndex], coeffs[groupIndex + 16]); GroupMemoryBarrierWithGroupSync(); if (faceIndex \u0026lt; 8) coeffs[groupIndex] = CoeffsAdd(coeffs[groupIndex], coeffs[groupIndex + 8]); GroupMemoryBarrierWithGroupSync(); if (faceIndex \u0026lt; 4) coeffs[groupIndex] = CoeffsAdd(coeffs[groupIndex], coeffs[groupIndex + 4]); GroupMemoryBarrierWithGroupSync(); if (faceIndex \u0026lt; 2) coeffs[groupIndex] = CoeffsAdd(coeffs[groupIndex], coeffs[groupIndex + 2]); GroupMemoryBarrierWithGroupSync(); if (faceIndex \u0026lt; 1) coeffs[groupIndex] = CoeffsAdd(coeffs[groupIndex], coeffs[groupIndex + 1]); GroupMemoryBarrierWithGroupSync(); if (groupIndex == 0) { CoeffsPack3 output = coeffs[0]; output = CoeffsAdd(output, coeffs[64]); output = CoeffsAdd(output, coeffs[128]); output = CoeffsAdd(output, coeffs[192]); output = CoeffsAdd(output, coeffs[256]); output = CoeffsAdd(output, coeffs[320]); _GroupCoefficients[indexGroup] = output; } } [numthreads(8, 8, 6)] void CalculateSHMain(uint3 groupID : SV_GroupID, uint groupIndex : SV_GroupIndex, uint3 dispatchThreadID : SV_DispatchThreadID) { uint indexGroup = groupID.z * _DispatchCount.x * _DispatchCount.y + groupID.y * _DispatchCount.x + groupID.x; bool inRange = CheckInRange(_TextureSize, dispatchThreadID); if (inRange) { float3 direction = GetDirectionFromIndex(_TextureSize, dispatchThreadID); float weight = GetWeightFromIndex(_TextureSize, dispatchThreadID); //I leave out sin(theta) term here float3 sampleColor = SampleCubeMap(_CubeMapTexture, direction).rgb * weight;// * sqrt(1 - direction.z * direction.z); SHBasis shBasis = Evaluate(direction); CoeffsPack3 tempCoeffs = (CoeffsPack3)0; switch ((uint)_SHPackIndex) { case 0: tempCoeffs.coeff0 = shBasis.y00 * sampleColor; tempCoeffs.coeff1 = shBasis.y1p1 * sampleColor; tempCoeffs.coeff2 = shBasis.y10 * sampleColor; break; case 1: tempCoeffs.coeff0 = shBasis.y1n1 * sampleColor; tempCoeffs.coeff1 = shBasis.y2p1 * sampleColor; tempCoeffs.coeff2 = shBasis.y2n1 * sampleColor; break; case 2: tempCoeffs.coeff0 = shBasis.y2n2 * sampleColor; tempCoeffs.coeff1 = shBasis.y20 * sampleColor; tempCoeffs.coeff2 = shBasis.y2p2 * sampleColor; break; default: break; } coeffs[groupIndex] = tempCoeffs; } SumUp(groupIndex, indexGroup); } 这样我们就写好了Compute Shader中所有的内容了，现在只需要把所有需要的参数传入Compute Shader就可以了。\nCalculateHarmonics.cs C#脚本中就没什么特别的操作了，创建Compute Buffer，传递到Compute Shader中，执行Compute Shader就能得到包含了每一个Thread Group的球谐参数的和的一个长度为(8 * 8 * 6)，每个参数包含3个float3的数组，对这个数组求和之后再除以\\(4\\pi\\)就能得到最终的球谐参数的三项了。执行三个循环，就能获得全部的球谐参数，最后再传递给相应的shader:\nusing System.Collections; using System.Collections.Generic; using System; using UnityEngine; using UnityEditor; using Unity.Mathematics; public class CalculateSphericalHarmonics : EditorWindow, IDisposable { struct CoeffsPack3 { public float3 coeff0; public float3 coeff1; public float3 coeff2; public CoeffsPack3(float x) { coeff0 = new float3(x); coeff1 = new float3(x); coeff2 = new float3(x); } } private CoeffsPack3 SumCoeffs(CoeffsPack3[] coeffsArray) { CoeffsPack3 tempCoeffs = new CoeffsPack3(0); for (int i = 0; i \u0026lt; coeffsArray.Length; i++) { tempCoeffs.coeff0 += coeffsArray[i].coeff0; tempCoeffs.coeff1 += coeffsArray[i].coeff1; tempCoeffs.coeff2 += coeffsArray[i].coeff2; } return tempCoeffs; } private Cubemap cubeMap; private ComputeShader sphericalHarmonicsComputeShader; private ComputeBuffer pack3Buffer; private ComputeBuffer coefficiencesBuffer; private CoeffsPack3[] pack3Array; public class Coefficience : ScriptableObject { public float3[] coefficiencesArray; } private Coefficience coefficience; private MeshRenderer targetMesh; private void OnEnable() { coefficience = CreateInstance\u0026lt;Coefficience\u0026gt;(); } [MenuItem(\u0026#34;zznewclear13/Calculate Spherical Harmonics\u0026#34;)] public static CalculateSphericalHarmonics GetWindow() { CalculateSphericalHarmonics window = GetWindow\u0026lt;CalculateSphericalHarmonics\u0026gt;(\u0026#34;Calculate Spherical Harmonics\u0026#34;); return window; } Rect baseRect { get { return new Rect(20f, 20f, position.width - 40f, position.height - 40f); } } private void OnGUI() { using (new GUILayout.AreaScope(baseRect)) { if (!EditorGUIUtility.wideMode) { EditorGUIUtility.wideMode = true; EditorGUIUtility.labelWidth = EditorGUIUtility.currentViewWidth - 212; } cubeMap = (Cubemap)EditorGUILayout.ObjectField(\u0026#34;Cube Map\u0026#34;, cubeMap, typeof(Cubemap), false); sphericalHarmonicsComputeShader = (ComputeShader)EditorGUILayout.ObjectField(\u0026#34;Compute Shader\u0026#34;, sphericalHarmonicsComputeShader, typeof(ComputeShader), false); targetMesh = (MeshRenderer)EditorGUILayout.ObjectField(\u0026#34;Target Mesh\u0026#34;, targetMesh, typeof(MeshRenderer), true); EditorGUILayout.Space(); using (new EditorGUI.DisabledGroupScope(!sphericalHarmonicsComputeShader)) { if (GUILayout.Button(\u0026#34;Calculate!\u0026#34;, GUILayout.Height(30f))) { if (cubeMap == null) { cubeMap = new Cubemap(256, TextureFormat.RGBAFloat, false); } Calculate(); SetBuffer(); } } EditorGUILayout.Space(); using (new EditorGUI.DisabledGroupScope(true)) { if(coefficience != null) { SerializedObject serializedObject = new SerializedObject(coefficience); SerializedProperty arrayProperty = serializedObject.FindProperty(\u0026#34;coefficiencesArray\u0026#34;); EditorGUILayout.PropertyField(arrayProperty, true); serializedObject.ApplyModifiedProperties(); } } } } private void Calculate() { coefficience.coefficiencesArray = new float3[9]; int kernel = sphericalHarmonicsComputeShader.FindKernel(\u0026#34;CalculateSHMain\u0026#34;); sphericalHarmonicsComputeShader.GetKernelThreadGroupSizes(kernel, out uint x, out uint y, out uint z); Vector3Int dispatchCounts = new Vector3Int(Mathf.CeilToInt((float)cubeMap.width / (float)x), Mathf.CeilToInt((float)cubeMap.height / (float)y), Mathf.CeilToInt((float)6 / (float)z)); if (pack3Buffer != null) { pack3Buffer.Release(); } pack3Buffer = new ComputeBuffer(dispatchCounts.x * dispatchCounts.y * dispatchCounts.z, 3 * 3 * 4, ComputeBufferType.Structured); pack3Array = new CoeffsPack3[dispatchCounts.x * dispatchCounts.y * dispatchCounts.z]; for (int i = 0; i \u0026lt; dispatchCounts.x * dispatchCounts.y * dispatchCounts.z; i++) { pack3Array[i] = new CoeffsPack3(0); } pack3Buffer.SetData(pack3Array); //用于确认传入的参数 Debug.Log(\u0026#34;_TextureSize: \u0026#34; + cubeMap.width + \u0026#34;, \u0026#34; + cubeMap.height + \u0026#34;, \u0026#34; + 6); Debug.Log(\u0026#34;_ThreadGroups: \u0026#34; + x + \u0026#34;, \u0026#34; + y + \u0026#34;, \u0026#34; + z); Debug.Log(\u0026#34;_DispatchCounts: \u0026#34; + dispatchCounts.x + \u0026#34;, \u0026#34; + dispatchCounts.y + \u0026#34;, \u0026#34; + dispatchCounts.z); Debug.Log(\u0026#34;_GroupCounts: \u0026#34; + dispatchCounts.x * dispatchCounts.y * dispatchCounts.z); Debug.Log(\u0026#34;_GroupThreadCounts: \u0026#34; + x * y * z); Debug.Log(\u0026#34;_CubeMapTexture: \u0026#34; + cubeMap.name); Debug.Log(\u0026#34;---------------------------------\u0026#34;); sphericalHarmonicsComputeShader.SetTexture(kernel, \u0026#34;_CubeMapTexture\u0026#34;, cubeMap); sphericalHarmonicsComputeShader.SetBuffer(kernel, \u0026#34;_GroupCoefficients\u0026#34;, pack3Buffer); sphericalHarmonicsComputeShader.SetVector(\u0026#34;_DispatchCount\u0026#34;, (Vector3)dispatchCounts); sphericalHarmonicsComputeShader.SetVector(\u0026#34;_TextureSize\u0026#34;, new Vector3(cubeMap.width, cubeMap.height, 6)); float inv4PI = 0.25f / Mathf.PI; //分三次，每次计算三个球谐参数 for (int shPackIndex = 0; shPackIndex \u0026lt; 3; shPackIndex++) { sphericalHarmonicsComputeShader.SetFloat(\u0026#34;_SHPackIndex\u0026#34;, shPackIndex); sphericalHarmonicsComputeShader.Dispatch(kernel, dispatchCounts.x, dispatchCounts.y, dispatchCounts.z); Debug.Log(\u0026#34;Dispatch with (\u0026#34; + dispatchCounts.x + \u0026#34;, \u0026#34; + dispatchCounts.y + \u0026#34;, \u0026#34; + dispatchCounts.z + \u0026#34;).\u0026#34;); pack3Buffer.GetData(pack3Array); CoeffsPack3 tempCoeffs = SumCoeffs(pack3Array); coefficience.coefficiencesArray[shPackIndex * 3 + 0] = tempCoeffs.coeff0 * inv4PI; coefficience.coefficiencesArray[shPackIndex * 3 + 1] = tempCoeffs.coeff1 * inv4PI; coefficience.coefficiencesArray[shPackIndex * 3 + 2] = tempCoeffs.coeff2 * inv4PI; } pack3Buffer.Release(); string debugLog = \u0026#34;\u0026#34;; for (int i = 0; i \u0026lt; 9; i++) { debugLog += coefficience.coefficiencesArray[i] + \u0026#34;...\u0026#34;; } } //给对应的mesh renderer的材质传入buffer private void SetBuffer() { if(coefficiencesBuffer != null) { coefficiencesBuffer.Release(); } coefficiencesBuffer = new ComputeBuffer(9, 3 * 4, ComputeBufferType.Structured); coefficiencesBuffer.SetData(coefficience.coefficiencesArray); targetMesh.sharedMaterial.SetBuffer(\u0026#34;_CoefficienceBuffer\u0026#34;, coefficiencesBuffer); } public void Dispose() { pack3Buffer.Release(); coefficiencesBuffer.Release(); } } SphericalHarmonics.shader 从传入的StructuredBuffer中获取球谐参数，计算辐照度，同时我还增加了一个选项，用于对比我计算出的球谐参数和实际的球谐参数：\nShader \u0026#34;Unlit/SphericalHarmonicsShader\u0026#34; { Properties { [Toggle(USE_GRACE)]_Use_Grace (\u0026#34;Use Grace\u0026#34;, float) = 1 } HLSLINCLUDE #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; #pragma multi_compile _ USE_GRACE #if USE_GRACE #define Coeffs grace #else #define Coeffs _CoefficienceBuffer #endif StructuredBuffer\u0026lt;float3\u0026gt; _CoefficienceBuffer; static float3 grace[] = { float3(0.7953949, 0.4405923, 0.5459412), float3(0.3981450, 0.3526911, 0.6097158), float3(-0.3424573, -0.1838151, -0.2715583), float3(-0.2944621, -0.0560606, 0.0095193), float3(-0.1123051, -0.0513088, -0.1232869), float3(-0.2645007, -0.2257996, -0.4785847), float3(-0.1569444, -0.0954703, -0.1485053), float3(0.5646247, 0.2161586, 0.1402643), float3(0.2137442, -0.0547578, -0.3061700) }; struct Attributes { float4 positionOS : POSITION; float2 uv : TEXCOORD0; float3 normalOS : NORMAL; }; struct Varyings { float4 positionCS : SV_POSITION; float2 texcoord : TEXCOORD0; float3 normalWS : TEXCOORD1; }; float3 CalcIrradiance(float3 nor) { float c1 = 0.429043; float c2 = 0.511664; float c3 = 0.743125; float c4 = 0.886227; float c5 = 0.247708; return ( c1 * Coeffs[8] * (nor.x * nor.x - nor.y * nor.y) + c3 * Coeffs[6] * nor.z * nor.z + c4 * Coeffs[0] - c5 * Coeffs[6] + 2.0 * c1 * Coeffs[4] * nor.x * nor.y + 2.0 * c1 * Coeffs[7] * nor.x * nor.z + 2.0 * c1 * Coeffs[5] * nor.y * nor.z + 2.0 * c2 * Coeffs[3] * nor.x + 2.0 * c2 * Coeffs[1] * nor.y + 2.0 * c2 * Coeffs[2] * nor.z ); } Varyings Vert(Attributes input) { Varyings output = (Varyings)0; VertexPositionInputs vertexInput = GetVertexPositionInputs(input.positionOS.xyz); VertexNormalInputs normalInput = GetVertexNormalInputs(input.normalOS); output.positionCS = vertexInput.positionCS; output.texcoord = input.uv; output.normalWS = normalInput.normalWS; return output; } float4 Frag(Varyings input) : SV_TARGET { float3 shColor = CalcIrradiance(normalize(input.normalWS)); return float4(shColor, 1); } ENDHLSL SubShader { Pass { Name \u0026#34;Spherical Harmonics Pass\u0026#34; Tags{\u0026#34;LightMode\u0026#34; = \u0026#34;UniversalForward\u0026#34;} Cull Back ZTest LEqual ZWrite On HLSLPROGRAM #pragma vertex Vert #pragma fragment Frag ENDHLSL } } } 最后点击“Calculate!”就能看到我们重新构建的全局光照的漫反射部分的光照信息了，大概就是这个样子。\n","permalink":"https://zznewclear13.github.io/posts/calculate-spherical-harmonics-using-compute-shader/","summary":"为什么要用球谐函数来计算全局光照 在物体的渲染中，除了计算直接光照的BRDF之外，也要计算间接光照对物体的影响。在引擎中获取间接光照信息的方法通常是在场景中布置一个反射探针，离线渲染一个360度的场景贴图。这样在计算间接光照的高光部分的时候，可以使用视线在物体上的反射方向reflect(-viewDirection, normal)，对渲染好的贴图进行采样，再进行brdf的计算，因此这张贴图也会被称作是specular map；然而在计算间接光照的漫反射部分时，因为目标点会受到来自各个方向上的光线带来的漫反射，不能再简单的使用视线的反射来采样这张帖图。这时有两种解决办法，一种是采样这张贴图的mipmap，在一定程度上模糊的mipmap可以认为是综合了各个方向的光照的信息，另一种则是Ravi Ramamoorthi和Pat Hanrahan2001年在An Efficient Representation for Irradiance Environment Maps中提出的，通过球谐函数重新构建低频光照信息的方式，将其作为简介光漫反射部分的贴图。\n如何使用球谐函数重新构建光照信息 在Ravi Ramamoorthi的论文中他给出了球谐参数的计算公式和重构光照信息的公式： $$ \\tag*{球谐参数} L_{lm} = \\int_{\\theta = 0}^\\pi\\int_{\\theta = 0}^{2\\pi}L(\\theta)Y_{lm}(\\theta, \\phi)sin\\theta d\\theta d\\phi $$ $$ \\begin{align*}其中(x, y, z) \u0026amp;= (sin\\theta cos\\phi, sin\\theta sin\\phi, cos\\theta) \\cr Y_{00}(\\theta, \\phi) \u0026amp;= 0.282095 \\cr (Y_{11};Y_{10};Y_{1-1})(\\theta, \\phi) \u0026amp;= 0.488603(x;z;y) \\cr (Y_{21};Y_{2-1};Y_{2-2})(\\theta, \\phi) \u0026amp;= 1.092548(xz;yz;xy) \\cr Y_{20}(\\theta, \\phi) \u0026amp;= 0.315392(3z^2 - 1) \\cr Y_{22}(\\theta, \\phi) \u0026amp;= 0.546274(x^2 - y^2) \\end{align*} $$ $$ \\tag*{重构光照} \\begin{equation}\\begin{split} E(n) =\u0026amp;\\ c_1L_{22}(x^2 - y^2) + c_3L_{20}z^2 + c_4L_{00} - c_5L_{20} \\cr +\u0026amp;\\ 2c_1(L_{2-2}xy + L_{21}xz + L_{2-1}yz) \\cr +\u0026amp;\\ 2c_2(L_{11}x + L_{1-1}y + L_{10}z)\\end{split}\\end{equation} $$ $$ 其中c1 = 0.","title":"使用Compute Shader计算球谐全局光照"},{"content":"什么是Compute Shader，以及它能用来干什么 首先根据Direct3D 11的说明文档，我们可以知道一个Compute Shader是一个能够利用泛式（通用）的内存访问（即输入和输出），来进行任意运算的可编程着色器。作为类比，我认为Compute Shader和Unity中的Job System很像，准备好一系列的输入，并设置好输出的目标，然后执行这个Compute Shader或者Job，就能得到想要的结果。GPU和多线程，我认为在某种程度上是异曲同工的。Job System有一个优势，就是快，而Compute Shader也有这个优势，而且还要更快！Compute shader背后的思想就是General-purpose computing on graphics processing units(GPGPU)，即在图形处理单元上进行通用计算。\n如何在Unity里面使用Compute Shader 在Unity中创建Compute Shader之后，会得到一个默认的Compute Shader，里面的代码是这样的:\n// Each #kernel tells which function to compile; you can have many kernels #pragma kernel CSMain // Create a RenderTexture with enableRandomWrite flag and set it // with cs.SetTexture RWTexture2D\u0026lt;float4\u0026gt; Result; [numthreads(8,8,1)] void CSMain (uint3 id : SV_DispatchThreadID) { // TODO: insert actual code here! Result[id.xy] = float4(id.x \u0026amp; id.y, (id.x \u0026amp; 15)/15.0, (id.y \u0026amp; 15)/15.0, 0.0); } 需要执行Compute Shader的时候，需要获取该Compute Shader的引用，再调用它的Dispatch的方法：\n//使用(256, 250, 1)的大小创建一个支持随机读写的四通道浮点数的RenderTexture Vector3Int dispatchThreadCount = new Vector3Int(256, 250, 1); RenderTextureDescriptor desc = new RenderTextureDescriptor { dimension = TextureDimension.Tex2D, width = dispatchThreadCount.x, height = dispatchThreadCount.y, volumeDepth = dispatchThreadCount.z, graphicsFormat = UnityEngine.Experimental.Rendering.GraphicsFormat.R16G16B16A16_SFloat, msaaSamples = 1, enableRandomWrite = true, }; renderTexture = new RenderTexture(desc); //找到叫做\u0026#34;CSMain\u0026#34;的kernel int kernel = computeShader.FindKernel(\u0026#34;CSMain\u0026#34;); //获取numthreads，这里获取的值是(8, 8, 1) computeShader.GetKernelThreadGroupSizes(kernel, out uint x, out uint y, out uint z); //计算Dispatch Count，要注意先用浮点数计算，再向上取整 Vector3Int dispatchCount = new Vector3Int(Mathf.CeilToInt(dispatchThreadCount.x / (float)x), Mathf.CeilToInt(dispatchThreadCount.y / (float)y), Mathf.CeilToInt(dispatchThreadCount.z / (float)z)); computeShader.SetTexture(kernel, \u0026#34;Result\u0026#34;, renderTexture); computeShader.Dispatch(kernel, dispatchCount.x, dispatchCount.y, dispatchCount.z); 这样就能够根据Compute Shader里面的代码，在Render Texture上绘制出想要的效果了。最终绘制的效果是这样的： Compute Shader里面的代码有什么含义 可以看到Compute Shader的#pragma kernel CSMain使用了一个叫做CSMain的函数作为kernel，就和普通shader中的vertex和fragment差不多。这个Compute Shader的输入和输出是同一张float4的2D贴图Result。RWTexture2D的RW(Read Write)表示这张贴图支持随机读写(unordered access view, UAV)，这样在Compute Shader读取这张贴图的同时也能对这张贴图进行写入。这里需要注意一张可读写贴图，和两张贴图一张读一张写，在需要读取除当前像素外的像素信息并写入当前像素时，是会有一定区别的。再有就是CSMain中两条看不太懂的代码了[numthreads(8,8,1)]和SV_DispatchThreadID。这里可以参考HLSL官方文档上的定义来进行理解： 通常用到的有SV_GroupID, SV_GroupThreadID, SV_GroupIndex, 和SV_DispatchThreadID，一般按我的想法，我还会额外传入一个_DispatchCount记录Compute Shader的Dispatch的数目。Compute Shader是这样运作的：假设我们执行了Dispatch(5, 3, 2)和numthreads(10, 8, 3)，那么一共会产生5 * 3 * 2 = 30个Thread Group，每个Thread Group会有一个SV_GroupID，范围是(0, 0, 0)到(4, 2, 1)；每个Thread Group中会有10 * 8 * 3个Thread，每个Thread会有一个SV_GroupThreadID, SV_GroupIndex和SV_DispatchThreadID。SV_GroupThreadID的范围是(0, 0, 0)到(9, 7, 2)。SV_GroupIndex是Thread在Thread Group内的序号，其值等于SV_GroupThreadID.z * numthreads.y * numthreads.x + SV_GroupThreadID.y * numthreads.x + SV_GroupThreadID.x，其范围是0到10 * 8 * 3 - 1，在涉及到groupshared memory的时候，往往会用到SV_GroupIndex。SV_DispatchThreadID的范围是(0, 0, 0)到(5 * 10 - 1, 3 * 8 -1, 2 * 3 - 1)，SV_DispatchThreadID的一个优点是对于任何一个Thread，它的SV_DispatchThreadID都是独一无二的，因此在很多情况下都可以利用SV_DispatchThreadID来进行数据操作；此外，在使用RWStructuredBuffer来作为输入输出的时候，往往会需要知道每个Thread Group在所有Thread Group中的序号，或者是每个Thread在所有Thread中的序号，我将其记为IndexGroup和IndexThread，用来和自带的Index区分，IndexGroup的值等于SV_GroupID.z * _DispatchCount.y * _DispatchCount.x + SV_GroupID.y * _DispatchCount.x + SV_GroupID.x，范围是0到5 * 3 * 2 - 1，IndexThread的值等于IndexGroup * numthreads.z * numthreads.y * numthreads.x + SV_GroupIndex，范围是0到5 * 3 * 2 * 10 * 8 * 3 - 1。此外还要注意，由于DispatchCount有被向上取整的可能，所得到的如SV_DispatchThreadID可能会超过贴图、Buffer的大小，最好再传入一个_TextureSize来将SV_DisparchThreadID限制到合适的大小。\n特别需要记住的是，Compute Shader中的numthreads、DispatchCount这些数值，跟输如输出的数据结构没有直接的关系，对于任意的数据结构，都能使用“任意”的numthreads和DispatchCount，但是我们需要在代码中指定这些数值和输入输出的数据结构的关系。比较常用的是使用IndexThread来读取写入RWStructuredBuffer，用SV_DispatchThreadID来读取写入贴图。\n","permalink":"https://zznewclear13.github.io/posts/get-started-with-unity-compute-shader/","summary":"什么是Compute Shader，以及它能用来干什么 首先根据Direct3D 11的说明文档，我们可以知道一个Compute Shader是一个能够利用泛式（通用）的内存访问（即输入和输出），来进行任意运算的可编程着色器。作为类比，我认为Compute Shader和Unity中的Job System很像，准备好一系列的输入，并设置好输出的目标，然后执行这个Compute Shader或者Job，就能得到想要的结果。GPU和多线程，我认为在某种程度上是异曲同工的。Job System有一个优势，就是快，而Compute Shader也有这个优势，而且还要更快！Compute shader背后的思想就是General-purpose computing on graphics processing units(GPGPU)，即在图形处理单元上进行通用计算。\n如何在Unity里面使用Compute Shader 在Unity中创建Compute Shader之后，会得到一个默认的Compute Shader，里面的代码是这样的:\n// Each #kernel tells which function to compile; you can have many kernels #pragma kernel CSMain // Create a RenderTexture with enableRandomWrite flag and set it // with cs.SetTexture RWTexture2D\u0026lt;float4\u0026gt; Result; [numthreads(8,8,1)] void CSMain (uint3 id : SV_DispatchThreadID) { // TODO: insert actual code here! Result[id.xy] = float4(id.x \u0026amp; id.","title":"Unity Compute Shader备忘录"}]