<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Screen Space Reflection | ZZNEWCLEAR13</title><meta name=keywords content="Screen Space Reflection,Screen Space"><meta name=description content="The goal is to get the correct reflection color using as few iterations as possible within a single shader."><meta name=author content="zznewclear13"><link rel=canonical href=https://zznewclear13.github.io/posts/screen-space-reflection-en/><link crossorigin=anonymous href=/assets/css/stylesheet.min.05062af87031756c80e5d65f0cc75e37e589bbf77383569463393b1f73d94f87.css integrity="sha256-BQYq+HAxdWyA5dZfDMdeN+WJu/dzg1aUYzk7H3PZT4c=" rel="preload stylesheet" as=style><link rel=preload href=/images/address.png as=image><link rel=preload href=/apple-touch-icon.png as=image><link rel=icon href=https://zznewclear13.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zznewclear13.github.io/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://zznewclear13.github.io/favicon.ico><link rel=apple-touch-icon href=https://zznewclear13.github.io/favicon.ico><link rel=mask-icon href=https://zznewclear13.github.io/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-157509723-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Screen Space Reflection"><meta property="og:description" content="The goal is to get the correct reflection color using as few iterations as possible within a single shader."><meta property="og:type" content="article"><meta property="og:url" content="https://zznewclear13.github.io/posts/screen-space-reflection-en/"><meta property="og:image" content="https://zznewclear13.github.io/posts/screen-space-reflection-en/posts/images/ScreenSpaceReflection_64.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-09T18:00:00+08:00"><meta property="article:modified_time" content="2024-03-09T18:00:00+08:00"><meta property="og:site_name" content="ZZNEWCLEAR13 - Should I say something cool here?"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zznewclear13.github.io/posts/screen-space-reflection-en/posts/images/ScreenSpaceReflection_64.png"><meta name=twitter:title content="Screen Space Reflection"><meta name=twitter:description content="The goal is to get the correct reflection color using as few iterations as possible within a single shader."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zznewclear13.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Screen Space Reflection","item":"https://zznewclear13.github.io/posts/screen-space-reflection-en/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Screen Space Reflection","name":"Screen Space Reflection","description":"The goal is to get the correct reflection color using as few iterations as possible within a single shader.","keywords":["Screen Space Reflection","Screen Space"],"articleBody":"Screen Space Reflection While screen space reflection (SSR) is a well-known effect, this article aims to introduce a unique method for calculating screen space reflections – one that I haven’t encountered online before.\nMany online tutorials cover screen space reflection already, and most of them follow a similar process: calculate the reflection direction in world space, use a mostly uniform step size to traverse the world space, and for each step, compute the normalized device coordinates (NDC). Then, compare the current depth with the depth value sampled from depth texture. If the ray depth is beyond the sampled depth, consider it a reflection intersection (hit) and sample the color value at that location. This method can yield visually pleasing reflection effects, but hardly anyone mentions the staggering number of iterations required. We’ll discuss this further shortly.\nAdditionally, achieving good reflection results for objects at varying distances often requires different step sizes, but few people delve into this consideration. Some slightly improved approaches involve binary search after ray hitting the scene to ensure smoother transitions between reflection colors. Others may prematurely terminate steps (known as early return) or interpolate reflection colors and environment reflection probe based on comparison between NDC coordinate and the [-1, 1] range.\nCurrently, the most effective screen space ray marching method involves precomputing a Hierarchical ZBuffer. By stepping into and out of different LODs, this approach achieves the same results with fewer iterations. However, Hierarchical ZBuffer is not a feature available in every projects.\nThe most valuable tutorial one can find online is Screen Space Ray Tracing by Morgan McGuire. He also wrote a paper about his algorithm. In his article, McGuire highlights why stepping in world space can be problematic. After undergoing perspective transformation, the step positions in world space may not vary significantly in screen space, leading to the need for more iterations to achieve desirable reflection effects. Also, McGuire presents an ingenious approach in his article. He calculates the coordinates of the starting and ending points in both clip space and screen space. By linearly interpolating the z coordinate in clip space, the 1/w coordinate in clip space, and the xy coordinates in screen space, he eliminates the matrix computations required during each step. Definitely worth using!\nThe goal of this article is to get the correct reflection color using as few iterations as possible within a single shader. Random sampling, blurring, and Fresnel effect are not within the scope of this article. We will focus solely on Windows platform DX11 shaders, which allows us to avoid extensive platform-specific code. The Unity version used for this article is Unity 2022.3.21f1. The final shader code will be provided at the end of the article.\nCalculation of Reflections Parameters The calculation of reflections typically relies on three essential parameters:\n Max Distance: This parameter considers reflections from objects within a certain range around the reflection point. Step Count: Increasing the number of steps results in more accurate reflections but also impacts performance. Thickness Params: In this article, an object’s default thickness is calculated as depth * _Thickness.y + _Thickness.x. This ensures that when a ray passes through behind an object, it is not considered an intersection.  Comparion of Depth Value When considering what kind of depth value to compare during the stepping process, several factors come into play. We define the depth value we obtained from stepping as rayDepth and the depth value obtained from sampling as sampleDepth.\nBy directly sampling the depth texture, we obtain the depth value in normalized device coordinates. Therefore a straightforward approach is to compare these depths in NDC. When rayDepth , the ray intersects with the scene.\nAlternatively, we can compare the actual depth values in view space. This approach allows us to specify a thickness value. If the depth difference exceeds this thickness, we consider the ray passing through behind an object without intersection. Specifically, when rayDepth  sampleDepth \u0026\u0026 rayDepth , the ray intersects with the scene.\nOne thing worth noting is the sampler used when sampling depth texture. Linear interpolation can mistakenly identify intersections at the edges of two faces with different depths, resulting in unwanted artifacts (small dots) on the screen. If available, using a separate texture to mark object edges can help exclude these intersection points. But in our shader, we will stick to a Point Clamp sampler.\nRay Marching Here’s the workflow breakdown:\n  Define k0 and k1 as the reciprocals of the w-components of clip space coordinates for the starting and ending points. Define q0 and q1 as the xyz-components of clip space coordinates for the starting and ending points. Define p0 and p1 as the xy-components of normalized device coordinates for the starting and ending points. Define w as a variable that linearly increases in (0, 1) based on _StepCount. For each step, update the value of w and use it to linearly interpolate the three sets of components (k, q, and p). Calculate rayDepth using q.z * k, sample the depth texture at p to obtain sampleDepth. If rayDepth , the ray intersects with the scene, exit the loop and return p. Sample the color texture at p to obtain the reflection color.   It looks like this (32 steps): Quite poor! The most noticeable issue is the stretching effect. There are primarily two reasons for this: First, we did not use thickness to determine whether the ray passes through behind an object, resulting in significant stretching below suspended objects. Second, we did not restrict positions outside the screen area, causing us to sample depth values from coordinates beyond the screen and return depth values at clampped positions.\nThickness Test To address the thickness issue mentioned earlier, we introduce a method for determining whether the stepping position is behind an object. This method relies on the linear depths from the camera: linearRayDepth and linearSampleDepth.\nAs previously discussed, we use linearSampleDepth * _Thickness.y + _Thickness.x as the thickness of an object in the scene. To determine if the ray passes through behind an object, we compare (linearRayDepth - linearSampleDepth - _Thickness.x) / linearSampleDepth with _Thickness.y. If the former is greater than the latter, it indicates that the ray passes through behind an object.\nfloat getThicknessDiff(float diff, float linearSampleDepth, float2 thicknessParams)\r{\rreturn (diff - thicknessParams.x) / linearSampleDepth;\r}\rThe workflow now becomes:\n If rayDepth and thicknessDiff , the ray intersects with the scene, exit the loop and return p.   It looks like this (32 steps): Frustum Clipping For point p1 that falls outside the screen space, two issues arise: First, sampling the depth texture beyond the screen range yields incorrect depth values. Second, it reduces the effective sampling count. To address this, we can restrict p1 within the screen space. Here’s a method for constraining the stepping endpoint within the view frustum. We define nf as the near and far clipping plane depths (positive values), define s as the slope of the view frustum in the vertical and horizontal directions (positive values). Numerically, s is given by float2(aspect * tan(fovy * 0.5f), tan(fovy * 0.5f)). Note that for ease of calculation, the z-components of from and to are positive.\n#define INFINITY 1e10\rfloat3 frustumClip(float3 from, float3 to, float2 nf, float2 s)\r{\rfloat3 dir = to - from;\rfloat3 signDir = sign(dir);\rfloat nfSlab = signDir.z * (nf.y - nf.x) * 0.5f + (nf.y + nf.x) * 0.5f;\rfloat lenZ = (nfSlab - from.z) / dir.z;\rif (dir.z == 0.0f) lenZ = INFINITY;\rfloat2 ss = sign(dir.xy - s * dir.z) * s;\rfloat2 denom = ss * dir.z - dir.xy;\rfloat2 lenXY = (from.xy - ss * from.z) / denom;\rif (lenXY.x Actually I have wrote a shadertoy to demonstrate frustum clipping in 2D, interactable with mouse:\n\rFrustum Clip 2D\n\rThe workflow now becomes:\n Frustum clip the stepping endpoint to clippedPosVS, and transform it to clip space position endCS.   It looks like this (32 steps): The scene is starting to exhibit some reflection, although there’s still room for improvement. The view frustum clipping has indeed reduced the number of pixels between steps, filling in some of the gaps. However, the reflected colors appear distorted.\nBinary Search In our previous step, although p is ensured to be at the intersected object, there is still some distance from the actual intersection point. To reduce this error, we can use binary search. Here’s how it works: We maintain two variables during stepping, w1 and w2, representing the w values in last two steps (w1  w2). During each binary search iteration, we calculate w = 0.5f * (w1 + w2), if an intersection is detected, update w1 = w, otherwise, update w2 = w and proceed to the next iteration.\nThe workflow now becomes:\n Frustum clip the stepping endpoint to clippedPosVS, and transform it to clip space position endCS. Define k0 and k1 as the reciprocals of the w-components of clip space coordinates for the starting and ending points. Define q0 and q1 as the xyz-components of clip space coordinates for the starting and ending points. Define p0 and p1 as the xy-components of normalized device coordinates for the starting and ending points. Define w1 as a variable that linearly increases in (0, 1) based on _StepCount, initialize w1 and w2 with 0. For each step, w2 = w1, update the value of w1 and use it to linearly interpolate the three sets of components (k, q, and p). Calculate rayDepth using q.z * k, sample the depth texture at p to obtain sampleDepth. If rayDepth and thicknessDiff , the ray intersects with the scene, exit the loop. Let w be the average of w1 and w2. Repeat 5, 6, and 7 to check whether an intersection occurs until the binary search loop ends. We update either w1 or w2 in each step depending on the result. Sample the color texture at p to obtain the reflection color.   It looks like this (32 steps, 5 binary searches): The reflection effect now appears less distorted (particularly noticeable in the bottom left corner). However, there are still gaps between color segments due to our thickness test, which excludes potential intersections.\nPotential Intersections To compute potential intersections, let’s revisit the workflow where we do thickness test. When the ray passes through behind an object, if it is above the scene during last step, we can calculate the difference (thicknessDiff) between rayDepth and sampleDepth. If this value is smaller than the minimum difference (minThicknessDiff), we consider it a potential intersection. We update minThicknessDiff and record the current w1 and w2 for subsequent binary search.\nDuring binary search, if an actual intersection occurs, we follow the original code. If a potential intersection occurs, we also need to track thicknessDiff during binary search. We find the smallest thicknessDiff less than _Thickness.y, use current w to interpolate between p0 and p1 to obtain p, and finally use p to sample the color texture.\nThe workflow now becomes:\n Frustum clip the stepping endpoint to clippedPosVS, and transform it to clip space position endCS. Define k0 and k1 as the reciprocals of the w-components of clip space coordinates for the starting and ending points. Define q0 and q1 as the xyz-components of clip space coordinates for the starting and ending points. Define p0 and p1 as the xy-components of normalized device coordinates for the starting and ending points. Define w1 as a variable that linearly increases in (0, 1) based on _StepCount, initialize w1 and w2 with 0. For each step, w2 = w1, update the value of w1 and use it to linearly interpolate the three sets of components (k, q, and p). Calculate rayDepth using q.z * k, sample the depth texture at p to obtain sampleDepth. If rayDepth and thicknessDiff , the ray intersects with the scene, exit the loop. Otherwise, if rayZ , thicknessDiff  _Thickness.y, and the previous ray was in front of the scene, compare thicknessDiff with the minimum value. If smaller, update the minimum value, record the current w1 and w2, and mark this as a potential intersection, continue looping. If an actual intersection occurs, let w be the average of w1 and w2. Repeat 5, 6, and 7 to check whether an intersection occurs until the binary search loop ends. We update either w1 or w2 in each step depending on the result. If a potential intersection occurs, repeat 5, 6, and 7 to check whether an intersection occurs, and use the smallest thicknessDiff and w to update p. Sample the color texture at p to obtain the reflection color.   It looks like this (32 steps, 5 binary searches): And here is the result using 64 steps and 5 binary searches: ScreenSpaceReflection.shader /*\r// Copyright (c) 2024 zznewclear@gmail.com\r// // Permission is hereby granted, free of charge, to any person obtaining a copy\r// of this software and associated documentation files (the \"Software\"), to deal\r// in the Software without restriction, including without limitation the rights\r// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r// copies of the Software, and to permit persons to whom the Software is\r// furnished to do so, subject to the following conditions:\r// // The above copyright notice and this permission notice shall be included in all\r// copies or substantial portions of the Software.\r// // THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r// SOFTWARE.\r*/\rShader \"zznewclear13/SSRShader\"\r{\rProperties\r{\r[Toggle(USE_POTENTIAL_HIT)] _UsePotentialHit (\"Use Potential Hit\", Float) = 1.0\r[Toggle(USE_FRUSTUM_CLIP)] _UseFrustumClip (\"Use Frustum Clip\", Float) = 1.0\r[Toggle(USE_BINARY_SEARCH)] _UseBinarySearch (\"Use Binary Search\", Float) = 1.0\r[Toggle(USE_THICKNESS)] _UseThickness (\"Use Thickness\", Float) = 1.0\r_MaxDistance (\"Max Distance\", Range(0.1, 100.0)) = 15.0\r[int] _StepCount (\"Step Count\", Float) = 32\r_ThicknessParams (\"Thickness Params\", Vector) = (0.1, 0.02, 0.0, 0.0)\r}\rHLSLINCLUDE\r#include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\"\r#include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\"\r#include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\"\r#pragma shader_feature _ USE_POTENTIAL_HIT\r#pragma shader_feature _ USE_FRUSTUM_CLIP\r#pragma shader_feature _ USE_BINARY_SEARCH\r#pragma shader_feature _ USE_THICKNESS\r#define INFINITY 1e10\r#define DEPTH_SAMPLER sampler_PointClamp\rTexture2D _CameraOpaqueTexture;\rTexture2D _CameraDepthTexture;\rCBUFFER_START(UnityPerMaterial)\rfloat _MaxDistance;\rint _StepCount;\rfloat2 _ThicknessParams;\rCBUFFER_END\rstruct Attributes\r{\rfloat4 positionOS : POSITION;\rfloat3 normalOS : NORMAL;\rfloat2 texcoord : TEXCOORD0;\r};\rstruct Varyings\r{\rfloat4 positionCS : SV_POSITION;\rfloat3 positionWS : TEXCOORD0;\rfloat3 normalWS : TEXCOORD1;\rfloat2 uv : TEXCOORD2;\rfloat3 viewWS : TEXCOORD3;\r};\rVaryings vert(Attributes input)\r{\rVaryings output = (Varyings)0;\rVertexPositionInputs vpi = GetVertexPositionInputs(input.positionOS.xyz);\rVertexNormalInputs vni = GetVertexNormalInputs(input.normalOS);\routput.positionCS = vpi.positionCS;\routput.positionWS = vpi.positionWS;\routput.normalWS = vni.normalWS;\routput.uv = input.texcoord;\routput.viewWS = GetCameraPositionWS() - vpi.positionWS;\rreturn output;\r}\rfloat3 frustumClip(float3 from, float3 to, float2 nf, float2 s)\r{\rfloat3 dir = to - from;\rfloat3 signDir = sign(dir);\rfloat nfSlab = signDir.z * (nf.y - nf.x) * 0.5f + (nf.y + nf.x) * 0.5f;\rfloat lenZ = (nfSlab - from.z) / dir.z;\rif (dir.z == 0.0f) lenZ = INFINITY;\rfloat2 ss = sign(dir.xy - s * dir.z) * s;\rfloat2 denom = ss * dir.z - dir.xy;\rfloat2 lenXY = (from.xy - ss * from.z) / denom;\rif (lenXY.x 0.0f)\r{\rif (thicknessDiff thicknessDiff)\r{\rminPotentialHitPos = thicknessDiff;\rpotentialW12 = float2(w1, w2);\r}\r}\r}\rlastHit = hitDiff  0.0f;\r}\r#else\rfloat w1 = 0.0f;\rfloat w2 = 0.0f;\rbool hit = false;\r[unroll(64)]\rfor (int i=0; i0.0f \u0026\u0026 thicknessDiff 0.0f)\r{\rw1 = w;\rif (hit) hitPos = p;\r}\relse\r{\rw2 = w;\r}\rfloat thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams);\rfloat absThicknessDiff = abs(thicknessDiff);\rif (!hit \u0026\u0026 absThicknessDiff Future Optimization Currently, there is one aspect worth optimizing: controlling the overall step count based on the pixel distance between p0 and p1. We certainly don’t want to step 64 times for just 10 pixels. However, this is a relatively straightforward task, and I’ll leave it to someone with time to spare. As for random sampling, blurring, and Fresnel effects, let’s consider those when we really need them.\nPostscript This article was translated by Microsoft’s Copilot and I made a few adjustments. What an era we live in!\n","wordCount":"3399","inLanguage":"en","image":"https://zznewclear13.github.io/posts/screen-space-reflection-en/posts/images/ScreenSpaceReflection_64.png","datePublished":"2024-03-09T18:00:00+08:00","dateModified":"2024-03-09T18:00:00+08:00","author":{"@type":"Person","name":"zznewclear13"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zznewclear13.github.io/posts/screen-space-reflection-en/"},"publisher":{"@type":"Organization","name":"ZZNEWCLEAR13","logo":{"@type":"ImageObject","url":"https://zznewclear13.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://zznewclear13.github.io/ accesskey=h title="ZZNEWCLEAR13 (Alt + H)"><img src=/apple-touch-icon.png alt=logo aria-label=logo height=35>ZZNEWCLEAR13</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://zznewclear13.github.io/now/ title=进行时><span>进行时</span></a></li><li><a href=https://zznewclear13.github.io/tags/ title=标签><span>标签</span></a></li><li><a href=https://zznewclear13.github.io/categories/ title=分类><span>分类</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://zznewclear13.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://zznewclear13.github.io/posts/>Posts</a></div><h1 class=post-title>Screen Space Reflection</h1><div class=post-description>The goal is to get the correct reflection color using as few iterations as possible within a single shader.</div><div class=post-meta>March 9, 2024&nbsp;·&nbsp;zznewclear13&nbsp;|&nbsp;<a href=https://github.com/zznewclear13/zznewclear13.com/blob/main/content/posts/screen-space-reflection-en.md rel="noopener noreferrer" target=_blank>Edit</a></div></header><figure class=entry-cover><img loading=lazy src=https://zznewclear13.github.io/posts/images/ScreenSpaceReflection_64.png alt="Screen Space Reflection Cover"><p>Screen Space Reflection Example</p></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><div class=details>Screen Space Reflection</div></summary><div class=inner><ul><li><a href=#screen-space-reflection aria-label="Screen Space Reflection">Screen Space Reflection</a></li><li><a href=#calculation-of-reflections aria-label="Calculation of Reflections">Calculation of Reflections</a><ul><li><a href=#parameters aria-label=Parameters>Parameters</a></li><li><a href=#comparion-of-depth-value aria-label="Comparion of Depth Value">Comparion of Depth Value</a></li><li><a href=#ray-marching aria-label="Ray Marching">Ray Marching</a></li><li><a href=#thickness-test aria-label="Thickness Test">Thickness Test</a></li><li><a href=#frustum-clipping aria-label="Frustum Clipping">Frustum Clipping</a></li><li><a href=#binary-search aria-label="Binary Search">Binary Search</a></li><li><a href=#potential-intersections aria-label="Potential Intersections">Potential Intersections</a></li><li><a href=#screenspacereflectionshader aria-label=ScreenSpaceReflection.shader>ScreenSpaceReflection.shader</a></li><li><a href=#future-optimization aria-label="Future Optimization">Future Optimization</a></li></ul></li><li><a href=#postscript aria-label=Postscript>Postscript</a></li></ul></div></details></div><div class=post-content><h2 id=screen-space-reflection>Screen Space Reflection<a hidden class=anchor aria-hidden=true href=#screen-space-reflection>#</a></h2><p>While screen space reflection (SSR) is a well-known effect, this article aims to introduce a unique method for calculating screen space reflections &ndash; one that I haven’t encountered online before.</p><p>Many online tutorials cover screen space reflection already, and most of them follow a similar process: calculate the reflection direction in world space, use a mostly uniform step size to traverse the world space, and for each step, compute the normalized device coordinates (NDC). Then, compare the current depth with the depth value sampled from depth texture. If the ray depth is beyond the sampled depth, consider it a reflection intersection (hit) and sample the color value at that location. This method can yield visually pleasing reflection effects, but hardly anyone mentions the staggering number of iterations required. We’ll discuss this further shortly.</p><p>Additionally, achieving good reflection results for objects at varying distances often requires different step sizes, but few people delve into this consideration. Some slightly improved approaches involve binary search after ray hitting the scene to ensure smoother transitions between reflection colors. Others may prematurely terminate steps (known as early return) or interpolate reflection colors and environment reflection probe based on comparison between NDC coordinate and the [-1, 1] range.</p><p>Currently, the most effective screen space ray marching method involves precomputing a Hierarchical ZBuffer. By stepping into and out of different LODs, this approach achieves the same results with fewer iterations. However, Hierarchical ZBuffer is not a feature available in every projects.</p><p>The most valuable tutorial one can find online is <a href=http://casual-effects.blogspot.com/2014/08/screen-space-ray-tracing.html>Screen Space Ray Tracing</a> by Morgan McGuire. He also wrote a <a href=https://jcgt.org/published/0003/04/04/paper.pdf>paper</a> about his algorithm. In his article, McGuire highlights why stepping in world space can be problematic. After undergoing perspective transformation, the step positions in world space may not vary significantly in screen space, leading to the need for more iterations to achieve desirable reflection effects. Also, McGuire presents an ingenious approach in his article. He calculates the coordinates of the starting and ending points in both clip space and screen space. By linearly interpolating the z coordinate in clip space, the 1/w coordinate in clip space, and the xy coordinates in screen space, he eliminates the matrix computations required during each step. Definitely worth using!</p><p>The goal of this article is to get the correct reflection color using as few iterations as possible within a single shader. Random sampling, blurring, and Fresnel effect are not within the scope of this article. We will focus solely on Windows platform DX11 shaders, which allows us to avoid extensive platform-specific code. The Unity version used for this article is Unity 2022.3.21f1. The final shader code will be provided at the end of the article.</p><h2 id=calculation-of-reflections>Calculation of Reflections<a hidden class=anchor aria-hidden=true href=#calculation-of-reflections>#</a></h2><h3 id=parameters>Parameters<a hidden class=anchor aria-hidden=true href=#parameters>#</a></h3><p>The calculation of reflections typically relies on three essential parameters:</p><ol><li>Max Distance: This parameter considers reflections from objects within a certain range around the reflection point.</li><li>Step Count: Increasing the number of steps results in more accurate reflections but also impacts performance.</li><li>Thickness Params: In this article, an object’s default thickness is calculated as <code>depth * _Thickness.y + _Thickness.x</code>. This ensures that when a ray passes through behind an object, it is not considered an intersection.</li></ol><h3 id=comparion-of-depth-value>Comparion of Depth Value<a hidden class=anchor aria-hidden=true href=#comparion-of-depth-value>#</a></h3><p>When considering what kind of depth value to compare during the stepping process, several factors come into play. We define the depth value we obtained from stepping as <code>rayDepth</code> and the depth value obtained from sampling as <code>sampleDepth</code>.</p><p>By directly sampling the depth texture, we obtain the depth value in normalized device coordinates. Therefore a straightforward approach is to compare these depths in NDC. When <code>rayDepth &lt; sampleDepth</code>, the ray intersects with the scene.</p><p>Alternatively, we can compare the actual depth values in view space. This approach allows us to specify a thickness value. If the depth difference exceeds this thickness, we consider the ray passing through behind an object without intersection. Specifically, when <code>rayDepth > sampleDepth && rayDepth &lt; sampleDepth + thickness</code>, the ray intersects with the scene.</p><p>One thing worth noting is the sampler used when sampling depth texture. Linear interpolation can mistakenly identify intersections at the edges of two faces with different depths, resulting in unwanted artifacts (small dots) on the screen. If available, using a separate texture to mark object edges can help exclude these intersection points. But in our shader, we will stick to a <code>Point Clamp</code> sampler.</p><h3 id=ray-marching>Ray Marching<a hidden class=anchor aria-hidden=true href=#ray-marching>#</a></h3><p>Here’s the workflow breakdown:</p><blockquote><ol><li>Define <code>k0</code> and <code>k1</code> as the reciprocals of the w-components of clip space coordinates for the starting and ending points.</li><li>Define <code>q0</code> and <code>q1</code> as the xyz-components of clip space coordinates for the starting and ending points.</li><li>Define <code>p0</code> and <code>p1</code> as the xy-components of normalized device coordinates for the starting and ending points.</li><li>Define <code>w</code> as a variable that linearly increases in (0, 1) based on <code>_StepCount</code>.</li><li>For each step, update the value of <code>w</code> and use it to linearly interpolate the three sets of components (<code>k</code>, <code>q</code>, and <code>p</code>).</li><li>Calculate <code>rayDepth</code> using <code>q.z * k</code>, sample the depth texture at <code>p</code> to obtain <code>sampleDepth</code>.</li><li>If <code>rayDepth &lt; sampleDepth</code>, the ray intersects with the scene, exit the loop and return <code>p</code>.</li><li>Sample the color texture at <code>p</code> to obtain the reflection color.</li></ol></blockquote><p>It looks like this (32 steps):
<img loading=lazy src=../images/ScreenSpaceReflection_Naive.png#center alt="Screen Space Reflection Naive"></p><p>Quite poor! The most noticeable issue is the stretching effect. There are primarily two reasons for this: First, we did not use thickness to determine whether the ray passes through behind an object, resulting in significant stretching below suspended objects. Second, we did not restrict positions outside the screen area, causing us to sample depth values from coordinates beyond the screen and return depth values at clampped positions.</p><h3 id=thickness-test>Thickness Test<a hidden class=anchor aria-hidden=true href=#thickness-test>#</a></h3><p>To address the thickness issue mentioned earlier, we introduce a method for determining whether the stepping position is behind an object. This method relies on the linear depths from the camera: <code>linearRayDepth</code> and <code>linearSampleDepth</code>.</p><p>As previously discussed, we use <code>linearSampleDepth * _Thickness.y + _Thickness.x</code> as the thickness of an object in the scene. To determine if the ray passes through behind an object, we compare <code>(linearRayDepth - linearSampleDepth - _Thickness.x) / linearSampleDepth</code> with <code>_Thickness.y</code>. If the former is greater than the latter, it indicates that the ray passes through behind an object.</p><pre><code class=language-HLSL data-lang=HLSL>    float getThicknessDiff(float diff, float linearSampleDepth, float2 thicknessParams)
    {
        return (diff - thicknessParams.x) / linearSampleDepth;
    }
</code></pre><p>The workflow now becomes:</p><blockquote><ol start=7><li>If <code>rayDepth &lt; sampleDepth</code> and <code>thicknessDiff &lt; _Thickness.y</code>, the ray intersects with the scene, exit the loop and return <code>p</code>.</li></ol></blockquote><p>It looks like this (32 steps):
<img loading=lazy src=../images/ScreenSpaceReflection_ThicknessTest.png#center alt="Screen Space Reflection Thickness Test"></p><h3 id=frustum-clipping>Frustum Clipping<a hidden class=anchor aria-hidden=true href=#frustum-clipping>#</a></h3><p>For point <code>p1</code> that falls outside the screen space, two issues arise: First, sampling the depth texture beyond the screen range yields incorrect depth values. Second, it reduces the effective sampling count. To address this, we can restrict <code>p1</code> within the screen space. Here’s a method for constraining the stepping endpoint within the view frustum. We define <code>nf</code> as the near and far clipping plane depths (positive values), define <code>s</code> as the slope of the view frustum in the vertical and horizontal directions (positive values). Numerically, <code>s</code> is given by <code>float2(aspect * tan(fovy * 0.5f), tan(fovy * 0.5f))</code>. Note that for ease of calculation, the z-components of <code>from</code> and <code>to</code> are positive.</p><pre><code class=language-HLSL data-lang=HLSL>#define INFINITY 1e10

float3 frustumClip(float3 from, float3 to, float2 nf, float2 s)
{
    float3 dir = to - from;
    float3 signDir = sign(dir);

    float nfSlab = signDir.z * (nf.y - nf.x) * 0.5f + (nf.y + nf.x) * 0.5f;
    float lenZ = (nfSlab - from.z) / dir.z;
    if (dir.z == 0.0f) lenZ = INFINITY;

    float2 ss = sign(dir.xy - s * dir.z) * s;
    float2 denom = ss * dir.z - dir.xy;
    float2 lenXY = (from.xy - ss * from.z) / denom;
    if (lenXY.x &lt; 0.0f || denom.x == 0.0f) lenXY.x = INFINITY;
    if (lenXY.y &lt; 0.0f || denom.y == 0.0f) lenXY.y = INFINITY;

    float len = min(min(1.0f, lenZ), min(lenXY.x, lenXY.y));
    float3 clippedVS = from + dir * len;
    return clippedVS;
}
</code></pre><p>Actually I have wrote a shadertoy to demonstrate frustum clipping in 2D, interactable with mouse:</p><figure class=entry-cover><iframe width=640 height=360 frameborder=0 src="https://www.shadertoy.com/embed/4XfSDB?gui=true&t=10&paused=false&muted=true" allowfullscreen></iframe><p>Frustum Clip 2D</p></figure><p>The workflow now becomes:</p><blockquote><ol start=0><li>Frustum clip the stepping endpoint to <code>clippedPosVS</code>, and transform it to clip space position <code>endCS</code>.</li></ol></blockquote><p>It looks like this (32 steps):
<img loading=lazy src=../images/ScreenSpaceReflection_FrustumClip.png#center alt="Screen Space Reflection Frustum Clip"></p><p>The scene is starting to exhibit some reflection, although there’s still room for improvement. The view frustum clipping has indeed reduced the number of pixels between steps, filling in some of the gaps. However, the reflected colors appear distorted.</p><h3 id=binary-search>Binary Search<a hidden class=anchor aria-hidden=true href=#binary-search>#</a></h3><p>In our previous step, although <code>p</code> is ensured to be at the intersected object, there is still some distance from the actual intersection point. To reduce this error, we can use binary search. Here’s how it works: We maintain two variables during stepping, <code>w1</code> and <code>w2</code>, representing the <code>w</code> values in last two steps (<code>w1 > w2</code>). During each binary search iteration, we calculate <code>w = 0.5f * (w1 + w2)</code>, if an intersection is detected, update <code>w1 = w</code>, otherwise, update <code>w2 = w</code> and proceed to the next iteration.</p><p>The workflow now becomes:</p><blockquote><ol start=0><li>Frustum clip the stepping endpoint to <code>clippedPosVS</code>, and transform it to clip space position <code>endCS</code>.</li><li>Define <code>k0</code> and <code>k1</code> as the reciprocals of the w-components of clip space coordinates for the starting and ending points.</li><li>Define <code>q0</code> and <code>q1</code> as the xyz-components of clip space coordinates for the starting and ending points.</li><li>Define <code>p0</code> and <code>p1</code> as the xy-components of normalized device coordinates for the starting and ending points.</li><li>Define <code>w1</code> as a variable that linearly increases in (0, 1) based on <code>_StepCount</code>, initialize <code>w1</code> and <code>w2</code> with 0.</li><li>For each step, <code>w2 = w1</code>, update the value of <code>w1</code> and use it to linearly interpolate the three sets of components (<code>k</code>, <code>q</code>, and <code>p</code>).</li><li>Calculate <code>rayDepth</code> using <code>q.z * k</code>, sample the depth texture at <code>p</code> to obtain <code>sampleDepth</code>.</li><li>If <code>rayDepth &lt; sampleDepth</code> and <code>thicknessDiff &lt; _Thickness.y</code>, the ray intersects with the scene, exit the loop.</li><li>Let <code>w</code> be the average of <code>w1</code> and <code>w2</code>. Repeat 5, 6, and 7 to check whether an intersection occurs until the binary search loop ends. We update either <code>w1</code> or <code>w2</code> in each step depending on the result.</li><li>Sample the color texture at <code>p</code> to obtain the reflection color.</li></ol></blockquote><p>It looks like this (32 steps, 5 binary searches):
<img loading=lazy src=../images/ScreenSpaceReflection_BinarySearch.png#center alt="Screen Space Reflection Binary Search"></p><p>The reflection effect now appears less distorted (particularly noticeable in the bottom left corner). However, there are still gaps between color segments due to our thickness test, which excludes potential intersections.</p><h3 id=potential-intersections>Potential Intersections<a hidden class=anchor aria-hidden=true href=#potential-intersections>#</a></h3><p>To compute potential intersections, let’s revisit the workflow where we do thickness test. When the ray passes through behind an object, if it is above the scene during last step, we can calculate the difference (<code>thicknessDiff</code>) between <code>rayDepth</code> and <code>sampleDepth</code>. If this value is smaller than the minimum difference (<code>minThicknessDiff</code>), we consider it a potential intersection. We update <code>minThicknessDiff</code> and record the current <code>w1</code> and <code>w2</code> for subsequent binary search.</p><p>During binary search, if an actual intersection occurs, we follow the original code. If a potential intersection occurs, we also need to track <code>thicknessDiff</code> during binary search. We find the smallest <code>thicknessDiff</code> less than <code>_Thickness.y</code>, use current <code>w</code> to interpolate between <code>p0</code> and <code>p1</code> to obtain <code>p</code>, and finally use <code>p</code> to sample the color texture.</p><p>The workflow now becomes:</p><blockquote><ol start=0><li>Frustum clip the stepping endpoint to <code>clippedPosVS</code>, and transform it to clip space position <code>endCS</code>.</li><li>Define <code>k0</code> and <code>k1</code> as the reciprocals of the w-components of clip space coordinates for the starting and ending points.</li><li>Define <code>q0</code> and <code>q1</code> as the xyz-components of clip space coordinates for the starting and ending points.</li><li>Define <code>p0</code> and <code>p1</code> as the xy-components of normalized device coordinates for the starting and ending points.</li><li>Define <code>w1</code> as a variable that linearly increases in (0, 1) based on <code>_StepCount</code>, initialize <code>w1</code> and <code>w2</code> with 0.</li><li>For each step, <code>w2 = w1</code>, update the value of <code>w1</code> and use it to linearly interpolate the three sets of components (<code>k</code>, <code>q</code>, and <code>p</code>).</li><li>Calculate <code>rayDepth</code> using <code>q.z * k</code>, sample the depth texture at <code>p</code> to obtain <code>sampleDepth</code>.</li><li>If <code>rayDepth &lt; sampleDepth</code> and <code>thicknessDiff &lt; _Thickness.y</code>, the ray intersects with the scene, exit the loop.</li><li>Otherwise, if <code>rayZ &lt; sampleZ</code>, <code>thicknessDiff > _Thickness.y</code>, and the previous ray was in front of the scene, compare <code>thicknessDiff</code> with the minimum value. If smaller, update the minimum value, record the current <code>w1</code> and <code>w2</code>, and mark this as a potential intersection, continue looping.</li><li>If an actual intersection occurs, let <code>w</code> be the average of <code>w1</code> and <code>w2</code>. Repeat 5, 6, and 7 to check whether an intersection occurs until the binary search loop ends. We update either <code>w1</code> or <code>w2</code> in each step depending on the result.</li><li>If a potential intersection occurs, repeat 5, 6, and 7 to check whether an intersection occurs, and use the smallest <code>thicknessDiff</code> and <code>w</code> to update <code>p</code>.</li><li>Sample the color texture at <code>p</code> to obtain the reflection color.</li></ol></blockquote><p>It looks like this (32 steps, 5 binary searches):
<img loading=lazy src=../images/ScreenSpaceReflection_PotentialHit.png#center alt="Screen Space Reflection Potential Hit">
And here is the result using 64 steps and 5 binary searches:
<img loading=lazy src=../images/ScreenSpaceReflection_64.png#center alt="Screen Space Reflection Potential Hit"></p><h3 id=screenspacereflectionshader>ScreenSpaceReflection.shader<a hidden class=anchor aria-hidden=true href=#screenspacereflectionshader>#</a></h3><pre><code class=language-HLSL data-lang=HLSL>/*
// Copyright (c) 2024 zznewclear@gmail.com
// 
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the &quot;Software&quot;), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
// 
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
// 
// THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
*/

Shader &quot;zznewclear13/SSRShader&quot;
{
    Properties
    {
        [Toggle(USE_POTENTIAL_HIT)] _UsePotentialHit (&quot;Use Potential Hit&quot;, Float) = 1.0
        [Toggle(USE_FRUSTUM_CLIP)] _UseFrustumClip (&quot;Use Frustum Clip&quot;, Float) = 1.0
        [Toggle(USE_BINARY_SEARCH)] _UseBinarySearch (&quot;Use Binary Search&quot;, Float) = 1.0
        [Toggle(USE_THICKNESS)] _UseThickness (&quot;Use Thickness&quot;, Float) = 1.0
        
        _MaxDistance (&quot;Max Distance&quot;, Range(0.1, 100.0)) = 15.0
        [int] _StepCount (&quot;Step Count&quot;, Float) = 32
        _ThicknessParams (&quot;Thickness Params&quot;, Vector) = (0.1, 0.02, 0.0, 0.0)
    }

    HLSLINCLUDE
    #include &quot;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl&quot;
    #include &quot;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl&quot;
    #include &quot;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl&quot;
    
    #pragma shader_feature _ USE_POTENTIAL_HIT
    #pragma shader_feature _ USE_FRUSTUM_CLIP
    #pragma shader_feature _ USE_BINARY_SEARCH
    #pragma shader_feature _ USE_THICKNESS

    #define INFINITY 1e10
    #define DEPTH_SAMPLER sampler_PointClamp

    Texture2D _CameraOpaqueTexture;
    Texture2D _CameraDepthTexture;
    CBUFFER_START(UnityPerMaterial)
    float _MaxDistance;
    int _StepCount;
    float2 _ThicknessParams;
    CBUFFER_END

    struct Attributes
    {
        float4 positionOS   : POSITION;
        float3 normalOS     : NORMAL;
        float2 texcoord     : TEXCOORD0;
    };

    struct Varyings
    {
        float4 positionCS   : SV_POSITION;
        float3 positionWS   : TEXCOORD0;
        float3 normalWS     : TEXCOORD1;
        float2 uv           : TEXCOORD2;
        float3 viewWS       : TEXCOORD3;
    };

    Varyings vert(Attributes input)
    {
        Varyings output = (Varyings)0;
        VertexPositionInputs vpi = GetVertexPositionInputs(input.positionOS.xyz);
        VertexNormalInputs vni = GetVertexNormalInputs(input.normalOS);

        output.positionCS = vpi.positionCS;
        output.positionWS = vpi.positionWS;
        output.normalWS = vni.normalWS;
        output.uv = input.texcoord;
        output.viewWS = GetCameraPositionWS() - vpi.positionWS;
        return output;
    }

    float3 frustumClip(float3 from, float3 to, float2 nf, float2 s)
    {
        float3 dir = to - from;
        float3 signDir = sign(dir);

        float nfSlab = signDir.z * (nf.y - nf.x) * 0.5f + (nf.y + nf.x) * 0.5f;
        float lenZ = (nfSlab - from.z) / dir.z;
        if (dir.z == 0.0f) lenZ = INFINITY;

        float2 ss = sign(dir.xy - s * dir.z) * s;
        float2 denom = ss * dir.z - dir.xy;
        float2 lenXY = (from.xy - ss * from.z) / denom;
        if (lenXY.x &lt; 0.0f || denom.x == 0.0f) lenXY.x = INFINITY;
        if (lenXY.y &lt; 0.0f || denom.y == 0.0f) lenXY.y = INFINITY;

        float len = min(min(1.0f, lenZ), min(lenXY.x, lenXY.y));
        float3 clippedVS = from + dir * len;
        return clippedVS;
    }

    float getThicknessDiff(float diff, float linearSampleDepth, float2 thicknessParams)
    {
        return (diff - thicknessParams.x) / linearSampleDepth;
    }

    float4 frag(Varyings input) : SV_TARGET
    {
        float3 positionWS = input.positionWS;
        float3 normalWS = normalize(input.normalWS);
        float3 viewWS = normalize(input.viewWS);
        float3 reflWS = reflect(-viewWS, normalWS);
        float3 env = GlossyEnvironmentReflection(reflWS, 0.0f, 1.0f);
        float3 color = env;

        float3 originWS = positionWS;
        float3 endWS = positionWS + reflWS * _MaxDistance;

#if defined(USE_FRUSTUM_CLIP)
        float3 originVS = mul(UNITY_MATRIX_V, float4(originWS, 1.0f)).xyz;
        float3 endVS = mul(UNITY_MATRIX_V, float4(endWS, 1.0f)).xyz;
        float3 flipZ = float3(1.0f, 1.0f, -1.0f);
        float3 clippedVS = frustumClip(originVS * flipZ, endVS * flipZ, _ProjectionParams.yz, float2(1.0f, -1.0f) / UNITY_MATRIX_P._m00_m11);
        clippedVS *= flipZ;
        float4 originCS = mul(UNITY_MATRIX_VP, float4(originWS, 1.0f));
        float4 endCS = mul(UNITY_MATRIX_P, float4(clippedVS, 1.0f));
#else
        float4 originCS = mul(UNITY_MATRIX_VP, float4(originWS, 1.0f));
        float4 endCS = mul(UNITY_MATRIX_VP, float4(endWS, 1.0f));
#endif

        float k0 = 1.0f / originCS.w;
        float k1 = 1.0f / endCS.w;
        float3 q0 = originCS.xyz;
        float3 q1 = endCS.xyz;
        float2 p0 = originCS.xy * float2(1.0f, -1.0f) * k0 * 0.5f + 0.5f;
        float2 p1 = endCS.xy * float2(1.0f, -1.0f) * k1 * 0.5f + 0.5f;

#if defined(USE_POTENTIAL_HIT)
        float w1 = 0.0f;
        float w2 = 0.0f;
        bool hit = false;
        bool lastHit = false;
        bool potentialHit = false;
        float2 potentialW12 = float2(0.0f, 0.0f);
        float minPotentialHitPos = INFINITY;
        [unroll(64)]
        for (int i=0; i&lt;_StepCount; ++i)
        {
            w2 = w1;
            w1 += 1.0f / float(_StepCount);

            float3 q = lerp(q0, q1, w1);
            float2 p = lerp(p0, p1, w1);
            float k = lerp(k0, k1, w1);
            float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r;
            float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams);
            float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams);

            float hitDiff = linearRayDepth - linearSampleDepth;
            float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams);
            if (hitDiff &gt; 0.0f)
            {
                if (thicknessDiff &lt; _ThicknessParams.y)
                {
                    hit = true;
                    break;
                }
                else if(!lastHit)
                {
                    potentialHit = true;
                    if (minPotentialHitPos &gt; thicknessDiff)
                    {
                        minPotentialHitPos = thicknessDiff;
                        potentialW12 = float2(w1, w2);
                    }
                }
            }
            lastHit = hitDiff &gt; 0.0f;
        }
#else
        float w1 = 0.0f;
        float w2 = 0.0f;
        bool hit = false;
        [unroll(64)]
        for (int i=0; i&lt;_StepCount; ++i)
        {
            w2 = w1;
            w1 += 1.0f / float(_StepCount);

            float3 q = lerp(q0, q1, w1);
            float2 p = lerp(p0, p1, w1);
            float k = lerp(k0, k1, w1);
            float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r;
#if defined(USE_THICKNESS)
            float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams);
            float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams);

            float hitDiff = linearRayDepth - linearSampleDepth;
            float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams);
            if (hitDiff &gt; 0.0f &amp;&amp; thicknessDiff &lt; _ThicknessParams.y)
            {
                hit = true;
                break;
            }       
#else
            if (q.z * k &lt; sampleDepth)
            {
                hit = true;
                break;
            }
#endif
        }
#endif

#if defined(USE_POTENTIAL_HIT)
        if (hit || potentialHit)
        {
            if (!hit)
            {
                w1 = potentialW12.x;
                w2 = potentialW12.y;
            }

            bool realHit = false;
            float2 hitPos;
            float minThicknessDiff = _ThicknessParams.y;
            [unroll(5)]
            for (int i=0; i&lt;5; ++i)
            {
                float w = 0.5f * (w1 + w2);
                float3 q = lerp(q0, q1, w1);
                float2 p = lerp(p0, p1, w1);
                float k = lerp(k0, k1, w1);
                float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r;
                float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams);
                float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams);
                float hitDiff = linearRayDepth - linearSampleDepth;

                if (hitDiff &gt; 0.0f)
                {
                    w1 = w;
                    if (hit) hitPos = p;
                }
                else
                {
                    w2 = w;
                }

                float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams);
                float absThicknessDiff = abs(thicknessDiff);
                if (!hit &amp;&amp; absThicknessDiff &lt; minThicknessDiff) 
                {
                    realHit = true;
                    minThicknessDiff = thicknessDiff;
                    hitPos = p;
                }
            }

            if (hit || realHit) color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f;
        }
#elif defined(USE_BINARY_SEARCH)
        if (hit)
        {
            float2 hitPos;
            [unroll(5)]
            for (int i=0; i&lt;5; ++i)
            {
                float w = 0.5f * (w1 + w2);
                float3 q = lerp(q0, q1, w1);
                float2 p = lerp(p0, p1, w1);
                float k = lerp(k0, k1, w1);

                float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r;
                if (q.z * k &lt; sampleDepth)
                {
                    w1 = w;
                    hitPos = p;
                }
                else
                {
                    w2 = w;
                }
            }
            color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f;
        }
#else
        if (hit)
        {
            float2 hitPos = lerp(p0, p1, w1);
            color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f;
        }
#endif

        return float4(color, 1.0f);
    }

    ENDHLSL

    SubShader
    {
        Tags { &quot;RenderType&quot;=&quot;Transparent&quot; &quot;Queue&quot;=&quot;Transparent&quot; }
        LOD 100

        Pass
        {
            HLSLPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            ENDHLSL
        }
    }
}
</code></pre><h3 id=future-optimization>Future Optimization<a hidden class=anchor aria-hidden=true href=#future-optimization>#</a></h3><p>Currently, there is one aspect worth optimizing: controlling the overall step count based on the pixel distance between <code>p0</code> and <code>p1</code>. We certainly don’t want to step 64 times for just 10 pixels. However, this is a relatively straightforward task, and I’ll leave it to someone with time to spare. As for random sampling, blurring, and Fresnel effects, let’s consider those when we really need them.</p><h2 id=postscript>Postscript<a hidden class=anchor aria-hidden=true href=#postscript>#</a></h2><p>This article was translated by Microsoft’s Copilot and I made a few adjustments. What an era we live in!</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://zznewclear13.github.io/tags/screen-space-reflection/>Screen Space Reflection</a></li><li><a href=https://zznewclear13.github.io/tags/screen-space/>Screen Space</a></li></ul><nav class=paginav><a class=next href=https://zznewclear13.github.io/posts/screen-space-reflection/><span class=title>Next Page »</span><br><span>屏幕空间反射</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://zznewclear13.github.io/>ZZNEWCLEAR13</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script></body></html>