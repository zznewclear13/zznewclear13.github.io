<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>从深度图中获取视空间的法线 | ZZNEWCLEAR13</title><meta name=keywords content="Space Transformation"><meta name=description content="记录一下从深度图重建视空间法线的方法."><meta name=author content="zznewclear13"><link rel=canonical href=https://zznewclear13.github.io/posts/get-view-space-normal-from-depth-texture/><link crossorigin=anonymous href=/assets/css/stylesheet.min.05062af87031756c80e5d65f0cc75e37e589bbf77383569463393b1f73d94f87.css integrity="sha256-BQYq+HAxdWyA5dZfDMdeN+WJu/dzg1aUYzk7H3PZT4c=" rel="preload stylesheet" as=style><link rel=preload href=/images/address.png as=image><link rel=preload href=/apple-touch-icon.png as=image><link rel=icon href=https://zznewclear13.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zznewclear13.github.io/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://zznewclear13.github.io/favicon.ico><link rel=apple-touch-icon href=https://zznewclear13.github.io/favicon.ico><link rel=mask-icon href=https://zznewclear13.github.io/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css integrity=sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js integrity=sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-157509723-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="从深度图中获取视空间的法线"><meta property="og:description" content="记录一下从深度图重建视空间法线的方法."><meta property="og:type" content="article"><meta property="og:url" content="https://zznewclear13.github.io/posts/get-view-space-normal-from-depth-texture/"><meta property="og:image" content="https://zznewclear13.github.io/posts/get-view-space-normal-from-depth-texture/posts/images/ViewSpaceNormalFromDepthTexture.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-27T12:00:00+08:00"><meta property="article:modified_time" content="2022-01-27T12:00:00+08:00"><meta property="og:site_name" content="ZZNEWCLEAR13 - Should I say something cool here?"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zznewclear13.github.io/posts/get-view-space-normal-from-depth-texture/posts/images/ViewSpaceNormalFromDepthTexture.jpg"><meta name=twitter:title content="从深度图中获取视空间的法线"><meta name=twitter:description content="记录一下从深度图重建视空间法线的方法."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zznewclear13.github.io/posts/"},{"@type":"ListItem","position":2,"name":"从深度图中获取视空间的法线","item":"https://zznewclear13.github.io/posts/get-view-space-normal-from-depth-texture/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"从深度图中获取视空间的法线","name":"从深度图中获取视空间的法线","description":"记录一下从深度图重建视空间法线的方法.","keywords":["Space Transformation"],"articleBody":"为什么要从深度图重建视空间法线 一个很大的应用情景是在后处理的阶段，或是计算一些屏幕空间的效果（如SSR、SSAO等），只能获取到一张深度贴图，而不是每一个几何体的顶点数据，很多的计算中却又需要用到世界空间的法线或者是视空间的法线，这时我们就需要通过深度图来重建视空间的法线。（诶这段话我是不是写过一遍了）\n重建视空间法线的方法 bgolus在他的WorldNormalFromDepthTexture.shader里面很全面的介绍了各种重建视空间法线的方法。其中比较值得注意的是来自Janos Turanszki的根据深度差判断当前像素属于哪个平面的方法，和来自吴彧文的横向和纵向多采样一个点来判断当前像素属于哪个平面的方法，其中吴彧文的方法能够在绝大部分情况下获取到最准确的法线（除了尖角的一个像素）。\n除了bgolus介绍的方法之外，我在GameTechDev/XeGTAO中还看到了一种方法。这种方法类似于Janos Turanszki的深度差的方法，不过从深度差中获取的是0-1的边缘值（edgesLRTB，edgesLRTB.x越接近0即代表该像素的左侧越是一条边缘），再使用边缘的两两乘积对四个法线进行插值，最终计算出视空间法线。我个人认为当在两个面相接的地方不需要特别准确的法线值时，这是最好的计算法线的方法。用这个方式计算的法线，在两个面相接的地方，法线会有一种从一个面插值到另一个面的效果（且一定程度上抗锯齿），在两个面远近排布的时候，也能获取到准确的法线。\n具体的实现方法  根据需要使用的方法，采样深度图。在采样比较集中的情况下，可以使用GatherRed方法来减少采样的次数。GatherRed可以得到双线性采样时的四个像素的R通道的值并封装到一个float4中，当屏幕左下角是(0, 0)时，这个float4的x分量对应采样点左上角的颜色的R通道的值，y对应右上角，z对应右下角，w对应左下角，可以在HLSL的文档中看到Gather的相关介绍。Compute Shader的话可以使用group shared memory进一步减少采样。 使用深度图和当前的uv值计算出像素的视空间的坐标，这一步尤其需要注意视空间坐标Z分量的正负性的问题。Unity的视空间变换矩阵UNITY_MATRIX_V是摄像机位于视空间(0, 0, 0)，看向视空间Z轴负方向的，右手系的矩阵。即视空间的坐标Z分量往往是一个负值，其法线的Z分量在往往下是正值（即画面看上去应该多为蓝色）。 从深度图中计算视空间坐标的时候，如果Unity版本比较旧，会没有UNITY_MATRIX_I_P这个矩阵，这时可以使用unity_CameraInvProjection来代替，但需要注意DirectX平台UV上下翻转的问题。 当屏幕左下角是(0, 0)时，使用右侧的视空间坐标减去左侧的视空间坐标，使用上侧的视空间坐标减去下侧的视空间坐标。五个采样点（包括位于中心的当前像素）可以获得四个向量，对于右手系的视空间坐标来说，将这四个向量按照水平向量叉乘竖直向量的顺序，就可以获得四个当前像素的法线了。 最后使用前面介绍的获取法线的方法，从这四个法线中获取最为正确的法线。这些方法往往都会使用深度值来进行判断，这里需要注意的是透视变换带来的深度的非线性的问题。对于屏幕上等距分布的三个点ABC，当他们在世界空间中处于同一条直线时，有 $$ 2 \\cdot rawDepthB = rawDepthA + rawDepthC \\newline \\frac 2 {linearDepthB} = \\frac 1 {linearDepthA} + \\frac 1 {linearDepthC} $$  ReconstructNormalComputeShader.compute 使用GatherRed的方法，可以减少ReconstructNormalAccurate所需要的的采样，但是在屏幕的边缘会有一些瑕疵，把采样的sampler改成sampler_LinearRepeat在一定程度上能够解决这些瑕疵。这样的话ReconstructNormalFast需要两次采样，ReconstructNormalAccurate则需要五次采样。 要注意使用边缘信息对法线进行插值的方法，需要先对法线进行归一化，不然叉乘导致前后平面计算出的向量长度会远大于同一平面的向量长度，影响最终的法线。\n#pragma kernel ReconstructNormalFast\r#pragma kernel ReconstructNormalAccurate\r#include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\"\r#include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\"\r#define FEWER_SAMPLES 0\rTexture2D _DepthTexture;\rRWTexture2D _RW_NormalTexture;\rSamplerState sampler_LinearClamp;\rSamplerState sampler_LinearRepeat;\rfloat4 _TextureSize;\rfloat3 GetViewSpacePosition(float2 uv, float depth)\r{\r#if UNITY_UV_STARTS_AT_TOP\ruv.y = 1.0 - uv.y;\r#endif\rfloat3 positionNDC = float3(uv * 2.0 - 1.0, depth);\rfloat4 positionVS = mul(UNITY_MATRIX_I_P, float4(positionNDC, 1.0));\rpositionVS /= positionVS.w;\rreturn positionVS.xyz;\r}\r//-UNITY_MATRIX_P._m11 = rcp(tan(fovy / 2))\rfloat3 GetViewSpacePositionFromLinearDepth(float2 uv, float linearDepth)\r{\r#if UNITY_UV_STARTS_AT_TOP\ruv.y = 1.0 - uv.y;\r#endif\rfloat2 uvNDC = uv * 2.0 - 1.0;\rreturn float3(uvNDC * linearDepth * UNITY_MATRIX_I_P._m00_m11, -linearDepth);\r}\r//Calculate 4 linear eye depths at one time\rfloat4 LinearEyeDepthFloat4(float4 depthTBLR, float4 zBufferParams)\r{\rreturn rcp(depthTBLR * zBufferParams.z + zBufferParams.w);\r}\r//Heavily based on normal reconstruction method in github repository GameTechDev/XeGTAO.\r//https://github.com/GameTechDev/XeGTAO/blob/0d177ce06bfa642f64d8af4de1197ad1bcb862d4/Source/Rendering/Shaders/XeGTAO.hlsli#L143-L160\r[numthreads(8,8,1)]\rvoid ReconstructNormalFast (uint3 dispatchThreadID : SV_DispatchThreadID)\r{\rfloat4 depthGatherBL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw);\rfloat4 depthGatherTR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(1, 1));\rfloat depthC = depthGatherBL.y;\rfloat depthT = depthGatherTR.x;\rfloat depthB = depthGatherBL.z;\rfloat depthL = depthGatherBL.x;\rfloat depthR = depthGatherTR.z;\rfloat linearDepth = LinearEyeDepth(depthC, _ZBufferParams);\rfloat4 linearDepths = LinearEyeDepthFloat4(float4(depthT, depthB, depthL, depthR), _ZBufferParams);\rfloat4 depthDifferenceTBLR = linearDepths - linearDepth;\rfloat slopeTB = (depthDifferenceTBLR.x - depthDifferenceTBLR.y) * 0.5;\rfloat slopeLR = (depthDifferenceTBLR.w - depthDifferenceTBLR.z) * 0.5;\rfloat4 depthDifferenceTBLRAverage = depthDifferenceTBLR + float4(-slopeTB, slopeTB, slopeLR, -slopeLR);\rdepthDifferenceTBLR = min(abs(depthDifferenceTBLR), abs(depthDifferenceTBLRAverage));\r//0: edge; 1: non-edge\rfloat4 edgesTBLR = saturate(1.25 - depthDifferenceTBLR / (linearDepth * 0.011));\r//TL, TR, BR, BL float4 acceptedNormals = saturate(float4(edgesTBLR.x * edgesTBLR.z, edgesTBLR.w * edgesTBLR.x, edgesTBLR.y * edgesTBLR.w, edgesTBLR.z * edgesTBLR.y) + 0.001);\rfloat3 viewPosC = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 0.5)) * _TextureSize.zw, depthC);\rfloat3 viewPosT = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 1.5)) * _TextureSize.zw, depthT);\rfloat3 viewPosB = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, -0.5)) * _TextureSize.zw, depthB);\rfloat3 viewPosL = GetViewSpacePosition((dispatchThreadID.xy + float2(-0.5, 0.5)) * _TextureSize.zw, depthL);\rfloat3 viewPosR = GetViewSpacePosition((dispatchThreadID.xy + float2(1.5, 0.5)) * _TextureSize.zw, depthR);\rfloat3 t = normalize(viewPosT - viewPosC);\rfloat3 b = normalize(viewPosC - viewPosB);\rfloat3 l = normalize(viewPosC - viewPosL);\rfloat3 r = normalize(viewPosR - viewPosC);\rfloat3 normalVS = acceptedNormals.x * cross(l, t) + acceptedNormals.y * cross(r, t) + acceptedNormals.z * cross(r, b) + acceptedNormals.w * cross(l, b);\rnormalVS = normalize(normalVS);\r_RW_NormalTexture[dispatchThreadID.xy] = float4(normalVS, 1.0);\r}\r//Heavily based on github gist bgolus/WorldNormalFromDepthTexture.shader\r//https://gist.github.com/bgolus/a07ed65602c009d5e2f753826e8078a0#file-worldnormalfromdepthtexture-shader-L153-L218\r//https://atyuwen.github.io/posts/normal-reconstruction/\r[numthreads(8,8,1)]\rvoid ReconstructNormalAccurate (uint3 dispatchThreadID : SV_DispatchThreadID)\r{\r#if FEWER_SAMPLES\rfloat4 depthGatherTL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(-1, 1));\rfloat4 depthGatherTR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(1, 2));\rfloat4 depthGatherBR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(2, 0));\rfloat4 depthGatherBL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(0, -1));\rfloat depthC = _DepthTexture.Load(int3(dispatchThreadID.xy, 0));\rfloat depthT = depthGatherTR.w;\rfloat depthB = depthGatherBL.y;\rfloat depthL = depthGatherTL.z;\rfloat depthR = depthGatherBR.x;\rfloat depthT2 = depthGatherTR.x;\rfloat depthB2 = depthGatherBL.z;\rfloat depthL2 = depthGatherTL.w;\rfloat depthR2 = depthGatherBR.y;\r#else\rfloat4 depthGatherBL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw);\rfloat4 depthGatherTR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(1, 1));\rfloat depthC = depthGatherBL.y;\rfloat depthT = depthGatherTR.x;\rfloat depthB = depthGatherBL.z;\rfloat depthL = depthGatherBL.x;\rfloat depthR = depthGatherTR.z;\rfloat depthT2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(0, 2), 0));\rfloat depthB2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(0, -2), 0));\rfloat depthL2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(-2, 0), 0));\rfloat depthR2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(2, 0), 0));\r#endif\rfloat3 viewPosC = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 0.5)) * _TextureSize.zw, depthC);\rfloat3 viewPosT = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 1.5)) * _TextureSize.zw, depthT);\rfloat3 viewPosB = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, -0.5)) * _TextureSize.zw, depthB);\rfloat3 viewPosL = GetViewSpacePosition((dispatchThreadID.xy + float2(-0.5, 0.5)) * _TextureSize.zw, depthL);\rfloat3 viewPosR = GetViewSpacePosition((dispatchThreadID.xy + float2(1.5, 0.5)) * _TextureSize.zw, depthR);\rfloat3 t = viewPosT - viewPosC;\rfloat3 b = viewPosC - viewPosB;\rfloat3 l = viewPosC - viewPosL;\rfloat3 r = viewPosR - viewPosC;\rfloat4 H = float4(depthL, depthR, depthL2, depthR2);\rfloat4 V = float4(depthB, depthT, depthB2, depthT2);\rfloat2 he = abs((2 * H.xy - H.zw) - depthC);\rfloat2 ve = abs((2 * V.xy - V.zw) - depthC);\rfloat3 hDeriv = he.x 最后的思考 本来还想使用3x3的采样，使用类似于吴彧文的方法，延伸第三个点到当前像素来计算准确的法线的，但是实际操作了一下发现，只要四个点构成了平行四边形就会认为是接近于当前采样，于是就会导致计算出错误的法线了。XeGTAO里面使用的计算法线的方式确实很巧妙，应该多用用，之后可能会再写一篇计算GTAO的文章吧。\n","wordCount":"701","inLanguage":"en","image":"https://zznewclear13.github.io/posts/get-view-space-normal-from-depth-texture/posts/images/ViewSpaceNormalFromDepthTexture.jpg","datePublished":"2022-01-27T12:00:00+08:00","dateModified":"2022-01-27T12:00:00+08:00","author":{"@type":"Person","name":"zznewclear13"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zznewclear13.github.io/posts/get-view-space-normal-from-depth-texture/"},"publisher":{"@type":"Organization","name":"ZZNEWCLEAR13","logo":{"@type":"ImageObject","url":"https://zznewclear13.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://zznewclear13.github.io/ accesskey=h title="ZZNEWCLEAR13 (Alt + H)"><img src=/apple-touch-icon.png alt=logo aria-label=logo height=35>ZZNEWCLEAR13</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://zznewclear13.github.io/outreach/ title=对外联系><span>对外联系</span></a></li><li><a href=https://zznewclear13.github.io/now/ title=进行时><span>进行时</span></a></li><li><a href=https://zznewclear13.github.io/memos/ title=备忘录><span>备忘录</span></a></li><li><a href=https://zznewclear13.github.io/tags/ title=标签><span>标签</span></a></li><li><a href=https://zznewclear13.github.io/categories/ title=分类><span>分类</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://zznewclear13.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://zznewclear13.github.io/posts/>Posts</a></div><h1 class=post-title>从深度图中获取视空间的法线</h1><div class=post-description>记录一下从深度图重建视空间法线的方法.</div><div class=post-meta>January 27, 2022&nbsp;·&nbsp;zznewclear13&nbsp;|&nbsp;<a href=https://github.com/zznewclear13/zznewclear13.com/blob/main/content/posts/get-view-space-normal-from-depth-texture.md rel="noopener noreferrer" target=_blank>编辑</a></div></header><figure class=entry-cover><img loading=lazy src=https://zznewclear13.github.io/posts/images/ViewSpaceNormalFromDepthTexture.jpg alt="View Space Normal From Depth Texture Cover"><p>View Space Normal From Depth Texture</p></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><div class=details>从深度图中获取视空间的法线</div></summary><div class=inner><ul><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e4%bb%8e%e6%b7%b1%e5%ba%a6%e5%9b%be%e9%87%8d%e5%bb%ba%e8%a7%86%e7%a9%ba%e9%97%b4%e6%b3%95%e7%ba%bf aria-label=为什么要从深度图重建视空间法线>为什么要从深度图重建视空间法线</a></li><li><a href=#%e9%87%8d%e5%bb%ba%e8%a7%86%e7%a9%ba%e9%97%b4%e6%b3%95%e7%ba%bf%e7%9a%84%e6%96%b9%e6%b3%95 aria-label=重建视空间法线的方法>重建视空间法线的方法</a></li><li><a href=#%e5%85%b7%e4%bd%93%e7%9a%84%e5%ae%9e%e7%8e%b0%e6%96%b9%e6%b3%95 aria-label=具体的实现方法>具体的实现方法</a><ul><li><a href=#reconstructnormalcomputeshadercompute aria-label=ReconstructNormalComputeShader.compute>ReconstructNormalComputeShader.compute</a></li></ul></li><li><a href=#%e6%9c%80%e5%90%8e%e7%9a%84%e6%80%9d%e8%80%83 aria-label=最后的思考>最后的思考</a></li></ul></div></details></div><div class=post-content><h2 id=为什么要从深度图重建视空间法线>为什么要从深度图重建视空间法线<a hidden class=anchor aria-hidden=true href=#为什么要从深度图重建视空间法线>#</a></h2><p>一个很大的应用情景是在后处理的阶段，或是计算一些屏幕空间的效果（如SSR、SSAO等），只能获取到一张深度贴图，而不是每一个几何体的顶点数据，很多的计算中却又需要用到世界空间的法线或者是视空间的法线，这时我们就需要通过深度图来重建视空间的法线。（诶这段话我是不是写过一遍了）</p><h2 id=重建视空间法线的方法>重建视空间法线的方法<a hidden class=anchor aria-hidden=true href=#重建视空间法线的方法>#</a></h2><p>bgolus在他的<a href=https://gist.github.com/bgolus/a07ed65602c009d5e2f753826e8078a0>WorldNormalFromDepthTexture.shader</a>里面很全面的介绍了各种重建视空间法线的方法。其中比较值得注意的是来自Janos Turanszki的根据深度差判断当前像素属于哪个平面的<a href=https://wickedengine.net/2019/09/22/improved-normal-reconstruction-from-depth/>方法</a>，和来自吴彧文的横向和纵向多采样一个点来判断当前像素属于哪个平面的<a href=https://atyuwen.github.io/posts/normal-reconstruction/>方法</a>，其中吴彧文的方法能够在绝大部分情况下获取到最准确的法线（除了尖角的一个像素）。</p><p>除了bgolus介绍的方法之外，我在<a href=https://github.com/GameTechDev/XeGTAO/blob/0d177ce06bfa642f64d8af4de1197ad1bcb862d4/Source/Rendering/Shaders/XeGTAO.hlsli#L143-L160>GameTechDev/XeGTAO</a>中还看到了一种方法。这种方法类似于Janos Turanszki的深度差的方法，不过从深度差中获取的是0-1的边缘值（edgesLRTB，edgesLRTB.x越接近0即代表该像素的左侧越是一条边缘），再使用边缘的两两乘积对四个法线进行插值，最终计算出视空间法线。我个人认为当在两个面相接的地方不需要特别准确的法线值时，这是最好的计算法线的方法。用这个方式计算的法线，在两个面相接的地方，法线会有一种从一个面插值到另一个面的效果（且一定程度上抗锯齿），在两个面远近排布的时候，也能获取到准确的法线。</p><h2 id=具体的实现方法>具体的实现方法<a hidden class=anchor aria-hidden=true href=#具体的实现方法>#</a></h2><ol><li>根据需要使用的方法，采样深度图。在采样比较集中的情况下，可以使用<code>GatherRed</code>方法来减少采样的次数。<code>GatherRed</code>可以得到双线性采样时的四个像素的R通道的值并封装到一个<code>float4</code>中，当屏幕左下角是(0, 0)时，这个<code>float4</code>的x分量对应采样点左上角的颜色的R通道的值，y对应右上角，z对应右下角，w对应左下角，可以在HLSL的<a href=https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/gather4--sm5---asm->文档</a>中看到<code>Gather</code>的相关介绍。Compute Shader的话可以使用<code>group shared memory</code>进一步减少采样。</li><li>使用深度图和当前的uv值计算出像素的视空间的坐标，这一步尤其需要注意视空间坐标Z分量的正负性的问题。Unity的视空间变换矩阵<code>UNITY_MATRIX_V</code>是摄像机位于视空间(0, 0, 0)，看向视空间Z轴负方向的，右手系的矩阵。即视空间的坐标Z分量往往是一个负值，其法线的Z分量在往往下是正值（即画面看上去应该多为蓝色）。</li><li>从深度图中计算视空间坐标的时候，如果Unity版本比较旧，会没有<code>UNITY_MATRIX_I_P</code>这个矩阵，这时可以使用<code>unity_CameraInvProjection</code>来代替，但需要注意DirectX平台UV上下翻转的问题。</li><li>当屏幕左下角是(0, 0)时，使用右侧的视空间坐标减去左侧的视空间坐标，使用上侧的视空间坐标减去下侧的视空间坐标。五个采样点（包括位于中心的当前像素）可以获得四个向量，对于右手系的视空间坐标来说，将这四个向量按照水平向量叉乘竖直向量的顺序，就可以获得四个当前像素的法线了。</li><li>最后使用前面介绍的获取法线的方法，从这四个法线中获取最为正确的法线。这些方法往往都会使用深度值来进行判断，这里需要注意的是透视变换带来的深度的非线性的问题。对于屏幕上等距分布的三个点ABC，当他们在世界空间中处于同一条直线时，有
$$
2 \cdot rawDepthB = rawDepthA + rawDepthC
\newline
\frac 2 {linearDepthB} = \frac 1 {linearDepthA} + \frac 1 {linearDepthC}
$$</li></ol><h3 id=reconstructnormalcomputeshadercompute>ReconstructNormalComputeShader.compute<a hidden class=anchor aria-hidden=true href=#reconstructnormalcomputeshadercompute>#</a></h3><p>使用<code>GatherRed</code>的方法，可以减少<code>ReconstructNormalAccurate</code>所需要的的采样，但是在屏幕的边缘会有一些瑕疵，把采样的sampler改成<code>sampler_LinearRepeat</code>在一定程度上能够解决这些瑕疵。这样的话<code>ReconstructNormalFast</code>需要两次采样，<code>ReconstructNormalAccurate</code>则需要五次采样。
要注意使用边缘信息对法线进行插值的方法，需要先对法线进行归一化，不然叉乘导致前后平面计算出的向量长度会远大于同一平面的向量长度，影响最终的法线。</p><pre><code class=language-HLSL data-lang=HLSL>#pragma kernel ReconstructNormalFast
#pragma kernel ReconstructNormalAccurate

#include &quot;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl&quot;
#include &quot;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl&quot;

#define FEWER_SAMPLES 0

Texture2D&lt;float&gt; _DepthTexture;
RWTexture2D&lt;float4&gt; _RW_NormalTexture;

SamplerState sampler_LinearClamp;
SamplerState sampler_LinearRepeat;
float4 _TextureSize;

float3 GetViewSpacePosition(float2 uv, float depth)
{
#if UNITY_UV_STARTS_AT_TOP
    uv.y = 1.0 - uv.y;
#endif
    float3 positionNDC = float3(uv * 2.0 - 1.0, depth);
    float4 positionVS = mul(UNITY_MATRIX_I_P, float4(positionNDC, 1.0));
    positionVS /= positionVS.w;
    return positionVS.xyz;
}

//-UNITY_MATRIX_P._m11 = rcp(tan(fovy / 2))
float3 GetViewSpacePositionFromLinearDepth(float2 uv, float linearDepth)
{
#if UNITY_UV_STARTS_AT_TOP
    uv.y = 1.0 - uv.y;
#endif
    float2 uvNDC = uv * 2.0 - 1.0;
    return float3(uvNDC * linearDepth * UNITY_MATRIX_I_P._m00_m11, -linearDepth);
}

//Calculate 4 linear eye depths at one time
float4 LinearEyeDepthFloat4(float4 depthTBLR, float4 zBufferParams)
{
    return rcp(depthTBLR * zBufferParams.z + zBufferParams.w);
}

//Heavily based on normal reconstruction method in github repository GameTechDev/XeGTAO.
//https://github.com/GameTechDev/XeGTAO/blob/0d177ce06bfa642f64d8af4de1197ad1bcb862d4/Source/Rendering/Shaders/XeGTAO.hlsli#L143-L160
[numthreads(8,8,1)]
void ReconstructNormalFast (uint3 dispatchThreadID : SV_DispatchThreadID)
{
    float4 depthGatherBL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw);
    float4 depthGatherTR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(1, 1));

    float depthC = depthGatherBL.y;
    float depthT = depthGatherTR.x;
    float depthB = depthGatherBL.z;
    float depthL = depthGatherBL.x;
    float depthR = depthGatherTR.z;

    float linearDepth = LinearEyeDepth(depthC, _ZBufferParams);
    float4 linearDepths = LinearEyeDepthFloat4(float4(depthT, depthB, depthL, depthR), _ZBufferParams);

    float4 depthDifferenceTBLR = linearDepths - linearDepth;
    float slopeTB = (depthDifferenceTBLR.x - depthDifferenceTBLR.y) * 0.5;
    float slopeLR = (depthDifferenceTBLR.w - depthDifferenceTBLR.z) * 0.5;
    float4 depthDifferenceTBLRAverage = depthDifferenceTBLR + float4(-slopeTB, slopeTB, slopeLR, -slopeLR);
    depthDifferenceTBLR = min(abs(depthDifferenceTBLR), abs(depthDifferenceTBLRAverage));

    //0: edge; 1: non-edge
    float4 edgesTBLR = saturate(1.25 - depthDifferenceTBLR / (linearDepth * 0.011));
    //TL, TR, BR, BL 
    float4 acceptedNormals = saturate(float4(edgesTBLR.x * edgesTBLR.z, edgesTBLR.w * edgesTBLR.x, edgesTBLR.y * edgesTBLR.w, edgesTBLR.z * edgesTBLR.y) + 0.001);

    float3 viewPosC = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 0.5)) * _TextureSize.zw, depthC);
    float3 viewPosT = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 1.5)) * _TextureSize.zw, depthT);
    float3 viewPosB = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, -0.5)) * _TextureSize.zw, depthB);
    float3 viewPosL = GetViewSpacePosition((dispatchThreadID.xy + float2(-0.5, 0.5)) * _TextureSize.zw, depthL);
    float3 viewPosR = GetViewSpacePosition((dispatchThreadID.xy + float2(1.5, 0.5)) * _TextureSize.zw, depthR);

    float3 t = normalize(viewPosT - viewPosC);
    float3 b = normalize(viewPosC - viewPosB);
    float3 l = normalize(viewPosC - viewPosL);
    float3 r = normalize(viewPosR - viewPosC);

    float3 normalVS =   acceptedNormals.x * cross(l, t) + 
                        acceptedNormals.y * cross(r, t) + 
                        acceptedNormals.z * cross(r, b) + 
                        acceptedNormals.w * cross(l, b);
    normalVS = normalize(normalVS);
    _RW_NormalTexture[dispatchThreadID.xy] = float4(normalVS, 1.0);
}

//Heavily based on github gist bgolus/WorldNormalFromDepthTexture.shader
//https://gist.github.com/bgolus/a07ed65602c009d5e2f753826e8078a0#file-worldnormalfromdepthtexture-shader-L153-L218
//https://atyuwen.github.io/posts/normal-reconstruction/
[numthreads(8,8,1)]
void ReconstructNormalAccurate (uint3 dispatchThreadID : SV_DispatchThreadID)
{
#if FEWER_SAMPLES
    float4 depthGatherTL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(-1, 1));
    float4 depthGatherTR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(1, 2));
    float4 depthGatherBR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(2, 0));
    float4 depthGatherBL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(0, -1));
    
    float depthC = _DepthTexture.Load(int3(dispatchThreadID.xy, 0));
    float depthT = depthGatherTR.w;
    float depthB = depthGatherBL.y;
    float depthL = depthGatherTL.z;
    float depthR = depthGatherBR.x;
    float depthT2 = depthGatherTR.x;
    float depthB2 = depthGatherBL.z;
    float depthL2 = depthGatherTL.w;
    float depthR2 = depthGatherBR.y;

#else
    float4 depthGatherBL = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw);
    float4 depthGatherTR = _DepthTexture.GatherRed(sampler_LinearClamp, dispatchThreadID.xy * _TextureSize.zw, int2(1, 1));

    float depthC = depthGatherBL.y;
    float depthT = depthGatherTR.x;
    float depthB = depthGatherBL.z;
    float depthL = depthGatherBL.x;
    float depthR = depthGatherTR.z;
    float depthT2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(0, 2), 0));
    float depthB2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(0, -2), 0));
    float depthL2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(-2, 0), 0));
    float depthR2 = _DepthTexture.Load(int3(dispatchThreadID.xy + int2(2, 0), 0));
#endif

    float3 viewPosC = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 0.5)) * _TextureSize.zw, depthC);
    float3 viewPosT = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, 1.5)) * _TextureSize.zw, depthT);
    float3 viewPosB = GetViewSpacePosition((dispatchThreadID.xy + float2(0.5, -0.5)) * _TextureSize.zw, depthB);
    float3 viewPosL = GetViewSpacePosition((dispatchThreadID.xy + float2(-0.5, 0.5)) * _TextureSize.zw, depthL);
    float3 viewPosR = GetViewSpacePosition((dispatchThreadID.xy + float2(1.5, 0.5)) * _TextureSize.zw, depthR);

    float3 t = viewPosT - viewPosC;
    float3 b = viewPosC - viewPosB;
    float3 l = viewPosC - viewPosL;
    float3 r = viewPosR - viewPosC;

    float4 H = float4(depthL, depthR, depthL2, depthR2);
    float4 V = float4(depthB, depthT, depthB2, depthT2);

    float2 he = abs((2 * H.xy - H.zw) - depthC);
    float2 ve = abs((2 * V.xy - V.zw) - depthC);

    float3 hDeriv = he.x &lt; he.y ? l : r;
    float3 vDeriv = ve.x &lt; ve.y ? b : t;
    float3 normalVS = normalize(cross(hDeriv, vDeriv));
    _RW_NormalTexture[dispatchThreadID.xy] = float4(normalVS, 1.0);
}
</code></pre><h2 id=最后的思考>最后的思考<a hidden class=anchor aria-hidden=true href=#最后的思考>#</a></h2><p>本来还想使用3x3的采样，使用类似于吴彧文的方法，延伸第三个点到当前像素来计算准确的法线的，但是实际操作了一下发现，只要四个点构成了平行四边形就会认为是接近于当前采样，于是就会导致计算出错误的法线了。XeGTAO里面使用的计算法线的方式确实很巧妙，应该多用用，之后可能会再写一篇计算GTAO的文章吧。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://zznewclear13.github.io/tags/space-transformation/>Space Transformation</a></li></ul><nav class=paginav><a class=prev href=https://zznewclear13.github.io/posts/create-volumetric-fog-using-view-aligned-3d-texture/><span class=title>« Prev Page</span><br><span>使用和视锥体对齐的3D纹理来渲染体积雾</span></a>
<a class=next href=https://zznewclear13.github.io/posts/create-plant-swaying-in-wind-using-vertex-animation/><span class=title>Next Page »</span><br><span>使用顶点动画制作随风飘动的植物</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://zznewclear13.github.io/>ZZNEWCLEAR13</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script></body></html>